Automatically generated by Mendeley 0.9.7.1
Any changes to this file will be lost if it is regenerated by Mendeley.

@inproceedings{Perkins,
abstract = {We present ClearView, a system for automatically patching errors in deployed software. ClearView works on stripped Windows x86 binaries without any need for source code, debugging information, or other external information, and without human intervention.

ClearView(1) observes normal executions to learn invariants that characterize the applications normal behavior, (2) uses error detectors to monitor the execution to detect failures, (3) identifies violations of learned invariants that occur during failed executions, (4) generates candidate repair patches that enforce selected invariants by changing the state or the flow of control to make the invariant true, and (5) observes the continued execution of patched applications to select the most successful patch.

ClearView is designed to correct errors in software with high availability requirements. Aspects of ClearView that make it particularly appropriate for this context include its ability to generate patches without human intervention, to apply and remove patches in running applications without requiring restarts or otherwise perturbing the execution, and to identify and discard ineffective or damaging patches by evaluating the continued behavior of patched applications.

In a Red Team exercise, ClearView survived attacks that exploit security vulnerabilities. A hostile external Red Team developed ten code-injection exploits and used these exploits to repeatedly attack an application protected by ClearView. ClearView detected and blocked all of the attacks. For seven of the ten exploits, ClearView automatically generated patches that corrected the error, enabling the application to survive the attacks and successfully process subsequent inputs. The Red Team also attempted to make ClearView apply an undesirable patch, but ClearViews patch evaluation mechanism enabled ClearView to identify and discard both ineffective patches and damaging patches.},
address = {Big Sky, Montana},
annote = {Trying to patch arbitrary bugs in x86 binaries, but concentrating on deterministic heap bugs.  Phase 1 is to learn a model of program behaviour during normal operation, a la Daikon.  Phase 2 is to run the system with some suitable error detectors until an error is found.  Phase 3 patches the system to check the phase 1 invariants near to the phase 2 failure, and thence find which ones predict failure.  Phase 4 produces a bunch of candidate patches, one for each failure predictor, each of which hits the world with a hammer to make its predictor not fire.  Phase 5 filters the patches to figure out which ones are safe and effective by running them for a bit.
        
Learning invariants on binaries is kind of interesting.  They use every output of an instruction, and everything which was written to by a predominator of the instruction.
        
Procedure discovery is mixed dynamic/static.  Procedure heads are discovered dynamically in a fairly obvious way, and then explored statically.  Not clear from paper how they handle tail call optimisations.
        
There's a lot of stuff here about application communities which I think is pointless.
        
They do a red team evaluation, but cheat slightly: the blue team knew in advance what attach vectors the red team were going to be using, and trained their system using that information.
        
It looks like, after training, they usually need \~{}5 presentations to produce a fix, which is pretty decent.
        
Learning phase is incredibly expensive: factor of 300 overhead.  Don't need to run that all the time, but it's not clear whether you can automatically figure out when you do need to run it.  Hint from the eval that you probably need a few hours, but v. hard to quantify exactly.
        
Really quite clever, and annoyingly close to what I'm trying to do.
      },
author = {Perkins, JH and Kim, S and Larsen, S and Amarasinghe, S and Bachrach, Jonathan and Carbin, Michael and Pacheco, Carlos and Sherwood, Frank and Sidiroglou, Stelios and Sullivan, Greg and Wong, Weng-Fai and Zibin, Yoav and Ernst, Michael D. and Rinard, Martin},
booktitle = {Symposium on Operating System Principles},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perkins$\alpha$ et al. - Unknown - Automatically Patching Errors in Deployed Software.pdf:pdf},
keywords = {Clearview},
mendeley-tags = {Clearview},
publisher = {ACM},
title = {{Automatically Patching Errors in Deployed Software}},
year = {2009}
}
@misc{Mendeley2009,
address = {London},
annote = {Double click on the entry on the left to view the PDF.},
author = {Mendeley},
booktitle = {Mendeley Desktop},
file = {:usr/share/doc/mendeleydesktop/FAQ.pdf:pdf},
keywords = {Mendeley},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
year = {2009}
}
@article{Panigrahy2006,
arxivId = {arXiv:cs.DS/0510019},
author = {Panigrahy, R.},
file = {:home/sos22/papers/classification/nearest\_neighbour.pdf:pdf},
journal = {Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm},
keywords = {new},
mendeley-tags = {new},
pages = {1195},
publisher = {ACM},
title = {{Entropy based nearest neighbor search in high dimensions}},
year = {2006}
}
@article{Foster2002,
author = {Foster, J.S. and Terauchi, T. and Aiken, A.},
file = {:home/sos22/papers/typing/p1-foster.pdf:pdf},
journal = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
keywords = {alias analysis,ccr-9457812,constraints,e ect inference,in part by nsf,linux kernel,locking,new,ow-,restrict,sensitivity,this research was supported,type quali ers,types},
mendeley-tags = {new},
pages = {1--12},
publisher = {ACM New York, NY, USA},
title = {{Flow-sensitive type qualifiers}},
url = {http://portal.acm.org/citation.cfm?id=512531\&amp;dl=GUIDE,},
year = {2002}
}
@article{Bolzoni2005,
arxivId = {arXiv:cs.CR/0511043},
author = {Bolzoni, D. and Zambon, E. and Etalle, S. and Hartel, P.},
file = {:home/sos22/papers/anomaly-ids/posieden.pdf:pdf},
journal = {Arxiv preprint cs/0511043},
keywords = {new},
mendeley-tags = {new},
number = {1},
title = {{Poseidon: A 2-tier anomaly-based intrusion detection system}},
url = {http://arxiv.org/abs/cs/0511043},
year = {2005}
}
@article{Aftandilian2008,
abstract = {This paper introduces GC assertions, a system interface that programmers can use to check for errors, such as data structure invariant violations, and to diagnose performance problems, such as memory leaks. GC assertions are checked by the garbage collector, which is in a unique position to gather information and answer questions about the lifetime and connectivity of objects in the heap. We introduce several kinds of GC assertions, and we describe how they are implemented in the collector. We also describe our reporting mechanism, which provides a complete path through the heap to the offending objects. We show results for one type of assertion that allows the programmer to indicate that an object should be reclaimed at the next GC. We find that using this assertion we can quickly identify a memory leak and its cause with negligible overhead.},
annote = {Checks structure invariants in GC pass.  3\% overhead.  Assumes monolothic (non-generational, non-incremental) GC.  Manually added assertions only.  Variability of performance increases quite a bit; average overhead less so.},
author = {Aftandilian, Edward and Guyer, Samuel Z.},
journal = {Architectural Support for Programming Languages and Operating Systems},
keywords = {garbage collection,managed languages,memory leaks},
pages = {4},
title = {{GC assertions: using the garbage collector to check heap properties}},
url = {http://portal.acm.org/citation.cfm?id=1353533},
year = {2008}
}
@article{Jump2007,
abstract = {A memory leak in a garbage-collected program occurs when the program inadvertently maintains references to objects that it no longer needs. Memory leaks cause systematic heap growth, degrading performance and resulting in program crashes after perhaps days or weeks of execution. Prior approaches for detecting memory leaks rely on heap differencing or detailed object statistics which store state proportional to the number of objects in the heap. These overheads preclude their use on the same processor for deployed long-running applications.This paper introduces a dynamic heap-summarization technique based on type that accurately identifies leaks, is space efficient (adding less than 1\% to the heap), and is time efficient (adding 2.3\% on average to total execution time). We implement this approach in Cork which utilizes dynamic type information and garbage collection to summarize the live objects in a type points-from graph (TPFG) whose nodes (types) and edges (references between types) are annotated with volume. Cork compares TPFGs across multiple collections, identifies growing data structures, and computes a type slice for the user. Cork is accurate: it identifies systematic heap growth with no false positives in 4 of 15 benchmarks we tested. Cork's slice report enabled us (non-experts) to quickly eliminate growing data structures in SPECjbb2000 and Elipse, something their developers had not previously done. Cork is accurate, scalable, and efficient enough to consider using online.},
author = {Jump, Maria and McKinley, Kathryn S.},
journal = {Annual Symposium on Principles of Programming Languages},
keywords = {dynamic,garbage collection,memory leak detection,memory leaks,runtime analysis,unread},
mendeley-tags = {unread},
number = {1},
title = {{Cork:Â dynamic memory leak detection for garbage-collected languages}},
url = {http://portal.acm.org/citation.cfm?id=1190216.1190224},
volume = {42},
year = {2007}
}
@article{Chilimbi2006,
abstract = {We present the design, implementation, and evaluation of HeapMD, a dynamic analysis tool that finds heap-based bugs using anomaly detection. HeapMD is based upon the observation that, in spite of the evolving nature of the heap, several of its properties remain stable. HeapMD uses this observation in a novel way: periodically, during the execution of the program, it computes a suite of metrics which are sensitive to the state of the heap. These metrics track heap behavior, and the stability of the heap reflects quantitatively in the values of these metrics. The "normal" ranges of stable metrics, obtained by running a program on multiple inputs, are then treated as indicators of correct behaviour, and are used in conjunction with an anomaly detector to find heap-based bugs. Using HeapMD, we were able to find 40 heap-based bugs, 31 of them previously unknown, in 5 large, commercial applications.},
author = {Chilimbi, Trishul M. and Ganapathy, Vinod},
file = {:home/sos22/papers/error-detecting/heapmd.pdf:pdf},
issn = {0163-5964},
journal = {ACM SIGARCH Computer Architecture News},
keywords = {anomaly detection,bugs,debugging,heap,metrics,unread},
mendeley-tags = {unread},
number = {5},
title = {{HeapMD: identifying heap-based bugs using anomaly detection}},
url = {http://portal.acm.org/citation.cfm?id=1168885},
volume = {34},
year = {2006}
}
@article{Jump2009,
abstract = {Applications continue to increase in size and complexity which makes debugging and program understanding more challenging. Programs written in managed languages, such as Java, C\#, and Ruby, further exacerbate this challenge because they tend to encode much of their state in the heap. This paper introduces dynamic shape analysis, which seeks to characterize data structures in the heap by dynamically summarizing the object pointer relationships and detecting dynamic degree metrics based on class. The analysis identifies recursive data structures, automatically discovers dynamic degree metrics, and reports errors when degree metrics are violated. Uses of dynamic shape analysis include helping programmers find data structure errors during development, generating assertions for verification with static or dynamic analysis, and detecting subtle errors in deployment. We implement dynamic shape analysis in a Java Virtual Machine (JVM). Using SpecJVM and DaCapo benchmarks, we show that most objects in the heap are part of recursive data structures that maintain strong dynamic degree metrics. We show that once dynamic shape analysis establishes degree metrics from correct executions, it can find automatically inserted errors on subsequent executions in microbenchmarks. These suggests it can be used in deployment for improving software reliability.},
author = {Jump, Maria and McKinley, Kathryn S.},
journal = {International Symposium on Memory Management},
keywords = {degree metrics,dynamic invariants,dynamic shape analysis,unread},
mendeley-tags = {unread},
pages = {9},
title = {{Dynamic shape analysis via degree metrics}},
url = {http://portal.acm.org/citation.cfm?id=1542449},
year = {2009}
}
@article{Shankar2007,
abstract = {We present DITTO, an automatic incrementalizer for dynamic, side-effect-free data structure invariant checks. Incrementalization speeds up the execution of a check by reusing its previous executions, checking the invariant anew only the changed parts of the data structure. DITTO exploits properties specific to the domain of invariant checks to automate and simplify the process without restricting what mutations the program can perform. Our incrementalizer works for modern imperative languages such as Java and C\#. It can incrementalize,for example, verification of red-black tree properties and the consistency of the hash code in a hash table bucket. Our source-to-source implementation for Java is automatic, portable, and efficient. DITTO provides speedups on data structures with as few as 100 elements; on larger data structures, its speedups are characteristic of non-automatic incrementalizers: roughly 5-fold at 5,000 elements,and growing linearly with data structure size.},
author = {Shankar, Ajeet and Bod\'{\i}k, Rastislav},
journal = {Conference on Programming Language Design and Implementation},
keywords = {automatic,data structure invariants,dynamic optimization,incrementalization,optimistic memoization,program analysis,unread},
mendeley-tags = {unread},
number = {6},
title = {{DITTO:Â automatic incrementalization of data structure invariant checks (in Java)}},
url = {http://portal.acm.org/citation.cfm?id=1250770},
volume = {42},
year = {2007}
}
@article{Gorbovitski2008,
abstract = {This paper describes a general and powerful framework for efficient runtime invariant checking. The framework supports (1) declarative specification of arbitrary invariants using high-level queries, with easy use of information from any data in the execution, (2) powerful analysis and transformations for automatic generation of instrumentation for efficient incremental checking of invariants, and (3) convenient mechanisms for reporting errors, debugging, and taking preventive or remedial actions, as well as recording history data for use in queries. We demonstrate the advantages and effectiveness of the framework through implementations and case studies with abstract syntax tree transformations, authentication in a SMB client, and the BitTorrent peer-to-peer file distribution protocol.},
author = {Gorbovitski, Michael and Rothamel, Tom and Liu, Yanhong A. and Stoller, Scott D.},
journal = {International Symposium on Software Testing and Analysis},
keywords = {alias analysis,incrementalization,program transformation,runtime verification,unread},
mendeley-tags = {unread},
pages = {6},
title = {{Efficient runtime invariant checking:Â a framework and case study}},
url = {http://portal.acm.org/citation.cfm?id=1401827.1401837},
year = {2008}
}
@article{O'Neill2006,
abstract = {We introduce a method for providing lightweight daemons, called simplifiers, that attach themselves to program data. If a data item has a simplifier, the simplifier may be run automatically from time to time, seeking an opportunity to "simplify" the object in some way that improves the program's time or space performance.It is not uncommon for programs to improve their data structures as they traverse them, but these improvements must wait until such a traversal occurs. Simplifiers provide an alternative mechanism for making improvements that is not tied to the vagaries of normal control flow.Tracing garbage collectors can both support the simplifier abstraction and benefit from it. Because tracing collectors traverse program data structures, they can trigger simplifiers as part of the tracing process. (In fact, it is possible to view simplifiers as analogous to finalizers; whereas an object can have a finalizer that is run automatically when the object found to be dead, a simplifier can be run when the object is found to be live.)Simplifiers can aid efficient collection by simplifying objects before they are traced, thereby eliminating some data that would otherwise have been traced and saved by the collector. We present performance data to show that appropriately chosen simplifiers can lead to tangible space and speed benefits in practice.Different variations of simplifiers are possible, depending on the triggering mechanism and the synchronization policy. Some kinds of simplifier are already in use in mainstream systems in the form of ad-hoc garbage-collector extensions. For one kind of simplifier we include a complete and portable Java implementation that is less than thirty lines long.},
author = {O'Neill, Melissa E. and Burton, F. Warren},
journal = {Memory System Performance},
keywords = {finalizer,lightweight daemon,simplifier,unread,weak pointer},
mendeley-tags = {unread},
title = {{Smarter garbage collection with simplifiers}},
url = {http://portal.acm.org/citation.cfm?id=1178597.1178601},
year = {2006}
}
@article{Arnold2008,
abstract = {Coping with software defects that occur in the post-deployment stage is a challenging problem: bugs may occur only when the system uses a specific configuration and only under certain usage scenarios. Nevertheless, halting production systems until the bug is tracked and fixed is often impossible. Thus, developers have to try to reproduce the bug in laboratory conditions. Often the reproduction of the bug consists of the lion share of the debugging effort. In this paper we suggest an approach to address the aforementioned problem by using a specialized runtime environment (QVM, for Quality Virtual Machine). QVM efficiently detects defects by continuously monitoring the execution of the application in a production setting. QVM enables the efficient checking of violations of user-specified correctness properties, e.g., typestate safety properties, Java assertions, and heap properties pertaining to ownership. QVM is markedly different from existing techniques for continuous monitoring by using a novel overhead manager which enforces a user-specified overhead budget for quality checks. Existing tools for error detection in the field usually disrupt the operation of the deployed system. QVM, on the other hand, provides a balanced trade off between the cost of the monitoring process and the maintenance of sufficient accuracy for detecting defects. Specifically, the overhead cost of using QVM instead of a standard JVM, is low enough to be acceptable in production environments. We implemented QVM on top of IBM's J9 Java Virtual Machine and used it to detect and fix various errors in real-world applications.},
annote = {User-specified assertions.  Explicit overhead budget.  Some assertions are turned on and off per-object in response to ther assertions failing (assumption is that the failure isn't going to be immediately fatal, and will probably repeat).  Done by modifying the JVM.  Sampling is object-centric.

        
Overhead management: rdtsc around the optional code -> unsafe.

        
Don't stick like glue to the overhead budget, but not very far off.

      },
author = {Arnold, Matthew and Vechev, Martin and Yahav, Eran},
file = {:home/sos22/papers/qvm.pdf:pdf},
journal = {Conference on Object Oriented Programming Systems Languages and Applications},
keywords = {algorithms,java,reliability,unread,virtual machines},
mendeley-tags = {java,unread},
number = {10},
pages = {19},
title = {{QVM: an efficient runtime for detecting defects in deployed systems}},
url = {http://portal.acm.org/citation.cfm?id=1449764.1449776},
volume = {43},
year = {2008}
}
@article{Medard2003,
annote = {contains an article on baum-welch implementation.

      },
author = {M\'{e}dard, M},
file = {:home/sos22/papers/stats/Baum-Welch.pdf:pdf},
journal = {Citeseer},
keywords = {unread},
mendeley-tags = {unread},
number = {4},
pages = {1--24},
title = {{IEEE Information Theory Society Newsletter}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:IEEE+Information+Theory+Society+Newsletter\#3},
volume = {53},
year = {2003}
}
@misc{Davis2004,
abstract = {Hidden Markov Models have many applications in signal processing and pattern recognition, but their convergence-based training algorithms are known to suffer from over-sensitivity to the initial random model choice. This paper describes the boundary between regions in which ensemble learning is superior to Rabinerâs multiple-sequence Baum-Welch training method, and proposes techniques for determining the best method in any arbitrary situation. It also studies the suitability of the training methods using the condition number, a recently proposed diagnostic tool for test-ing the quality of the model. A new method for training Hidden Markov Models Correspondence to:},
author = {Davis, Richard I. A. and Lovell, Brian C.},
doi = {10.1.1.5.2069},
file = {::},
keywords = {unread},
mendeley-tags = {unread},
title = {{Comparing and Evaluating HMM Ensemble Training Algorithms Using Train and Test and Condition Number Criteria.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.2069},
year = {2004}
}
@article{Baum1970,
author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
file = {:home/sos22/papers/stats/Baum-Welch-early.pdf:pdf},
journal = {The Annals of Mathematical Statistics},
keywords = {unread},
mendeley-tags = {unread},
number = {1},
pages = {164 -- 171},
title = {{A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov ... more}},
url = {http://www.jstor.org/stable/2239727},
volume = {41},
year = {1970}
}
@article{Baum1970a,
author = {Baum, L.E. and Petrie, T. and Soules, G. and Weiss, N.},
file = {:home/sos22/papers/stats/Baum-Welch-early.pdf:pdf},
journal = {The Annals of Mathematical Statistics},
number = {1},
pages = {164--171},
publisher = {Institute of Mathematical Statistics},
title = {{A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains}},
url = {http://www.jstor.org/stable/2239727},
volume = {41},
year = {1970}
}
@article{Rabiner1990,
author = {Rabiner, L.R.},
file = {:home/sos22/papers/stats/tutorial\_on\_hmms\_rabiner\_1990.pdf:pdf},
journal = {Readings in speech recognition},
number = {3},
pages = {267--296},
publisher = {San Mateo, CA},
title = {{A tutorial on hidden Markov models and selected applications in speech recognition}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=iDHgboYRzmgC\&amp;oi=fnd\&amp;pg=PA267\&amp;dq=A+tutorial+on+hidden+markov+models+and+selected+applications+in+speech+recognition\&amp;ots=j9aOyVUqlK\&amp;sig=D5\_2WMi6zjoqersOFeDwDNBiRBg},
volume = {53},
year = {1990}
}
@article{Avgustinov2007,
author = {Avgustinov, Pavel and Tibble, Julian and de Moor, Oege},
doi = {10.1145/1297105.1297070},
file = {:home/sos22/papers/modelling/p589-avgustinov.pdf:pdf},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {pro-,program monitors,runtime veri cation},
month = oct,
number = {10},
pages = {589},
title = {{Making trace monitors feasible}},
url = {http://portal.acm.org/citation.cfm?doid=1297105.1297070},
volume = {42},
year = {2007}
}
@phdthesis{Liblit2004,
abstract = {Debugging does not end with deployment. Static analysis, in-house testing, and good software engineering practices can catch or prevent many problems before software is dis- tributed. Yet mainstream commercial software still ships with both known and unknown bugs. Real software still fails in the hands of real users. The need remains to identify and repair bugs that are only discovered, or whose importance is only revealed, after the software is released. Unfortunately we know almost nothing about how software behaves (and misbehaves) in the hands of end users. Traditional post-deployment feedback mecha- nisms, such as technical support phone calls or hand-composed bug reports, are informal, inconsistent, and highly dependent on manual, human intervention. This approach clouds the view, preventing engineers from seeing a complete and truly representative picture of how and why problems occur.

This dissertation proposes a system to support debugging based on feedback from ac- tual users. Cooperative Bug Isolation (CBI) leverages the key strength of user communi- ties: their overwhelming numbers. We propose a low-overhead instrumentation strategy for gathering information from the executions experienced by large numbers of software end users. Our approach limits overhead using sparse random sampling rather than complete data collection, while simultaneously ensuring that the observed data is an unbiased, rep- resentative subset of the complete program behavior across all runs. We discuss a number of specific instrumentation schemes that may be coupled with the general sampling trans- formation to produce feedback data that we have found to be useful for isolating the causes of a wide variety of bugs.

Collecting feedback from real code, especially real buggy code, is a nontrivial exercise. This dissertation presents our approach to a number of practical challenges that arise in building a complete, working CBI system. We discuss how the general sampling transfor- mation scheme can be extended to deal with native compilers, libraries, dynamically loaded code, threads, and other features of modern software. We address questions of privacy and security as well as related issues of user interaction and informed user consent. This design and engineering investment has allowed us to begin an actual public deployment of a CBI system, initial results from which we report here. Of course, feedback data is only as useful as the sense we can make of it. When data is fair but very sparse, the noise level is high and traditional manual debugging techniques insufficient. This dissertation presents a suite of new algorithms for statistical debugging: finding and fixing software errors based on statistical analysis of sparse feedback data. The techniques vary in complexity and sophistication, from simple process of elimina- tion strategies to regression techniques that build models of suspect program behaviors as failure predictors. Our most advanced technique combines a number of general and domain-specific statistical filtering and ranking techniques to separate the effects of differ- ent bugs and identify predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur and the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies. These case studies include examples in which the algorithm found previously unknown, significant crashing bugs in widely used systems.
},
author = {Liblit, B.R.},
file = {:home/sos22/papers/liblit.pdf:pdf},
publisher = {Citeseer},
title = {{Cooperative bug isolation}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Cooperative+bug+isolation\#0},
year = {2004}
}
@article{Strom1986,
author = {Strom, RE and Yemini, S.},
file = {:home/sos22/papers/typing/tse12-typestate.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
number = {1},
pages = {157--171},
publisher = {IEEE Press Piscataway, NJ, USA},
title = {{Typestate: A programming language concept for enhancing software reliability}},
url = {http://portal.acm.org/citation.cfm?id=10677.10693},
volume = {12},
year = {1986}
}
@inproceedings{Xu,
address = {San Diego},
author = {Xu, W. and Huang, L. and Fox, A. and Patterson, D. and Jordan, M.},
booktitle = {Third Workshop on Tackling Computer Systems Problems with Machine Learning Techniques},
file = {:home/sos22/papers/logging/xu.pdf:pdf},
publisher = {Usenix},
title = {{Mining console logs for large-scale system problem detection}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Mining+Console+Logs+for+Large-Scale+System+Problem+Detection\#0},
year = {2008}
}
@article{Lu2007,
abstract = {Software defects significantly reduce system dependability. Among various types of software bugs, semantic and concurrency bugs are two of the most difficult to detect. This paper proposes a novel method, called MUVI, that detects an important class of semantic and concurrency bugs. MUVI automatically infers commonly existing multi-variable access correlations through code analysis and then detects two types of related bugs: (1) inconsistent updates--correlated variables are not updated in a consistent way, and (2) multi-variable concurrency bugs--correlated accesses are not protected in the same atomic sections in concurrent programs.We evaluate MUVI on four large applications: Linux, Mozilla,MySQL, and PostgreSQL. MUVI automatically infers more than 6000 variable access correlations with high accuracy (83\%).Based on the inferred correlations, MUVI detects 39 new inconsistent update semantic bugs from the latest versions of these applications, with 17 of them recently confirmed by the developers based on our reports.We also implemented MUVI multi-variable extensions to tworepresentative data race bug detection methods (lock-set and happens-before). Our evaluation on five real-world multi-variable concurrency bugs from Mozilla and MySQL shows that the MUVI-extension correctly identifies the root causes of four out of the five multi-variable concurrency bugs with 14\% additional overhead on average. Interestingly, MUVI also helps detect four new multi-variable concurrency bugs in Mozilla that have never been reported before. None of the nine bugs can be identified correctly by the original race detectors without our MUVI extensions.},
annote = {Infer correlations between variable updates through source-code analysis, and then treat violations of that correlation as a potential bug (in a static analysis pass).  Accesses can be read, write, or either.  Correlation means that the variables are accessed close to each other (statically) most of the time (or one is accessed shortly after the other, or...).  Follows call chain to depth one.  Uses frequent itemset mining to extract correlations.  Correlations are then fed to dynamic checkers like happens-before or lockset.  No concept of conditional dependency.},
author = {Lu, Shan and Park, Soyeon and Hu, Chongfeng and Ma, Xiao and Jiang, Weihang and Li, Zhenmin and Popa, Raluca A. and Zhou, Yuanyuan},
file = {:home/sos22/papers/error-detecting/p103-lu.pdf:pdf},
journal = {ACM Symposium on Operating Systems Principles},
keywords = {bug detection,concurrency bug,variable correlation},
number = {6},
title = {{MUVI: automatically inferring multi-variable access correlations and detecting related semantic and concurrency bugs}},
url = {http://portal.acm.org/citation.cfm?id=1294272},
volume = {41},
year = {2007}
}
@article{Cadar,
abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverageon average over 90\% per tool (median: over 94\%)  and significantly beat the coverage of the developers own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.

We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
annote = {Automatically generating test suites.  Includes cross-checking models of different nominally-equivalent programs.  Results impressive. Works on LLVM bytecode.  Symbolic execution based.  Single-threaded only.  Pointer aliasing handling is simplistic but apparently effective.  Environment simulation is essentially the SDV model: simple stubs capturing the interesting properties of system calls.  Includes a thingy to shepherd unmodified programs to follow failing tests.  Can also be used to assert that two implementations of a function are equivalent (for some developer-supplied definition of equivalent).

        
Seems pretty solid, although the lack of support for non-determinism limits the real-world applicability.

      },
author = {Cadar, C. and Dunbar, D. and Engler, D.},
file = {:home/sos22/papers/klee.pdf:pdf},
journal = {usenix.org},
pages = {209--224},
title = {{KLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:KLEE:+Unassisted+and+Automatic+Generation+of+High-Coverage+Tests+for+Complex+Systems+Programs\#0},
year = {2008}
}
@article{Shalizi2001a,
author = {Shalizi, C.R.},
file = {:home/sos22/papers/shalizi-hmms.pdf:pdf},
journal = {Unpublished PhD thesis, University of Wisconsin at Madison, Wisconsin},
number = {May},
publisher = {Citeseer},
title = {{Causal architecture, complexity and self-organization in time series and cellular automata}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Causal+Architecture,+Complexity+and+Self-Organization+in+Time+Series+and+Cellular+Automata\#0},
year = {2001}
}
@article{Shalizi2001,
author = {Shalizi, Cosma Rohilla and Olsson, Martin},
file = {:home/sos22/papers/shalizi-hmms.pdf:pdf},
title = {{Causal architecture, complexity and self-organization in time series and cellular automata}},
url = {http://portal.acm.org/citation.cfm?id=933791},
year = {2001}
}
@inproceedings{Brumley2007,
author = {Brumley, David and Caballero, Juan and Liang, Zhenkai and Newsome, James},
booktitle = {16th USENIX Security Symposium},
file = {:home/sos22/papers/anomaly-ids/brumley.pdf:pdf},
pages = {213--228},
title = {{Towards Automatic Discovery of Deviations in Binary Implementations with Applications to Error Detection and Fingerprint Generation}},
url = {https://www.usenix.org/events/sec07/tech/brumley/brumley\_html/},
year = {2007}
}
@misc{Chandola2009,
author = {Chandola, V. and Banerjee, A. and Kumar, V.},
booktitle = {ACM Computing Surveys (CSUR)},
file = {:home/sos22/papers/anomaly-ids/survey-chandola.pdf:pdf},
institution = {University of Minnesota},
number = {3},
pages = {15},
publisher = {ACM},
title = {{Anomaly detection: A survey}},
url = {http://portal.acm.org/citation.cfm?id=1541882},
volume = {41},
year = {2009}
}
@article{Weiss1998,
author = {Weiss, G.M. and Hirsh, H.},
file = {:home/sos22/papers/anomaly-ids/kdd98.pdf:pdf},
journal = {Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining},
pages = {359--363},
publisher = {Citeseer},
title = {{Learning to predict rare events in event sequences}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Learning+to+Predict+Rare+Events+in+Event+Sequences\#0},
year = {1998}
}
@article{Marceau2005,
author = {Marceau, C.},
file = {:home/sos22/papers/anomaly-ids/marceau.pdf:pdf},
journal = {Construction},
keywords = {automata,computational,intrusion detection,string processing},
pages = {101--110},
publisher = {Citeseer},
title = {{Characterizing the behavior of a program using multiple-length n-grams}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Characterizing+the+Behavior+of+a+Program+Using+Multiple-Length+N-grams\#0},
year = {2005}
}
@article{Forrest1996,
author = {Forrest, S. and Hofmeyr, S.A. and Somayaji, A. and Longstaff, T.A. and Others},
file = {:home/sos22/papers/anomaly-ids/sense-of-self.pdf:pdf},
journal = {IEEE Symposium on Security and Privacy},
pages = {120--128},
publisher = {IEEE COMPUTER SOCIETY},
title = {{A sense of self for unix processes}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/SECPRI.1996.502675},
year = {1996}
}
@misc{Newsome2005,
author = {Newsome, James and Song, Dawn},
booktitle = {Analysis},
file = {:home/sos22/papers/anomaly-ids/taintcheck-full.pdf:pdf},
institution = {Carnegie Mellon University},
number = {May 2004},
pages = {43},
title = {{Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software}},
year = {2005}
}
@article{Lee2001,
author = {Lee, Wenke and Xiang, Dong},
doi = {10.1109/SECPRI.2001.924294},
file = {:home/sos22/papers/anomaly-ids/info-theory-aids.pdf:pdf},
isbn = {0-7695-1046-9},
journal = {Proceedings 2001 IEEE Symposium on Security and Privacy. S\&P 2001},
pages = {130--143},
publisher = {IEEE Comput. Soc},
title = {{Information-theoretic measures for anomaly detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=924294},
year = {2001}
}
@misc{Tip1995,
abstract = {A program slice consists of the parts of a program that (potentially) affect the values computed at some point of interest referred to as a slicing criterion. The task of computing program slices is called program slicing. The original definition of a program slice was presented by Weiser in 1979. Since then various slightly different notions of program slices have been proposed as well as a number of methods to compute them.  An important distinction is that between a static and a dynamic slice.  The former notion is computed without making assumptions regarding a program's input, whereas the latter relies on some specific test case.

Procedures, arbitrary control flow, composite datatypes and pointers, and inter-process communication each require a specific solution.  We classify static and dynamic slicing methods for each of these features, and compare their accuracy and efficiency.  Moreover, the possibilities for combining solutions for different features are investigated.  We discuss how compiler-optimization techniques can be used to obtain more accurate slices.  The paper is concluded with an overview of the applications of program slicing, which include debugging, program integration, dataflow testing, and softwaremain tenance.},
address = {Amsterdam},
author = {Tip, F.},
booktitle = {Journal of programming languages},
file = {:home/sos22/papers/slicing/tip94.pdf:pdf},
institution = {Centrum voor Wiskunde en Informatica},
number = {3},
pages = {121--189},
publisher = {Citeseer},
title = {{A survey of program slicing techniques}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:A+survery+of+Program+Slicing+Techniques\#0},
volume = {3},
year = {1995}
}
@article{Marceau2001,
author = {Marceau, Carla},
journal = {New Security Paradigms Workshop},
keywords = {computational immunology,finite automata,intrusion detection,string processing},
title = {{Characterizing the behavior of a program using multiple-length N-grams}},
url = {http://portal.acm.org/citation.cfm?id=366173.366197},
year = {2001}
}
@article{Weiser1981,
abstract = {Program slicing is a method used by experienced computer programmers for abstracting from programs. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a âsliceâ, is an independent program guaranteed to faithfully represent the original program within the domain of the specified subset of behavior.Finding a slice is in general unsolvable. A dataflow algorithm is presented for approximating slices when the behavior subset is specified as the values of a set of variables at a statement. Experimental evidence is presented that these slices are used by programmers during debugging. Experience with two automatic slicing tools is summarized. New measures of program complexity are suggested based on the organization of a program's slices.},
author = {Weiser, Mark},
journal = {International Conference on Software Engineering},
keywords = {Data flow analysis,Debugging,Human factors,Program maintenance,Program metrics,Software tools},
title = {{Program slicing}},
url = {http://portal.acm.org/citation.cfm?id=800078.802557},
year = {1981}
}
@phdthesis{Ammons2003,
abstract = {Program verification tools (such as model checkers) are powerful tools for finding errors in programs. Unfortunately, these tools need lots of formal specifications of correct program behavior. Can we really expect programmers to write all of these specifications by hand? This dissertation is about Strauss, a tool I wrote to bring automation to specification-writing. By observing traces of working programs, Strauss infers many temporal specifications, each of which says how correct programs use a small part of an interface. I used Strauss to derive 17 formal specifications for the X11 windowing system (whose libraries contain over 2000 routines and over 500 data structures), and used the specifications to find bugs in several widely distributed applications.},
annote = {Very few citations.  Doesn't look overly impressive from the introduction.

      },
author = {Ammons, G.},
file = {:home/sos22/papers/modelling/strauss.pdf:pdf},
publisher = {Citeseer},
school = {The University of Wisconsin - Madison},
title = {{Strauss: a specification miner}},
type = {PhD},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Strauss:+a+specification+miner\#0},
year = {2003}
}
@article{Cook1998,
abstract = {Many software process methods and tools presuppose the existence of a formal model of a process. Unfortunately, developing a formal model for an on-going, complex process can be difficult, costly, and error prone. This presents a practical barrier to the adoption of process technologies, which would be lowered by automated assistance in creating formal models. To this end, we have developed a data analysis technique that we term process discovery. Under this technique, data describing process events are first captured from an on-going process and then used to generate a formal model of the behavior of that process. In this article we describe a Markov method that we developed specifically for process discovery, as well as describe two additional methods that we adopted from other domains and augmented for our purposes. The three methods range from the purely algorithmic to the purely statistical. We compare the methods and discuss their application in an industrial case study.},
author = {Cook, Jonathan E. and Wolf, Alexander L.},
doi = {10.1145/287000.287001},
file = {:home/sos22/papers/modelling/tosem0798.pdf:pdf},
issn = {1049331X},
journal = {ACM Transactions on Software Engineering and Methodology},
month = jul,
number = {3},
pages = {215--249},
title = {{Discovering models of software processes from event-based data}},
url = {http://portal.acm.org/citation.cfm?doid=287000.287001},
volume = {7},
year = {1998}
}
@inproceedings{Hirzel2001,
abstract = {With processor speed increasing much more rapidly than memory access speed, memory system optimizations have the potential to significantly improve program perfor- mance. Unfortunately, cache-level optimizations often re- quire detailed temporal information about a programâs ref- erences to be effective. Traditional techniques for obtain- ing this information are too expensive to be practical in an on-line setting. We address this problem by describ- ing and evaluating a framework for low-overhead tempo- ral profiling. Our framework extends the Arnold-Ryder framework that uses instrumentation and counter-based sampling to collect frequency profiles with low overhead. Our framework samples bursts (sub-sequences) of the trace of all runtime events to construct a temporal program pro- file. Our bursty tracing profiler is built using Vulcan, an executable-editing tool for x86, and we evaluate it on opti- mized x86 binaries. Like the Arnold-Ryder framework, we have the advantages of not requiring operating system or hardware support and being deterministic. Unlike them, we are not limited to capturing temporal relationships on intraprocedural acyclic paths since our trace bursts can span procedure boundaries. In addition, our framework does not require access to program source or recompila- tion. A direct implementation of our extensions to the Arnold-Ryder framework results in profiling overhead of 6-35\%. We describe techniques that reduce this overhead to 3-18\%, making it suitable for use in an on-line setting.},
author = {Hirzel, M. and Chilimbi, Trishul},
booktitle = {4th ACM Workshop on Feedback-Directed and Dynamic Optimization (FDDO-4)},
file = {:home/sos22/papers/logging/bursts\_fddo.pdf:pdf},
number = {1},
pages = {117--126},
publisher = {Citeseer},
title = {{Bursty tracing: A framework for low-overhead temporal profiling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.25.1980\&amp;rep=rep1\&amp;type=pdf},
volume = {0430},
year = {2001}
}
@article{Reiss2001,
abstract = {Dynamic analysis is based on collecting data as the program runs. However, raw traces tend to be too voluminous and too unstructured to be used directly for visualization and understanding. We address this problem in two phases: the first phase selects subsets of the data and then compacts it, while the second phase encodes the data in an attempt to infer its structure. Our major compaction/selection techniques include gprof-style N-depth call sequences, selection based on class, compaction based on time intervals, and encoding the whole execution as a directed acyclic graph. Our structure inference techniques include run-length encoding, context-free grammar encoding, and the building of finite state automata.},
annote = {Cited all over the place.

        
Essentially a fuzzy  compression algorithm for dynamically-collected traces of program execution.  Involves static binary patching of executables, which is kind of exciting.  Very hacky.  Generates an enormous amount of data (hundreds of megabytes per second) even with all of their trace reduction stuff turned on.

        
There's a moderately useful bit on building approximate FSAs out of trace data.

        
This is really quite stupid and obvious.  A good demonstration that if you get in at the start of a field you don't actually need to do very much to get widely cited.

      },
author = {Reiss, Steven P. and Renieris, Manos},
file = {:home/sos22/papers/random/p221-reiss.pdf:pdf},
issn = {0270-5257},
journal = {International Conference on Software Engineering},
keywords = {dynamic program analysis,program tracing,software understand},
title = {{Encoding program executions}},
url = {http://portal.acm.org/citation.cfm?id=381473.381497},
year = {2001}
}
@book{Boigelot1997,
abstract = {VeriSoft [God97] is a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary C (or C++) code. VeriSoft can automatically detect coordination problems between the concurrent processes of a system. In this paper, we present a method to synthesize a finite-state machine that simulates all the sequences of visible operations of a given process that were observed during a state-space exploration performed by VeriSoft. The examination of this machine makes it possible to discover the dynamic behavior of the process in its environment and to understand how it contributes to the global behavior of the system.},
address = {Berlin/Heidelberg},
author = {Boigelot, Bernard and Godefroid, Patrice},
booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
doi = {10.1007/BFb0035375},
isbn = {3-540-62790-1},
pages = {321 -- 333},
publisher = {Springer-Verlag},
series = {Lecture Notes in Computer Science},
title = {{Tools and Algorithms for the Construction and Analysis of Systems}},
url = {http://www.springerlink.com/index/10.1007/BFb0035375},
volume = {1217},
year = {1997}
}
@article{Jackson1994,
abstract = {A dependence model for reverse engineering should treat procedures in a modular fashion and should be fine-grained, distinguishing dependences that are due to different variables. The program dependence graph (PDG) satisfies neither of these criteria. We present a new form of dependence graph that satisfies both, while retaining the advantages of the PDG: it is easy to construct and allows program slicing to be implemented as a simple graph traversal. We define 'chopping', a generalization of slicing that can express most of its variants, and show that, using our dependence graph, it produces more accurate results than algorithms based directly on the PDG.},
author = {Jackson, Daniel and Rollins, Eugene J.},
journal = {Foundations of Software Engineering},
keywords = {dataflow dependence,modularity,program dependence graph,program slicing,reverse engineering,specifications},
number = {5},
title = {{A new model of program dependences for reverse engineering}},
url = {http://portal.acm.org/citation.cfm?id=195281},
volume = {19},
year = {1994}
}
@article{Hangal2002,
abstract = {This paper introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program's results.We implemented the DIDUCE system for Java programs and applied it to four programs of significant size and complexity. DIDUCE succeeded in identifying the root causes of programming errors in each of the programs quickly and automatically. In particular, DIDUCE is effective in isolating a timing-dependent bug in a released JSSE (Java Secure Socket Extension) library, which would have taken an experienced programmer days to find. Our experience suggests that detecting and checking program invariants dynamically is a simple and effective methodology for debugging many different kinds of program errors across a wide variety of application domains.},
annote = {Start with a hypothesised invariant that nothing at all is possible, and slowly expand the allowable space to include whatever the program does when it's running.  Flag a warning whenever the seen space increases.  Theory is that just before a crash/bug you'll get a bunch of warnings as the program goes off-message, and these warning will tell you what's special about the input which caused the crash.  Obvious refinement is to look at statistical properties and ask what's rare rather than what's new, but that's a problem for another time.  Their implementation is in Java, but could be extended to real languages easily enough.
Their invariants are stateless and quite primitive.  A few interesting bits about how confident you can be in an inferred invariant.

        
Results suggest that you need to run for about an hour before violations become interesting; unclear how well that will generalise.

        
Really quite interesting.  Potentially useful in SLI because it gives you more information to do the analysis bits with, if you're careful.  Hard to do well without introducing lots of normal-mode overhead, though.

        
Lots of cites.

      },
author = {Hangal, Sudheendra and Lam, Monica S.},
file = {:home/sos22/papers/modelling/Diduce.pdf:pdf},
journal = {International Conference on Software Engineering},
keywords = {DIDUCE,Java},
mendeley-tags = {DIDUCE,Java},
title = {{Tracking down software bugs using automatic anomaly detection}},
url = {http://portal.acm.org/citation.cfm?id=581377},
year = {2002}
}
@article{Reiss2000,
author = {Reiss, Steven P. and Renieris, Manos},
journal = {Java Grande Conference},
title = {{Generating Java trace data}},
url = {http://portal.acm.org/citation.cfm?id=337481},
year = {2000}
}
@article{State-space,
abstract = {VeriSoft [God97] is a tool for systematically exploring the state spaces of systems composed of several concurrent processes exe- cuting arbitrary C (or C-l-+) code. VeriSoft can automatically detect coordination problems between the concurrent processes of a system. In this paper, we present a method to synthesize a finite-state machine that simulates all the sequences of visible operations of a given process that were observed during a state-space exploration performed by VeriSoft. The examination of this machine makes it possible to discover the dy- namic behavior of the process in its environment and to understand how it contributes to the global behavior of the system.},
annote = {Pretty much advertising bumpf for a model checker which the authors are trying to sell.  Not very interesting.

        
Content of paper has less to do with the title than you would expect: they don't actually make any dynamic observations; it's all pulled out of the model checker.

      },
author = {Boigelot, B. and Godefroid, P.},
file = {:home/sos22/papers/modelling/boigelet.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {321--333},
publisher = {Springer},
title = {{Automatic synthesis of specifications from the dynamic observation of reactive programs}},
url = {http://www.springerlink.com/index/dj4w40h91v4n0w29.pdf},
year = {1997}
}
@article{Wedyan2009,
abstract = {Many automated static analysis (ASA) tools have been developed in recent years for detecting software anomalies. The aim of these tools is to help developers to eliminate soft- ware defects at early stages and produce more reliable soft- ware at a lower cost. Determining the effectiveness of ASA tools requires empirical evaluation. This study evaluates coding concerns reported by three ASA tools on two open source software (OSS) projects with respect to two types of modifications performed in the studied software CVS repos- itories: corrections of faults that caused failures, and refac- toring modifications. The results show that fewer than 3\% of the detected faults correspond to the coding concerns re- ported by the ASA tools. ASA tools were more effective in identifying refactoring modifications and corresponded to about 71\% of them. More than 96\% of the coding concerns were false positives that do not relate to any fault or refac- toring modification},
annote = {Attempt to evaluate how useful static analysis is, but the methodology is completely invalid: ran the tool on a very old version of a project, and then looked forwards through the history to see if the reported bugs were ever fixed.  Under that metric, the tools were pretty useless, but that's completely unsurprising and contains almost no information.

        
Didn't read past the introduction because the paper was so useless.

        
No cites; no surprise.

      },
author = {Wedyan, Fadi and Alrmuny, Dalal and Bieman, James M.},
doi = {10.1109/ICST.2009.21},
file = {:home/sos22/papers/static-analysis/effectiveness-overview.pdf:pdf},
isbn = {978-1-4244-3775-7},
journal = {2009 International Conference on Software Testing Verification and Validation},
month = apr,
pages = {141--150},
publisher = {Ieee},
title = {{The Effectiveness of Automated Static Analysis Tools for Fault Detection and Refactoring Prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4815346},
year = {2009}
}
@article{Glerum2009,
abstract = {Windows Error Reporting (WER) is a distributed system that automates the processing of error reports coming from an installed base of a billion machines. WER has collected billions of error reports in ten years of operation. It collects error data automatically and classifies errors into buckets, which are used to prioritize developer effort and report fixes to users. WER uses a progressive approach to data collection, which minimizes overhead for most reports yet allows developers to collect detailed information when needed. WER takes advantage of its scale to use error statistics as a tool in debugging; this allows developers to isolate bugs that could not be found at smaller scale. WER has been designed for large scale: one pair of database servers can record all the errors that occur on all Windows computers worldwide.},
annote = {The Windows Error Reporting paper},
author = {Glerum, K. and Kinshumann, K. and Greenberg, S. and Aul, G. and Orgovan, V. and Nichols, G. and Grant, D. and Loihle, G. and Hunt, G.},
file = {:home/sos22/papers/debugging/wer.pdf:pdf},
journal = {Symposium on Operating System Principles},
keywords = {WER},
mendeley-tags = {WER},
publisher = {SOSP},
title = {{Debugging in the (Very) Large: Ten Years of Implementation and Experience}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Debugging+in+the+(Very)+Large:+Ten+Years+of+Implementation+and+Experience\#0},
year = {2009}
}
@book{Boigelot1997a,
abstract = {VeriSoft [God97] is a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary C (or C++) code. VeriSoft can automatically detect coordination problems between the concurrent processes of a system. In this paper, we present a method to synthesize a finite-state machine that simulates all the sequences of visible operations of a given process that were observed during a state-space exploration performed by VeriSoft. The examination of this machine makes it possible to discover the dynamic behavior of the process in its environment and to understand how it contributes to the global behavior of the system.},
address = {Berlin/Heidelberg},
author = {Boigelot, Bernard and Godefroid, Patrice},
booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
doi = {10.1007/BFb0035375},
isbn = {3-540-62790-1},
pages = {321 -- 333},
publisher = {Springer-Verlag},
series = {Lecture Notes in Computer Science},
title = {{Tools and Algorithms for the Construction and Analysis of Systems}},
url = {http://www.springerlink.com/index/10.1007/BFb0035375},
volume = {1217},
year = {1997}
}
@inproceedings{Kruegel,
author = {Kruegel, C. and Mutz, D. and Robertson, W. and Valeur, F.},
booktitle = {19th Annual Computer Security Applications Conference, 2003. Proceedings.},
doi = {10.1109/CSAC.2003.1254306},
file = {:home/sos22/papers/anomaly-ids/bayesian-ids.pdf:pdf},
isbn = {0-7695-2041-3},
pages = {14--23},
publisher = {IEEE},
title = {{Bayesian event classification for intrusion detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1254306},
year = {2003}
}
@book{Kernighan1988,
author = {Kernighan, Brian W. and Ritchie, Dennis},
isbn = {0131103628},
publisher = {Prentice Hall},
title = {{The  C Programming Language (2nd Edition)}},
url = {http://www.amazon.co.uk/C-Programming-Language-2nd/dp/0131103628},
year = {1988}
}
@inproceedings{citeulike:3376950,
abstract = {We describe the design and implementation of Dynamo, a software dynamic optimization system that is capable of transparently improving the performance of a native instruction stream as it executes on the processor. The input native instruction stream to Dynamo can be dynamically generated (by a JIT for example), or it can come from the execution of a statically compiled native binary. This paper evaluates the Dynamo system in the latter, more challenging situation, in order to emphasize the limits, rather than the potential, of the system. Our experiments demonstrate that even statically optimized native binaries can be accelerated Dynamo, and often by a significant degree. For example, the average performance of âO optimized SpecInt95 benchmark binaries created by the HP product C compiler is improved to a level comparable to their âO4 optimized version running without Dynamo. Dynamo achieves this by focusing its efforts on optimization opportunities that tend to manifest only at runtime, and hence opportunities that might be difficult for a static compiler to exploit. Dynamo's operation is transparent in the sense that it does not depend on any user annotations or binary instrumentation, and does not require multiple runs, or any special compiler, operating system or hardware support. The Dynamo prototype presented here is a realistic implementation running on an HP PA-8000 workstation under the HPUX 10.20 operating system. 1.},
author = {Bala, Vasanth},
booktitle = {ACM SIGPLAN Notices},
file = {:home/sos22/papers/random/10.1.1.36.7516.pdf:pdf},
keywords = {dynamic,translation},
pages = {1--12},
title = {{Dynamo: a transparent dynamic optimization system}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.7516},
volume = {35},
year = {2000}
}
@inproceedings{Luk2005,
abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), ItaniumÂ®, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
annote = {The Pin paper.  Does exactly what it says on the tin.},
author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
booktitle = {Conference on Programming Language Design and Implementation},
file = {:home/sos22/papers/dbt/p190-luk.pdf:pdf},
keywords = {dynamic compilation,instrumentation,program analysis tools},
number = {6},
publisher = {ACM},
title = {{Pin: building customized program analysis tools with dynamic instrumentation}},
url = {http://portal.acm.org/citation.cfm?doid=1065010.1065034},
volume = {40},
year = {2005}
}
@article{Nethercote2007,
author = {Nethercote, N. and Seward, J.},
file = {:home/sos22/papers/valgrind2007.pdf:pdf},
journal = {ACM SIGPLAN Notices},
keywords = {also,as no preparation,code,dynamic binary analysis,dynamic binary instrumentation,instrumentation coverage of user-mode,is needed,it gives 100,memcheck,relinking,shadow values,such as recompiling or,valgrind},
number = {6},
pages = {100},
publisher = {ACM},
title = {{Valgrind: a framework for heavyweight dynamic binary instrumentation}},
volume = {42},
year = {2007}
}
@article{Marino2009,
abstract = {Data races are one of the most common and subtle causes of pernicious concurrency bugs. Static techniques for preventing data races are overly conservative and do not scale well to large programs. Past research has produced several dynamic data race detectors that can be applied to large programs. They are precise in the sense that they only report actual data races. However, dynamic data race detectors incur a high performance overhead, slowing down a program's execution by an order of magnitude. In this paper we present LiteRace, a very lightweight data race detector that samples and analyzes only selected portions of a program's execution. We show that it is possible to sample a multithreaded program at a low frequency, and yet, find infrequently occurring data races. We implemented LiteRace using Microsoft's Phoenix compiler. Our experiments with several Microsoft programs, Apache, and Firefox show that LiteRace is able to find more than 70\% of data races by sampling less than 2\% of memory accesses in a given program execution.},
annote = {Try to find data races without sampling every memory access.

      },
author = {Marino, Daniel and Musuvathi, Madanlal and Narayanasamy, Satish},
file = {:home/sos22/papers/error-detecting/literace.pdf:pdf},
journal = {Conference on Programming Language Design and Implementation},
keywords = {concurrency bugs,dynamic data race detection,sampling},
number = {6},
pages = {9},
title = {{LiteRace: effective sampling for lightweight data-race detection}},
url = {http://portal.acm.org/citation.cfm?id=1542491},
volume = {44},
year = {2009}
}
@article{Tate2009,
abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
journal = {Annual Symposium on Principles of Programming Languages},
keywords = {compiler optimization,equality reasoning,intermediate representation},
number = {1},
pages = {12},
title = {{Equality saturation: a new approach to optimization}},
url = {http://portal.acm.org/citation.cfm?id=1480915},
volume = {44},
year = {2009}
}
@article{Dunlap2002,
abstract = {Current system loggers have two problems: they depend on the integrity of the operating system being logged, and they do not save sufficient information to replay and analyze attacks that include any non-deterministic events. ReVirt removes the dependency on the target operating system by moving it into a virtual machine and logging below the virtual machine. This allows ReVirt to replay the system's execution before, during, and after an intruder compromises the system, even if the intruder replaces the target operating system. ReVirt logs enough information to replay a long-term execution of the virtual machine instruction-by-instruction. This enables it to provide arbitrarily detailed observations about what transpired on the system, even in the presence of non-deterministic attacks and executions. ReVirt adds reasonable time and space overhead. Overheads due to virtualization are imperceptible for interactive use and CPU-bound workloads, and 13--58\% for kernel-intensive workloads. Logging adds 0--8\% overhead, and logging traffic for our workloads can be stored on a single disk for several months.},
author = {Dunlap, George W. and King, Samuel T. and Cinar, Sukru and Basrai, Murtaza A. and Chen, Peter M.},
file = {:home/sos22/papers/random/10.1.1.74.6084.pdf:pdf},
issn = {0163-5980},
journal = {ACM SIGOPS Operating Systems Review},
title = {{ReVirt:Â enabling intrusion analysis through virtual-machine logging and replay}},
year = {2002}
}
@article{Lefebvre2009,
abstract = {Program source is an intermediate representation of software; it lies between a developer's intention and the hardware's execution. Despite advances in languages and development tools, source itself and the applications we use to view it remain an essentially static representation of software, from which developers can spend considerable energy postulating actual behavior. Emerging techniques in execution logging promise to provide large shared repositories containing high-fidelity recordings of deployed, production software. Tralfamadore is a system that combines source and execution trace analysis to capitalize on these recordings, and to expose information from the "experience" of real execution within the software development environment, allowing developers to inform their understanding of source based on how it behaves during real execution.},
author = {Lefebvre, Geoffrey and Cully, Brendan and Feeley, Michael J. and Hutchinson, Norman C. and Warfield, Andrew},
journal = {European Conference on Computer Systems},
keywords = {debugging,dynamic analysis,program understanding,querying execution,trace analysis},
pages = {5},
title = {{Tralfamadore:Â unifying source code and execution experience}},
url = {http://portal.acm.org/citation.cfm?doid=1519065.1519087},
year = {2009}
}
@inproceedings{Park2009,
abstract = {Bug reproduction is critically important for diagnosing a production-run failure. Unfortunately, reproducing a concurrency bug on multi-processors (e.g., multi-core) is challenging. Previous techniques either incur large overhead or require new non-trivial hardware extensions. This paper proposes a novel technique called PRES (probabilis- tic replay via execution sketching) to help reproduce concurrency bugs on multi-processors. It relaxes the past (perhaps idealistic) ob- jective of reproducing the bug on the first replay attempt to signif- icantly lower production-run recording overhead. This is achieved by (1) recording only partial execution information (referred to as sketches) during the production run, and (2) relying on an in- telligent replayer during diagnosis time (when performance is less critical) to systematically explore the unrecorded non-deterministic space and reproduce the bug. With only partial information, our replayer may require more than one coordinated replay run to re- produce a bug. However, after a bug is reproduced once, PRES can reproduce it every time. We implemented PRES along with five different execution sketching mechanisms. We evaluated them with 11 representative applications, including 4 servers, 3 desktop/client applications, and 4 scientific/graphics applications, with 13 real world concurrency bugs of different types, including atomicity violations, order vio- lations and deadlocks. PRES (with synchronization or system call sketching) significantly lowered the production-run recording over- head of previous approaches (by up to 4416 times), while still re-producing most tested bugs in fewer than 10 replay attempts. More- over, PRES scaled well with the number of processors; PRESs feedback generation from unsuccessful replays is critical in bug re- production.},
address = {Big Sky, Montana},
annote = {The PRES paper.  Try to discover schedulings which are consistent with a sketch recording of a program run, using a somewhat under-specified search process.  Implemented on pin.  Results are very promising: many bugs reproduce quickly and run-time overheads are low.  Don't consider non-pin recorders, which is kind of a surprise; many of their experiments would go faster if they did that.

        
Observed overheads for their best case were on the order of 10-40\%, depending on application; would have been 5-20\% if they'd avoided pin.  Search times are very quick (O(minutes)), although for some reason they dropped a bunch of benchmarks for that section of the paper; hmm.},
author = {Park, S. and Zhou, Y. and Xiong, W. and Yin, Z. and Kaushik, R. and Lee, K.H. and Lu, S.},
booktitle = {22nd ACM Symposium on Operating Systems Principles},
file = {:home/sos22/papers/logging/sketches.pdf:pdf},
publisher = {ACM SIGOPS},
title = {{PRES: Probabilistic Replay with Execution Sketching on Multiprocessors}},
year = {2009}
}
@article{Ernst2007,
abstract = {Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often used in assert statements, documentation, and formal specifications. Examples include being constant (x = a), non-zero (x = 0), being in a range (a  x  b), linear relationships (y = ax +b), ordering (x  y), functions from a library (x = fn(y)), containment (x  y), sortedness (x is sorted), and many more. Users can extend Daikon to check for additional invariants. Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data. Daikon can detect invariants in C, C++, Java, and Perl programs, and in record-structured data sources; it is easy to extend Daikon to other applications. Invariants can be useful in program understanding and a host of other applications. Daikons output has been used for gen- erating test cases, predicting incompatibilities in component integration, automating theorem proving, repairing inconsistent data structures, and checking the validity of data streams, among other tasks. Daikon is freely available in source and binary form, along with extensive documentation, at http://pag.csail.mit.edu/daikon/.},
annote = {The Daikon paper.  Given a big trace of some values from a program, try to infer ``statistically justified'' invariants on the data (using significant tests).  Invariants come from a pre-coded list, and it pretty much just brute-forces the problem by trying each invariant in turn and reporting any which survive.

        
Cute, but not very cute.

      },
author = {Ernst, Michael D. and Perkins, Jeff H. and Guo, Philip J. and McCamant, Stephen and Pacheco, Carlos and Tschantz, Matthew S. and Xiao, Chen},
doi = {10.1016/j.scico.2007.01.015},
file = {:home/sos22/papers/modelling/daikon-tool-scp2007.pdf:pdf},
issn = {01676423},
journal = {Science of Computer Programming},
keywords = {daikon,dynamic analysis,dynamic invariant detection,inductive logic programming,inference,invariant,likely invariant,program,speci cation,speci cation mining,understanding},
number = {1-3},
pages = {35--45},
title = {{The Daikon system for dynamic detection of likely invariants}},
volume = {69},
year = {2007}
}
@article{Hangal2002a,
address = {New York, New York, USA},
author = {Hangal, Sudheendra and Lam, Monica S.},
doi = {10.1145/581376.581377},
file = {:home/sos22/papers/modelling/Diduce.pdf:pdf},
isbn = {158113472X},
journal = {Proceedings of the 24th international conference on Software engineering - ICSE '02},
pages = {291},
publisher = {ACM Press},
title = {{Tracking down software bugs using automatic anomaly detection}},
year = {2002}
}
@misc{Tassey2002,
author = {Tassey, G.},
booktitle = {National Institute of Standards and Technology RTI Project},
file = {:home/sos22/papers/debugging/nist-cost-of-bugs.pdf:pdf},
institution = {National Institute of Standards and Technology},
publisher = {Citeseer},
title = {{The economic impacts of inadequate infrastructure for software testing}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:The+Economic+Impacts+of+Inadequate+Infrastructure+for+Software+Testing\#0},
year = {2002}
}
@book{Peled,
abstract = {This volume presents a collection of methods for dealing with software reliability. Ideally, formal methods need to be intuitive to use, require a relatively brief learning period, and incur only small overhead to the development process. This book compares these varying methods and reveals their respective advantages and disadvantages, while also staying close to the dual themes of automata theory and logic. Topics and features:* Collects and compares the key software reliability methods currently in use: deductive verification, automatic verification, testing, and process algebra* Provides useful information suitable in the software selection process for a given project* Offers numerous exercises, projects, and running examples to facilitate learning formal methods and allows for \^{A}Â¿hands-on\^{A}Â¿ experience with these critical tools* Describes the mathematical principles supporting formal methods* Gives insights into new research directions in the field, as well as ways of developing new methods and/or adjusting existing ones.This volume can be used as an introduction to software methods techniques, a source for learning about various ways to enhance software reliability, and a guide to formal methods techniques. It is an essential resource for professionals and software engineers in R\&D departments in industry, using software reliability, program-modeling systems, and verification methods.},
author = {Peled, Doron},
editor = {Gries, David and Schneider, Fred},
isbn = {0-387-95106-7},
publisher = {Springer-Verlag},
title = {{Software reliability methods}},
url = {http://books.google.com/books?id=jJ-lTSlB71kC\&pgis=1},
year = {2001}
}
@article{GOULD1975,
author = {Gould, J},
doi = {10.1016/S0020-7373(75)80005-8},
file = {:home/sos22/papers/debugging/psychological-evidence-about-debugging.pdf:pdf;:home/sos22/papers/debugging/psychological-evidence-about-debugging.pdf:pdf},
issn = {00207373},
journal = {International Journal of Man-Machine Studies},
number = {2},
pages = {151--182},
title = {{Some psychological evidence on how people debug computer programs}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020737375800058},
volume = {7},
year = {1975}
}
@book{Maguire1993,
author = {Maguire, Steve},
isbn = {1556155514},
publisher = {Microsoft Press},
title = {{Writing Solid Code: Microsoft's Techniques for Developing Bug-Free C Programs (Microsoft Programming Series)}},
url = {http://www.amazon.com/Writing-Solid-Code-Microsofts-Programming/dp/1556155514},
year = {1993}
}
@inproceedings{Rinard2004,
abstract = {We present a newtechnique, failure-oblivious comput- ing, that enables servers to execute through memory er- rors without memory corruption. Our safe compiler for C inserts checks that dynamically detect invalid memory accesses. Instead of terminating or throwing an excep- tion, the generated code simply discards invalid writes and manufactures values to return for invalid reads, en- abling the server to continue its normal execution path. We have applied failure-oblivious computing to a set of widely-used servers from the Linux-based open- source computing environment. Our results show that our techniques 1) make these servers invulnerable to known security attacks that exploit memory errors, and 2) enable the servers to continue to operate successfully to service legitimate requests and satisfy the needs of their users even after attacks trigger their memory errors. We observed several reasons for this successful con- tinued execution. When the memory errors occur in ir- relevant computations, failure-oblivious computing en- ables the server to execute through the memory errors to continue on to execute the relevant computation. Even when the memory errors occur in relevant computations, failure-oblivious computing converts requests that trig- ger unanticipated and dangerous execution paths into an- ticipated invalid inputs, which the error-handling logic in the server rejects. Because servers tend to have small error propagation distances (localized errors in the com- putation for one request tend to have little or no effect on the computations for subsequent requests), redirect- ing reads that would otherwise cause addressing errors and discarding writes that would otherwise corrupt crit- ical data structures (such as the call stack) localizes the effect of the memory errors, prevents addressing excep- tions from terminating the computation, and enables the server to continue on to successfully process subsequent requests. The overall result is a substantial extension of the range of requests that the server can successfully pro- cess.},
annote = {The failure obliviousness paper.  Do some static checks on a C program, and then insert checks which make sure that you don't suffer a memory error.  Fix them by just making stuff up: invent something for reads, either go to lookaside or discard for write.  Aim is to achieve availability at the expense of reliability.  Synthesised values for reads are fairly stupid: always use the same sequence, which is modelled on distribution of values which get read from memory, independent of program and instruction.

        
Really very cute.

        
Performance hit is significant but tolerable for some applications: between 3\% and a factor of about ten, mostly O(tens of percent).  I'd guess that almost all of the hit is just from the error checker which they're using.

      },
author = {Rinard, M. and Cadar, C. and Dumitran, D. and Roy, D.M. and Leu, T. and {Beebee Jr}, W.S.},
booktitle = {Proceedings of the 6th conference on Symposium on Opearting Systems Design $\backslash$\& Implementation-Volume 6},
file = {:home/sos22/papers/random/failure-obliviousness.pdf:pdf},
pages = {2121},
publisher = {USENIX Association Berkeley, CA, USA},
title = {{Enhancing server availability and security through failure-oblivious computing}},
year = {2004}
}
@article{Guo2008,
abstract = {Library-based record and replay tools aim to reproduce an applicationâs execution by recording the results of se- lected functions in a log and during replay returning the results from the log rather than executing the functions. These tools must ensure that a replay run is identical to the record run. The challenge in doing so is that only invocations of a function by the application should be recorded, recording the side effects of a function call can be difficult, and not executing function calls during re- play, multithreading, and the presence of the tool may change the applicationâs behavior from recording to re- play. These problems have limited the use of such tools. R2 allows developers to choose functions that can be recorded and replayed correctly. Developers annotate the chosen functions with simple keywords so that R2 can handle callswith side effects andmultithreading.R2 gen- erates code for record and replay from templates, allow- ing developers to avoid implementing stubs for hundreds of functions manually. To track whether an invocation is on behalf of the application or the implementation of a selected function, R2 maintains a mode bit, which stubs save and restore. We have implemented R2 on Windows and anno- tated large parts (1,300 functions) of the Win32 API, and two higher-level interfaces (MPI and SQLite). R2 can replay multithreaded web and database servers that previous library-based tools cannot replay. By allowing developers to choose high-level interfaces, R2 can also keep recording overhead small; experiments show that its recording overhead forApache is approximately 10\%, that recording and replaying at the SQLite interface can reduce the log size up to 99\% (compared to doing so at theWin32 API), and that using optimization annotations for BitTorrent andMPI applications achieves log size re- duction ranging from 13.7\% to 99.4\%.},
annote = {Deterministic replay with help from the developer.  Developer chooses an interface at which to perform replay, and we then do the logging at that level.  Annotations are similar to Windows SAL.  Developer errors at this stage lead to application crashes during replay (but not record).  Logging must be on an arbitrary call graph cut.  No shared state across the cut (errno?).  Doesn't handle nondeterminism above the cut.  Argument on multithreading is basically to assume that the application got its locking right -> not immensely useful for concurrency bugs.  Asynchronous signals require manual annotations to replay correctly.  Assumes no wild pointers (although I suspect that most of the time you'll get away with it if there is one).  For complicated structures, developers must provide manual serialization stubs.

        
I can't think of many situations in which this would be useful.  It's basically developing debug infrastructure on the assumption that there are no bugs.

      },
author = {Guo, Z. and Wang, X. and Tang, J. and Liu, X. and Xu, Z. and Wu, M. and Kaashoek, M.F. and Zhang, Z.},
file = {:home/sos22/papers/logging/r2.pdf:pdf},
journal = {OSDI},
pages = {193--208},
title = {{R2: An application-level kernel for record and replay}},
url = {http://www.usenix.org/event/osdi08/tech/full\_papers/guo/guo\_html/},
year = {2008}
}
@inproceedings{Musuvathi2008,
abstract = {Concurrency is pervasive in large systems. Unexpected interferenceamong threads often results in âHeisenbugsâ that are extremely difficult to reproduce and eliminate. We have implemented a tool called CHESS for finding and reproducing such bugs.When attached to a program, CHESS takes control of thread scheduling and uses ef- ficient search techniques to drive the program through possible thread interleavings. This systematic explo- ration of program behavior enables CHESS to quickly uncover bugs that might otherwise have remained hid- den for a long time. For each bug, CHESS consistently reproduces an erroneous execution manifesting the bug, thereby making it significantly easier to debug the prob- lem. CHESS scales to large concurrent programs and has found numerous bugs in existing systems that had been tested extensively prior to being tested by CHESS. CHESS has been integrated into the test frameworks of many code bases inside Microsoft and is used by testers on a daily basis.},
annote = {CHESS system for driving thread scheduling into paths which are likely to trigger concurrency bugs.  Used in the real world (or at least in Microsoft).  Results impressive.  Seems to be more geared for deadlock bugs than strict races?  Depends on user being able to reproduce (most of) the initial environment; reasonable when the program is run under a test suite.

        
Thread rescheduling points mostly come from operating system primitives, but also use data race detector algorithms to find some ``likely'' places for data races to happen and insert rescheduling points there.

        
Schedule is an exploration of a Lamport-style happens-before graph.

        
Rather impressive.},
author = {Musuvathi, M. and Qadeer, S. and Ball, T. and Basler, G. and Nainar, P.A. and Neamtiu, I.},
booktitle = {Proceedings of the Eighth Symposium on Operating Systems Design and Implementation (OSDIâ08)},
file = {:home/sos22/papers/debugging/musuvathi.pdf:pdf},
keywords = {chase refs},
mendeley-tags = {chase refs},
pages = {267--280},
title = {{Finding and reproducing heisenbugs in concurrent programs}},
year = {2008}
}
@inproceedings{Wang2008,
abstract = {Deadlock is an increasingly pressing concern as the multicore revolution forces parallel programming upon the average programmer. Existing approaches to dead- lock impose onerous burdens on developers, entail high runtime performance overheads, or offer no help for unmodified legacy code. Gadara automates dynamic deadlock avoidance for conventional multithreaded pro- grams. It employs whole-program static analysis to model programs, and Discrete Control Theory to synthe- size lightweight, decentralized, highly concurrent logic that controls them at runtime. Gadara is safe, and can be applied to legacy code with modest programmer ef- fort. Gadara is efficient because it performs expensive deadlock-avoidance computations offline rather than on- line. We have implemented Gadara for C/Pthreads pro- grams. In benchmark tests, Gadara successfully avoids injected deadlock faults, imposes negligible to modest performance overheads (at most 18\%), and outperforms a software transactional memory system. Tests on a real application show that Gadara identifies and avoids both previously known and unknown deadlocks while adding performance overheads ranging from negligible to 10\%.},
annote = {Looking at classic mutex deadlocks.  Build model of how the system actually works, then tweak it to something which is provably deadlock-free, and then arrange to shepherd execution onto a deadlock-free schedule by playing with lock acquisition order etc.  Run-time hit is \~{}20\%.  Some theoretical results, but I don't really understand what they're saying.  Good performance depends on programmer-supplied annotations.  Can suffer deadlocks if you confuse the type analysis enough.

        
Model of world is based on Petri nets \& discrete control theory.  There's a lot of heavy-weight theory here which I don't know anything about.

        
A lot of this is just adding extra locks to guard against the usual lock inversion bugs.  

        
Evaluation is so-so but promising.

        
Cute, but I need to read more about the underlying theory.

      },
author = {Wang, Y. and Kelly, T. and Kudlur, M. and Lafortune, S. and Mahlke, S.},
booktitle = {8th Usenix symposium on Operating System Design and Implementation},
file = {:home/sos22/papers/debugging/wang.pdf:pdf},
keywords = {discrete control theory,ref-chain},
mendeley-tags = {discrete control theory,ref-chain},
pages = {281--294},
title = {{Gadara: Dynamic deadlock avoidance for multithreaded programs}},
url = {http://www.usenix.org/events/osdi08/tech/full\_papers/wang/wang\_html/},
year = {2008}
}
@inproceedings{Jula2008,
abstract = {Deadlock immunity is a property by which programs, once afflicted by a given deadlock, develop resistance against future occurrences of that and similar deadlocks. We describe a technique that enables programs to auto- matically gain such immunity without assistance from programmers or users. We implemented the technique for both Java and POSIX threads and evaluated it with several real systems, including MySQL, JBoss, SQLite, ApacheActiveMQ, Limewire, and Java JDK. The results demonstrate effectiveness against real deadlock bugs, while incurring modest performance overhead and scal- ing to 1024 threads.We therefore conclude that deadlock immunity offers programmers and users an attractive tool for coping with elusive deadlocks.},
annote = {Detect deadlocks and then alter future schedules to avoid them.  Need to see a deadlock once at runtime to avoid it.  Deadlocks are identified by ``signatures'': essentially a stack which is captured at deadlock time.  Threads get delayed at $\backslash$verb|lock()| time if letting them acquire the lock would cause some known deadlock signature to match the current world state.  This is somewhat more precise than just shoving in extra high-level locks, but that obviously means that you might need to see more instances to really fix the deadlock.

        
Signatures don't depend on data (EIPs only).

        
Can sometimes introduce starvation, but immunize against it in the same way as they immunize against deadlocks.

        
Needs a matching accuracy parameter, but there are some heuristics for this (and they look plausible).

        
Evaluation shows that the scheme is effective against at least some real-world deadlocks.  Overheads seem to be low; negligible on macrobenchmarks, and low tens of percent on microbenchmarks.  Memory overhead is a bit bigger.

        
Reasonable OS facility?

        
Rather cute, overall.

      },
author = {Jula, H. and Tralamazza, D. and Zamfir, C. and Candea, G.},
booktitle = {Proceedings of the 8th Symposium on Operating Systems Design and Implementation},
file = {:home/sos22/papers/debugging/jula.pdf:pdf},
pages = {295--308},
title = {{Deadlock immunity: Enabling systems to defend against deadlocks}},
year = {2008}
}
@article{Kavi2002,
author = {Kavi, K.M. and Moshtaghi, A. and Chen, D.J.},
file = {:home/sos22/papers/dct/modelling-multithreaded-programs-as-petri-nets.pdf:pdf},
journal = {International Journal of Parallel Programming},
keywords = {deadlocks,lost signals,petri nets,pthreads,race conditions},
number = {5},
pages = {353371},
publisher = {Springer},
title = {{Modeling multithreaded applications using Petri nets}},
url = {http://www.springerlink.com/index/T70831400773U350.pdf},
volume = {30},
year = {2002}
}
@inproceedings{Lu2008,
abstract = {The reality of multi-core hardware has made concurrent programs pervasive. Unfortunately, writing correct concurrent programs is difficult. Addressing this challenge requires advances in multiple directions, including concurrency bug detection, concurrent pro- gram testing, concurrent programming model design, etc. Design- ing effective techniques in all these directions will significantly benefit from a deep understanding of real world concurrency bug characteristics. This paper provides the first (to the best of our knowledge) com- prehensive realworld concurrency bug characteristic study. Specif- ically, we have carefully examined concurrency bug patterns, man- ifestation, and fix strategies of 105 randomly selected real world concurrency bugs from 4 representative server and client open- source applications (MySQL, Apache, Mozilla and OpenOffice). Our study reveals several interesting findings and provides use- ful guidance for concurrency bug detection, testing, and concurrent programming language design. Some of our findings are as follows: (1) Around one third of the examined non-deadlock concurrency bugs are caused by vio- lation to programmers order intentions, which may not be easily expressed via synchronization primitives like locks and transac- tional memories; (2) Around 34\% of the examined non-deadlock concurrency bugs involve multiple variables, which are not well addressed by existing bug detection tools; (3) About 92\% of the examined concurrency bugs can be reliably triggered by enforcing certain orders among no more than 4 memory accesses. This indi- cates that testing concurrent programs can target at exploring possi- ble orders among every small groups of memory accesses, instead of among all memory accesses; (4) About 73\% of the examined non-deadlock concurrency bugs were not fixed by simply adding or changing locks, and many of the fixes were not correct at the first try, indicating the difficulty of reasoning concurrent execution by programmers.},
author = {Lu, S. and Park, S. and Seo, E. and Zhou, Y.},
booktitle = {ASPLOS08},
file = {:home/sos22/papers/dct/p329-lu.pdf:pdf},
keywords = {bug character-,concurrency bug,concurrent program},
publisher = {ACM New York, NY, USA},
title = {{Learning from mistakes: a comprehensive study on real world concurrency bug characteristics}},
url = {http://portal.acm.org/citation.cfm?id=1353534.1346323},
year = {2008}
}
@inproceedings{Daran1996,
author = {Daran, M. and Th\'{e}venod-Fosse, P.},
booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
file = {:home/sos22/papers/debugging/p158-daran.pdf:pdf},
pages = {158171},
publisher = {ACM New York, NY, USA},
title = {{Software error analysis: a real case study involving real faults and mutations}},
url = {http://portal.acm.org/citation.cfm?id=229000.226313\&amp;type=series},
year = {1996}
}
@article{Soh1996,
author = {Soh, BC and Dillon, TS},
file = {:home/sos22/papers/debugging/hardware-fault-propagation.pdf:pdf},
journal = {Microelectronics Reliability},
number = {9},
pages = {1231--1235},
title = {{Hardware fault latency: model validation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/002627149500176X},
volume = {36},
year = {1996}
}
@article{Yount1996,
author = {Yount, C.R. and Siewiorek, D.P.},
doi = {10.1109/12.536231},
file = {:home/sos22/papers/debugging/rtl-fault-injection.pdf:pdf},
issn = {00189340},
journal = {IEEE Transactions on Computers},
number = {8},
pages = {881--891},
title = {{A methodology for the rapid injection of transient hardware errors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=536231},
volume = {45},
year = {1996}
}
@inproceedings{Lu2005,
author = {Lu, S. and Li, Z. and Qin, F. and Tan, L. and Zhou, P. and Zhou, Y.},
booktitle = {Workshop on the Evaluation of Software Defect Detection Tools},
file = {:home/sos22/papers/debugging/63-lu.pdf:pdf},
number = {3},
pages = {1--5},
publisher = {Citeseer},
title = {{Bugbench: Benchmarks for evaluating bug detection tools}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Bugbench:+Benchmarks+for+evaluating+bug+detection+tools\#0},
year = {2005}
}
@article{Beil2002,
abstract = {Text clustering methods can be used to structure large sets of text or hypertext documents. The well-known methods of text clustering, however, do not really address the special problems of text clustering: very high dimensionality of the data, very large size of the databases and understandability of the cluster description. In this paper, we introduce a novel approach which uses frequent item (term) sets for text clustering. Such frequent sets can be efficiently discovered using algorithms for association rule mining. To cluster based on frequent term sets, we measure the mutual overlap of frequent sets with respect to the sets of supporting documents. We present two algorithms for frequent term-based text clustering, FTC which creates flat clusterings and HFTC for hierarchical clustering. An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-the- art text clustering algorithms. Furthermore, our methods provide an understandable description of the discovered clusters by their frequent term sets.},
author = {Beil, Florian and Ester, Martin and Xu, Xiaowei},
journal = {International Conference on Knowledge Discovery and Data Mining},
keywords = {clustering,frequent item sets,text documents},
title = {{Frequent term-based text clustering}},
url = {http://portal.acm.org/citation.cfm?id=775047.775110},
year = {2002}
}
@inproceedings{Ashok2009,
abstract = {In large software development projects, when a programmer is assigned a bug to fix, she typically spends a lot of time searching (in an ad-hoc manner) for instances from the past where similar bugs have been debugged, analyzed and re- solved. Systematic search tools that allow the programmer to express the context of the current bug, and search through diverse data repositories associated with large projects can greatly improve the productivity of debugging. This paper presents the design, implementation and experience from such a search tool called DebugAdvisor. The context of a bug includes all the information a pro- grammer has about the bug, including natural language text, textual rendering of core dumps, debugger output etc. Our key insight is to allow the programmer to collate this en- tire context as a query to search for related information. Thus, DebugAdvisor allows the programmer to search us- ing a fat query, which could be kilobytes of structured and unstructured data describing the contextual information for the current bug. Information retrieval in the presence of fat queries and variegated data repositories, all of which contain a mix of structured and unstructured data is a challenging problem. We present novel ideas to solve this problem. We have deployed DebugAdvisor to over 100 users inside Microsoft. In addition to standard metrics such as precision and recall, we present extensive qualitative and quantitative feedback from our users.},
annote = {A way of intelligently mining PR databases etc. for bugs related to the current one.  ``fat queries'' include all available information (debugger logs, plaintext description of bug, etc.).  Data representation is one of list, multiset, weighted multiset, or dictionary, or some suitable combination of them.  Allows recursive types.  Mines VCS history and bug database to find people related to bits of source and existing bugs, and then tries to guess who is likely to know about the current one (relationship graph from BCT).  Uses probabilistic factor graphs for this.  Implemented on top of existing full-text indexing system by transforming documents so that the plain-text similarity of the output documents is the same as the typed similarity of the input.  This leads to a minor increase in the size of queries, and restricts the kinds of types which can be used.

        
Results are kind of middling; good enough to be useful, but nothing very exciting.  The idea is cute, though.

      },
author = {Ashok, B. and Joy, J. and Liang, H. and Rajamani, S.K. and Srinivasa, G. and Vangala, V.},
booktitle = {Proceedings of the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering on European software engineering conference and foundations of software engineering symposium},
file = {:home/sos22/papers/debugging/debugadvisorfse09.pdf:pdf},
keywords = {all or part of,debugging,factor graph,or hard copies of,permission to make digital,recommendation systems,search,this work for},
mendeley-tags = {factor graph},
pages = {373--382},
publisher = {ACM},
title = {{DebugAdvisor: a recommender system for debugging}},
url = {http://portal.acm.org/citation.cfm?id=1595766},
year = {2009}
}
@inproceedings{Schroeder2009,
author = {Schroeder, B. and Pinheiro, E. and Weber, W.D.},
booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
file = {:home/sos22/papers/hardware-errors/p193-schroeder.pdf:pdf},
keywords = {data cor-,dimm,dram,hard error,large-scale systems,memory,reliability,ruption,soft error},
pages = {193--204},
publisher = {ACM New York, NY, USA},
title = {{DRAM errors in the wild: a large-scale field study}},
url = {http://portal.acm.org/citation.cfm?id=1555372},
year = {2009}
}
@article{Shalizi2004,
abstract = {We present a new method for nonlinear prediction of discrete random sequences
under minimal structural assumptions. We give a mathematical construction for
optimal predictors of such processes, in the form of hidden Markov models. We
then describe an algorithm, CSSR (Causal-State Splitting Reconstruction), which
approximates the ideal predictor from data. We discuss the reliability of CSSR,
its data requirements, and its performance in simulations. Finally, we compare
our approach to existing methods using variable-length Markov models and
cross-validated hidden Markov models, and show theoretically and experimentally
that our method delivers results superior to the former and at least comparable
to the latter.},
arxivId = {cs/0406011v1},
author = {Shalizi, Cosma Rohilla and Shalizi, Kristina Lisa},
file = {:home/sos22/papers/modelling/0406011v1.pdf:pdf},
keywords = {Chaotic Dynamics,Data Analysis, Statistics and Probability,Learning,Statistics},
title = {{Blind Construction of Optimal Nonlinear Recursive Predictors for  Discrete Sequences}},
url = {http://arxiv.org/abs/cs.LG/0406011},
year = {2004}
}
@article{Shalizi2004a,
arxivId = {cs/0406011v1},
author = {Shalizi, Cosma Rohilla and Shalizi, Kristina Lisa},
file = {:home/sos22/papers/modelling/0406011v1.pdf:pdf},
keywords = {Chaotic Dynamics,Data Analysis, Statistics and Probability,Learning,Statistics},
title = {{Blind Construction of Optimal Nonlinear Recursive Predictors for  Discrete Sequences}},
url = {http://arxiv.org/abs/cs/0406011v1},
year = {2004}
}
@article{Cozzie2008,
abstract = {Because writing computer programs is hard, computer programmers are taught to use encapsulation and mod- ularity to hide complexity and reduce the potential for errors. Their programs will have a high-level, hierar- chical structure that reflects their choice of internal ab- stractions. We designed and forged a system, Laika, that detects this structure in memory using Bayesian unsu- pervised learning. Because almost all programs use data structures, their memory images consist of many copies of a relatively small number of templates. Given a mem- ory image, Laika can find both the data structures and their instantiations. We then used Laika to detect three common polymor- phic botnets by comparing their data structures. Because it avoids their code polymorphism entirely, Laika is extremely accurate. Finally, we argue that writing a data structure polymorphic virus is likely to be considerably harder than writing a code polymorphic virus.},
annote = {Use various statistical techniques to try to infer what the data structures inside a program look like.  Supposed to be able to fingerprint programs for virus scanning etc.  Results are so-so; clearly able to identify some structures, but not very reliable.  The structures they can infer are a little bit noddy, as well.  Need to use several heuristics to generate the model, and those aren't described in the paper.

        
Very strange writing style.},
author = {Cozzie, A. and Stratton, F. and Xue, H. and King, S.T.},
file = {:home/sos22/papers/modelling/cozzie.pdf:pdf},
journal = {Symposium on Operating Systems Design and Implementation (OSDI)},
pages = {255--266},
title = {{Digging for data structures}},
url = {http://www.usenix.org/event/osdi08/tech/full\_papers/cozzie/cozzie\_html/},
year = {2008}
}
@article{Rajlich2004,
author = {Rajlich, V.},
doi = {10.1109/COGINF.2004.1327473},
file = {:home/sos22/papers/debugging/psychology/cognitive\_models.pdf:pdf},
isbn = {0-7695-2190-8},
journal = {Proceedings of the Third IEEE International Conference on Cognitive Informatics, 2004.},
pages = {176--182},
publisher = {Ieee},
title = {{Cognitive process during program debugging}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1327473},
year = {2004}
}
@inproceedings{Prvulovic2003a,
abstract = {While removing software bugs consumes vast amounts of hu- man time, hardware support for debugging in modern computers remains rudimentary. Fortunately, we show that mechanisms for Thread-Level Speculation (TLS) can be reused to boost debug- ging productivity. Most notably, TLSs rollback capabilities can be extended to support rolling back recent buggy execution and repeating it as many times as necessary until the bug is fully char- acterized. These incremental re-executions are deterministic even in multithreaded codes. Importantly, this operation can be done automatically on the fly, and is compatible with production runs. As a specific implementation of a TLS-based debugging frame- work, we introduce ReEnact. ReEnact targets a particularly hairy class of bugs: data races in multithreaded programs. ReEnact ex- tends the communication monitoring mechanisms in TLS to also detect data races. It extends TLSs rollback capabilities to be able to roll back and deterministically re-execute the code with races to obtain the race signature. Finally, the signature is compared to a li- brary of race patterns and, if a match occurs, the execution may be repaired. Overall, ReEnact successfully detects, characterizes, and often repairs races automatically on the fly. Moreover, it is fully compatible with always-on use in production runs: the slowdown of race-free execution with ReEnact is on average only 5.8\%.},
annote = {Very, very close to what I want to do.

        
Use hardware thread-level speculation transactions to provide a roll-back and determinstic re-execution capability which can be used to get more information about the cause of failures.  Trying to do it in hardware complicates things quite a bit.

        
``Fix''es are very simple: just pattern match against a library of manually defined race patterns with fixes, and pick one, which probably works quite well in practice but lacks elegance.  When they fix, they just do it by tweaking the dynamic schedule at run time, which is quite sucky.

        
They seem to be able to roll back by on the order of a few tens of thousands of instructions, which sounds pretty adequate.  Runtime overhead is on the order of ten percent.

        
Interesting discussion of which kinds of bugs can be fixed by this approach, which will probably translate quite easily to SLI.

        
Summary: good idea, crippled by decision to do it all in hardware.

      },
author = {Prvulovic, M and Torrellas, J},
booktitle = {ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE},
file = {:home/sos22/papers/random/reenact.pdf:pdf},
pages = {110121},
publisher = {IEEE Computer Society},
title = {{ReEnact: Using thread-level speculation mechanisms to debug data races in multithreaded codes}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/ISCA.2003.1206993},
volume = {30},
year = {2003}
}
@article{Chandra2000,
author = {Lu, S. and Park, S. and Seo, E. and Zhou, Y.},
file = {:home/sos22/papers/random/chandra00.pdf:pdf},
journal = {ACM SIGARCH Computer Architecture News},
number = {1},
pages = {329--339},
publisher = {ACM},
title = {{Learning from mistakes: a comprehensive study on real world concurrency bug characteristics}},
url = {http://portal.acm.org/citation.cfm?id=1353534.1346323},
volume = {36},
year = {2008}
}
@inproceedings{Fidge1988,
author = {Fidge, C.J.},
booktitle = {Proceedings of the 11th Australian Computer Science Conference},
file = {:home/sos22/papers/vector-clocks/fidge88a.pdf:pdf},
number = {1},
pages = {56--66},
title = {{Timestamps in message-passing systems that preserve the partial ordering}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Timestamps+in+Message-Passing+Systems+That+Preserve+the+Partial+Ordering\#0},
volume = {10},
year = {1988}
}
@article{Helmbold1993,
author = {Helmbold, D.P. and McDowell, C.E. and Wang, J.-Z.},
doi = {10.1109/71.238303},
file = {:home/sos22/papers/vector-clocks/static\_analysis\_1.pdf:pdf},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
month = jul,
number = {7},
pages = {827--840},
title = {{Determining possible event orders by analyzing sequential traces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=238303},
volume = {4},
year = {1993}
}
@inproceedings{Krena2007,
abstract = {Testing of concurrent software is extremely difficult. Despite all the progress in the testing and verification technology, concurrent bugs, the most common of which are deadlocks and races, make it to the field. This paper describes a set of techniques, implemented in a tool called ConTest, allowing concurrent programs to self-heal at run-time. Concurrent bugs have the very desirable property for heal- ing that some of the interleaving produce correct results while in others bugs manifest. Healing concurrency prob- lems is about limiting, or changing the probability of in- terleaving, such that bugs will be seen less. When heal- ing concurrent programs, if a deadlock does not result from limiting the interleaving, we are sure that the result of the healed program could have been in the original program and therefore no new functional bug has been introduced. In this initial work which deals with different types of data races, we suggest three types of healing mechanisms: (1) changing the probability of interleaving by introducing sleep or yield statements or by changing thread priorities, (2) removing interleaving using synchronisation commands like locking and unlocking certain mutexes or waits and no- tifies, and (3) removing the result of bad interleaving by replacing the value of variables by the one that should have been taken. We also classify races according to the relevant healing strategies to apply.},
annote = {Use the Eraser algorithm to detect races, and then match the Java bytecode against a small number of statically-defined patterns, applying a fix for each one.  Fixes are along the lines of yield() or introducing a new lock.  Kind of assume that we have a single physical processor, and bugs are due to unfortunate OS-level thread preemptions; hmm.

        
Try to fix ``inherent races'' as well (e.g. blind writes to state variables).  Scheme they come up with is reasonable, if somewhat obvious, but doesn't appear to have been evaluated anywhere.

        
Not introducing new deadlocks is deferred to future work.  They want to use static analysis.

        
In fact, quite a lot of this is deferred to future work, if you read carefully enough.},
author = {Krena, B. and Letko, Z. and Tzoref, R. and Ur, S. and Vojnar, T.},
booktitle = {Proceedings of the 2007 ACM workshop on Parallel and distributed systems: testing and debugging},
file = {:home/sos22/papers/automatic\_bug\_fixing/p54-krena.pdf:pdf},
keywords = {ConTest,concurrency,self-healing,testing},
mendeley-tags = {ConTest},
pages = {64},
publisher = {ACM},
title = {{Healing data races on-the-fly}},
url = {http://portal.acm.org/citation.cfm?id=1273658},
year = {2007}
}
@article{Ratanaworabhan2008,
abstract = {This paper introduces ToleRace, a runtime system that allows programs to detect and even tolerate asymmetric data races. Asymmetric races are race conditions where one thread correctly acquires and releases a lock for a shared variable while another thread improperly accesses the same variable. ToleRace provides approximate isolation in the critical sections of lock-based parallel programs by creating a local copy of each shared variable when entering a critical section, operating on the local copies, and prop- agating the appropriate copies upon leaving the critical section. We start by characterizing all possible interleavings that can cause races and precisely describe the effect of ToleRace in each case. Then, we study the theoretical aspects of an oracle that knows exactly what type of interleaving has occurred. Finally, we present two software implementations of ToleRace and evaluate them on multithreaded applications from the SPLASH2 and PARSEC suites. Our implementation on top of a dynamic instrumentation tool, which works directly on executables and requires no source code modifications, incurs an overhead of a factor of two on aver- age. Manually adding ToleRace to the source code of these appli- cations results in an average overhead of 6.4 percent.},
address = {New York, New York, USA},
annote = {Basically, the other tolerace paper, but with differently incomprehensible maths.  A few more implementation details in this one.

        
Benefit over the Tech Report is marginal.},
author = {Ratanaworabhan, Paruj and Burtscher, Martin and Kirovski, Darko and Zorn, Benjamin and Nagpal, Rahul and Pattabiraman, Karthik},
doi = {10.1145/1504176.1504202},
file = {:home/sos22/papers/automatic\_bug\_fixing/p173-ratanaworabhan.pdf:pdf},
isbn = {9781605583976},
journal = {Proceedings of the 14th ACM SIGPLAN symposium on Principles and practice of parallel programming - PPoPP '09},
keywords = {dynamic,race detection and toleration,runtime support},
pages = {173},
publisher = {ACM Press},
title = {{Detecting and tolerating asymmetric races}},
url = {http://portal.acm.org/citation.cfm?doid=1504176.1504202},
year = {2008}
}
@article{Netzer1992,
author = {Netzer, R.H.B. and Miller, B.P.},
file = {:home/sos22/papers/vector-clocks/p74-netzer.pdf:pdf},
journal = {ACM Letters on Programming Languages and Systems (LOPLAS)},
number = {1},
pages = {74--88},
publisher = {ACM New York, NY, USA},
title = {{What are race conditions?: Some issues and formalizations}},
url = {http://portal.acm.org/citation.cfm?id=130623\&amp;dl=},
volume = {1},
year = {1992}
}
@inproceedings{Altekar2009,
abstract = {Reproducing bugs is hard. Deterministic replay systems ad- dress this problem by providing a high-fidelity replica of an original program run that can be repeatedly executed to zero-in on bugs. Unfortunately, existing replay systems for multiprocessor programs fall short. These systems either incur high overheads, rely on non-standard multiprocessor hardware, or fail to reliably reproduce executions. Their primary stumbling block is data races  a source of non- determinism that must be captured if executions are to be faithfully reproduced. In this paper, we present ODRa software-only replay sys- tem that reproduces bugs and provides low-overhead mul- tiprocessor recording. The key observation behind ODR is that, for debugging purposes, a replay system does not need to generate a high-fidelity replica of the original execution. Instead, it suffices to produce any execution that exhibits the same outputs as the original. Guided by this observa- tion, ODR relaxes its fidelity guarantees to avoid the problem of reproducing data-races altogether. The result is a sys- tem that replays real multiprocessor applications, such as Apache, MySQL, and the Java Virtual Machine, and pro- vides low record-mode overhead.},
annote = {Relaxed deterministic replay system which only bothers to make outputs reproducible, not all of the internal scheduling.  Overuse of clever-sounding words (e.g. determinant).  Core ideas are ``deterministic-run inference'', which searches the space of possible schedules, and relaxing the constistency model, which pretty much just Makes Shit Up for memory read values in order to make the schedule work, using some amount of theorem proving and abstract interpretation.

        
Paper focuses on giving debuggers a consistent record of what happened on a particular thread, not how multiple threads are interleaved.

        

        
FOO FOO FOO

      },
author = {Altekar, G. and Stoica, I.},
booktitle = {Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles},
file = {:home/sos22/papers/logging/altekar-sosp09.pdf:pdf},
keywords = {debugging,deterministic replay,inference,multicore},
pages = {193206},
publisher = {ACM},
title = {{ODR: output-deterministic replay for multicore debugging}},
year = {2009}
}
@inproceedings{Musuvathi2008a,
annote = {The CHESS paper.},
author = {Musuvathi, M. and Qadeer, S. and Ball, T. and Basler, G. and Nainar, P.A. and Neamtiu, I.},
booktitle = {Proceedings of the Eighth Symposium on Operating Systems Design and Implementation (OSDI08)},
file = {:home/sos22/papers/error-detecting/osdi2008-chess.pdf:pdf},
keywords = {CHESS},
mendeley-tags = {CHESS},
pages = {267280},
title = {{Finding and reproducing heisenbugs in concurrent programs}},
url = {http://research.microsoft.com/en-us/um/people/qadeer/abstracts/osdi08.chess.html},
year = {2008}
}
@book{Cunningham2008,
abstract = {Atomicity provides strong guarantees against errors caused by unanticipated thread interactions, but is difficult for programmers
 to implement with low-level concurrency primitives. With the introduction of multicore processors, the problems are compounded.
 Atomic sections are a high level language feature that programmers can use to designate the blocks of code that need to be
 free from unanticipated thread interactions, letting the language implementation handle the low-level details such as deadlock.
 From a language designerâs point of view, the challenge is to implement atomic sections without compromising performance.
 
 We propose an implementation of atomic sections that inserts locks transparently into object-oriented programs. The main advantages
 of our approach are: (1) We infer path expressions (that at run-time resolve to actual objects) for many more accesses in the atomic section than previous work
 could infer. (2) We use multi-granularity locking for guarding iterative traversals. (3) We ensure freedom from deadlock by
 rolling back the lock acquisition phase. (4) We release locks as early as possible. In summary, our approach uses a finer-grained
 locking discipline than previous lock inference techniques.},
address = {Berlin, Heidelberg},
author = {Cunningham, Dave and Gudka, Khilan and Eisenbach, Susan},
booktitle = {Compiler Construction},
doi = {10.1007/978-3-540-78791-4},
isbn = {978-3-540-78790-7},
pages = {276 -- 290},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Compiler Construction}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-78791-4},
volume = {4959},
year = {2008}
}
@article{Cherem2008,
abstract = {Atomic sections are a recent and popular idiom to support the development of concurrent programs. Updates performed within an atomic section should not be visible to other threads until the atomic section has been executed entirely. Traditionally, atomic sections are supported through the use of optimistic concurrency, either using a transactional memory hardware, or an equivalent software emulation (STM). This paper explores automatically supporting atomic sections using pessimistic concurrency. We present a system that combines compiler and runtime techniques to automatically transform programs written with atomic sections into programs that only use locking primitives. To minimize contention in the transformed programs, our compiler chooses from several lock granularities, using fine-grain locks whenever it is possible. This paper formally presents our framework, shows that our compiler is sound (i.e., it protects all shared locations accessed within atomic sections), and reports experimental results.},
author = {Cherem, Sigmund and Chilimbi, Trishul and Gulwani, Sumit},
issn = {0362-1340},
journal = {ACM SIGPLAN Notices},
keywords = {atomic sections,concurrency,static lock inference},
number = {6},
pages = {11},
title = {{Inferring locks for atomic sections}},
url = {http://portal.acm.org/citation.cfm?id=1375619},
volume = {43},
year = {2008}
}
@article{McCloskey2006,
abstract = {The movement to multi-core processors increases the need for simpler, more robust parallel programming models. Atomic sections have been widely recognized for their ease of use. They are simpler and safer to use than manual locking and they increase modularity. But existing proposals have several practical problems, including high overhead and poor interaction with I/O. We present pessimistic atomic sections, a fresh approach that retains many of the advantages of optimistic atomic sections as seen in "transactional memory" without sacrificing performance or compatibility. Pessimistic atomic sections employ the locking mechanisms familiar to programmers while relieving them of most burdens of lock-based programming, including deadlocks. Significantly, pessimistic atomic sections separate correctness from performance: they allow programmers to extract more parallelism via finer-grained locking without fear of introducing bugs. We believe this property is crucial for exploiting multi-core processor designs.We describe a tool, Autolocker, that automatically converts pessimistic atomic sections into standard lock-based code. Autolocker relies extensively on program analysis to determine a correct locking policy free of deadlocks and race conditions. We evaluate the expressiveness of Autolocker by modifying a 50,000 line high-performance web server to use atomic sections while retaining the original locking policy. We analyze Autolocker's performance using microbenchmarks, where Autolocker outperforms software transactional memory by more than a factor of 3.},
author = {McCloskey, Bill and Zhou, Feng and Gay, David and Brewer, Eric},
file = {:home/sos22/papers/random/p346-mccloskey.pdf:pdf},
journal = {Annual Symposium on Principles of Programming Languages},
keywords = {atomic,lock,pessimistic},
number = {1},
title = {{Autolocker:Â synchronization inference for atomic sections}},
url = {http://portal.acm.org/citation.cfm?id=1111068},
volume = {41},
year = {2006}
}
@article{Hicks,
author = {Hicks, M and Foster, JS and Pratikakis, P},
file = {::},
journal = {Citeseer},
title = {{Lock inference for atomic sections}},
url = {http://scholar.google.com/scholar?cluster=15211752039280312662\&hl=en\&as\_sdt=2000\#0}
}
@inproceedings{Ramalingam2009,
abstract = {In this paper, we focus on concurrent programs that use locks to achieve isolation of data accessed by critical sections of code. We present ISOLATOR, an algorithm that guarantees isolation for well-behaved threads of a program that obey a locking discipline even in the presence of ill-behaved threads that disobey the locking discipline. ISOLATOR uses code instrumentation, data replication, and virtual memory pro- tection to detect isolation violations and delays ill-behaved threads to ensure isolation. Our instrumentation scheme re- quires access only to the code of well-behaved threads. We have evaluated ISOLATOR on several benchmark programs and found that ISOLATOR can ensure isolation with reason- able runtime overheads. In addition, we present three general desideratasafety, isolation, and permissivenessfor any scheme that attempts to ensure isolation, and formally prove that ISOLATOR satisfies all of these desiderata.},
address = {Washington, DC},
annote = {Tries to make synchronisation-correct threads behave correctly even in the presence of non-synchronisation-correct threads.  Not obvious why that's useful.  Involves snapshotting lots of data when you acquire a lock, and then merging the results back in when you release it.  Non-synchronised threads are stopped by memory protection, and effectively made to acquire an implicit per-page lock.  Assumes synchronisation is at page granularity (and plays with the memory allocator to make that happen, at the expense of greater memory fragmentation).  Requires that the programmer provides a manually-specified locking discipline.  Assume access to source code, and requires you to use a special compiler for the synchronised threads.  Only evaluate with microbenchmarks, and present the results in the dumbest way imaginable, but overhead isn't obviously excessive.

        
Cute, but not obviously useful by itself.

      },
author = {Ramalingam, G. and Rajamani, S.K. and Ranganath, V.P. and Vaswani, K. and Rajamani, S.},
booktitle = {Asplos'09},
file = {:home/sos22/papers/automatic\_bug\_fixing/p181-rajamani.pdf:pdf},
keywords = {concurrency,isolation,memory protection},
pages = {181--192},
publisher = {ACM},
title = {{Isolator: Dynamically Ensuring Isolation in Concurrent Programs}},
year = {2009}
}
@article{Lucia2009,
abstract = {Writing shared-memory parallel programs is error-prone. Among the concurrency errors that programmers often face are atomicity violations, which are especially challenging. They hap- pen when programmers make incorrect assumptions about atomic- ity and fail to enclose memory accesses that should occur atomi- cally inside the same critical section. If these accesses happen to be interleaved with conflicting accesses from different threads, the program might behave incorrectly. Recent architectural proposals arbitrarily group consecutive dy- namic memory operations into atomic blocks to enforce memory or- dering at a coarse grain. This provides what we call implicit atom- icity, as the atomic blocks are not derived from explicit program annotations. In this paper, we make the fundamental observation that implicit atomicity probabilistically hides atomicity violations by reducing the number of interleaving opportunities between mem- ory operations. We then propose Atom-Aid, which creates implicit atomic blocks intelligently instead of arbitrarily, dramatically re- ducing the probability that atomicity violations will manifest them- selves. Atom-Aid is also able to report where atomicity violations might exist in the code, providing resilience and debuggability. We evaluate Atom-Aid using buggy code from applications including Apache, MySQL, and XMMS, showing that Atom-Aid virtually elim- inates the manifestation of atomicity violations.},
annote = {Some hardware already arbitrarily chunks memory accesses into atomic blocks, primarily for performance reasons, side effect is to hide races.  Want to extend that to deliberately hide as many bugs as possible.  Want to pre-emptively fix bugs, i.e. make it so that they never happen in the first place, rather than noticing them and then going back and fixing them up.

        
Implementation somewhat hardware-specific (BulkSC).  Assume availability of transactional memory.

        
There's some mildly dubious maths to demonstrate that fewer interleavings -> less probability of a race actually happening.  Duh.

        
Their intelligent fix is essentially to open a transaction just before the first instruction of a race, and then automatically commit it after some fixed number of instructions complete.  They have the multi-conflict heuristic, and the synchronisation-isn't-special rule.  They have a very lightweight race detector which piggy backs on the transactional memory protocol.

        
Eval is pretty decent, and shows some very good results.

        
A useful source of bugs.

        
Not a bad paper, overall, but I think the software approach which I'm thinking about is likely to work better, and certainly has more room for future work.

      },
author = {Lucia, Brandon and Devietti, Joseph and Ceze, Luis and Strauss, Karin},
doi = {10.1109/MM.2009.1},
file = {:home/sos22/papers/automatic\_bug\_fixing/lucia-atomaid.pdf:pdf},
issn = {0272-1732},
journal = {IEEE Micro},
month = jan,
number = {1},
pages = {73--83},
title = {{Atom-Aid: Detecting and Surviving Atomicity Violations}},
volume = {29},
year = {2009}
}
@inproceedings{Lu,
abstract = {Concurrency bugs are among the most difficult to test and diagnose of all software bugs. The multicore technology trend worsens this problem. Most previous concurrency bug detection work focuses on one bug subclass, data races, and neglects many other important ones such as atomicity violations, which will soon become increas- ingly important due to the emerging trend of transactional memory models. This paper proposes an innovative, comprehensive, invariant- based approach called AVIO to detect atomicity violations. Our idea is based on a novel observation called access interleaving in- variant, which is a good indication of programmers assumptions about the atomicity of certain code regions. By automatically ex- tracting such invariants and detecting violations of these invariants at run time, AVIO can detect a variety of atomicity violations. Based on this idea, we have designed and built two implementa- tions of AVIO and evaluated the trade-offs between them. The first implementation, AVIO-S, is purely in software, while the second, AVIO-H, requires some simple extensions to the cache coherence hardware. AVIO-S is cheaper and more accurate but incurs much higher overhead and thus more run-time perturbation than AVIO- H. Therefore, AVIO-S is more suitable for in-house bug detection and postmortem bug diagnosis, while AVIO-H can be used for bug detection during production runs. We evaluate both implementations of AVIO using large real- world server applications (Apache and MySQL) with six represen- tative real atomicity violation bugs, and SPLASH-2 benchmarks. Our results show that AVIO detects more tested atomicity viola- tions of various types and has 25 times fewer false positives than previous solutions on average.},
address = {San Jose},
annote = {Monitors programs while they're running their test suites, and finds all the bits of non-serialisibility (encode everything else in ``access interleaving invariants'').  Then assumes that those are the only allowed non-serialisability, and checks at run time.  Main implementation is in hardware, also have a software implementation using pin.

        
Rather cute, although I suspect that the sheer number of AI invariants would make it quite cumbersome in practice.

        
Bug source.},
author = {Lu, S and Tucek, J and Qin, F and Zhou, Y},
booktitle = {ASPLOS'06},
file = {:home/sos22/papers/error-detecting/asplos062-lu.pdf:pdf},
keywords = {atomicity violation,bug detection,concurrency bug,concurrent program,hardware support,program invariant},
publisher = {ACM},
title = {{AVIO: Detecting Atomicity Violations via Access Interleaving Invariants}},
url = {http://scholar.google.co.uk/scholar?hl=en\&lr=\&cluster=18377833533907816335\&um=1\&ie=UTF-8\&ei=HMUnS5TyHYr44Aahw8ChDQ\&sa=X\&oi=science\_links\&resnum=1\&ct=sl-allversions\&ved=0CA4Q0AIwAA\#1},
year = {2006}
}
@article{Wang,
abstract = {This paper describes the design and implementation of a research dynamic binary translation system, StarDBT, which runs many real-world applications. StarDBT is a multi-platform translation system that is capable of translating application level binaries on either Windows or Linux OSes. A system-level variant of StarDBT can also run on a bare machine by translating the whole system code. We evaluate performance of a user-mode system using both SPEC2000 and some challenging Windows applications. StarDBT runs the SPEC2000 benchmark competitively to other state-of-the-art binary translators. For Windows applications that are typically multi-threaded GUI-based interactive applications with large code footprint, the StarDBT system provides acceptable performance in many cases. However, there are important scenarios in which dynamic translation still incurs significant runtime overhead, raising issues for further research. The major overheads are caused by the translation overhead of large volume of infrequently-executed code and by the emulation overhead for indirect branches.},
annote = {Implemented a DBT from IA32 to AMD64 (why?).  Minimal rewriting; almost just macro expansion.  Seems a bit useless, really.},
author = {Wang, C and Hu, S and Kim, H and Nair, SR and Jr, M Breternitz and Ying, Z},
file = {:home/sos22/papers/dbt/fulltext.pdf:pdf},
journal = {LNCS},
number = {4697},
pages = {4--15},
title = {{StarDBT: An efficient multi-platform dynamic binary translation system}},
url = {http://www.springerlink.com/index/e7l5q774q3635780.pdf},
year = {2007}
}
@inproceedings{Sridhar,
abstract = {Dynamic translation is a general purpose tool used for instrument- ing programs at run time. Performance of translated execution re- lies on balancing the cost of translation against the benefits of any optimizations achieved, and many current translators perform sub- stantial rewriting during translation in an attempt to reduce execu- tion time. Our results show that these optimizations offer no sig- nificant benefit even when the translated program has a small, hot working set. When used in a broader range of applications, such as ubiquitous policy enforcement or penetration detection, transla- tor performance cannot rely on the presence of a hot working set to amortize the cost of translation. A simpler, more maintainable, adaptable, and smaller translator appears preferable to more com- plicated designs in most cases. HDTrans is a light-weight dynamic instrumentation system for the IA-32 architecture that uses some simple and effective trans- lation techniques in combination with established trace lineariza- tion and code caching optimizations. We present an evaluation of translation overhead under both benchmark and less idealized con- ditions, showing that conventional benchmarks do not provide a good prediction of translation overhead when used pervasively. A further contribution of this paper is an analysis of the effec- tiveness of post-link static pre-translation techniques for overhead reduction. Our results indicate that static pre-translation is effective only when expensive instrumentation or optimization is performed.},
annote = {Nice prior work section.  Not quite sure what the novel bit was; all of the ideas seem to be taken from elsewhere (although the work certainly needed doing).  Try to avoid rewriting as much as possible, to keep translation overhead down.  Get the impression that the thing's somewhat incomplete.  Show that, for this technique, a persistent cache isn't immensely useful, but completely fail to convince that it isn't useful for other algorithms, or that combining a persistent cache with e.g. valgrind wouldn't allow VG to catch up with them in the places where they win.  Unconvincing.},
author = {Sridhar, S and Shapiro, JS and Northup, E and Bungale, PP},
booktitle = {VEE'06},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sridhar et al. - Unknown - HDTrans An Open Source, Low-Level Dynamic Instrumentation System.pdf:pdf},
publisher = {ACM},
title = {{HDTrans: An Open Source, Low-Level Dynamic Instrumentation System}},
url = {http://scholar.google.co.uk/scholar?hl=en\&lr=\&cluster=9111823888008694139\&um=1\&ie=UTF-8\&ei=p2gqS\_K-MZGr4QaN4JyUCQ\&sa=X\&oi=science\_links\&resnum=2\&ct=sl-allversions\&ved=0CBAQ0AIwAQ\#3},
year = {2006}
}
@article{Reddi2005,
abstract = {Dynamic code transformation systems (DCTS) can broadly be grouped into three distinct categories: opti- mization, translation and instrumentation. All of these face the critical challenge of minimizing the overhead in- curred during transformation since their execution is in- terleaved with the execution of the application itself. The common DCTS tasks incurring overhead are the iden- tification of frequently executed code sequences, costly analysis of program information, and run-time creation (writing) of new code sequences. The cost of such work is amortized by the repeated execution of the transformed code. However, as these steps are applied to all general code regions (regardless of their execution frequency and characteristics), there is substantial overhead that im- pacts the applicationâs performance. As such, it is chal- lenging to effectively deploy dynamic transformation un- der fixed performance constraints. This paper explores a technique for eliminating the overhead incurred by ex- ploiting persistent application execution characteristics that are shared across different application invocations. This technique is implemented and evaluated in Pin, a dynamic instrumentation engine. This version of Pin is referred to as Persistent Pin (PPin). Initial PPin ex- perimental results indicate that using information from prior runs can reduce dynamic instrumentation overhead of SPEC applications by as much as 25\% and over 90\% for everyday applications like web browsers, display ren- dering systems, and spreadsheet programs.},
annote = {Discuss ways of making a DBT code cache persistent, mostly in the context of Pin.  Show that benefits are greater for real-world apps than for micro-benchmarks.  Nothing particularly exciting here.

      },
author = {Reddi, VJ and Connors, D and Cohn, RS},
file = {:home/sos22/papers/dbt/p69-reddi.pdf:pdf;::},
journal = {ACM SIGARCH Computer Architecture News},
number = {5},
title = {{Persistence in dynamic code transformation systems}},
url = {http://portal.acm.org/citation.cfm?id=1127577.1127591},
volume = {33},
year = {2005}
}
@article{Allan2005,
author = {Allan, Chris and Tibble, Julian and Avgustinov, Pavel and Christensen, Aske Simon and Hendren, Laurie and Kuzins, Sascha and Lhot\'{a}k, OndÅej and de Moor, Oege and Sereni, Damien and Sittampalam, Ganesh},
doi = {10.1145/1103845.1094839},
file = {:home/sos22/papers/modelling/tracematches.pdf:pdf},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {aspect-oriented programming,program monitoring},
month = oct,
number = {10},
pages = {345},
title = {{Adding trace matching with free variables to AspectJ}},
url = {http://portal.acm.org/citation.cfm?doid=1103845.1094839},
volume = {40},
year = {2005}
}
@article{Srivastava1994,
abstract = {ATOM (Analysis Tools with OM) is a single framework for building a wide range of customized program analysis tools. It provides the common infrastructure present in all code-instrumenting  tools; this is the difficult and time-consuming part. The user simply defines the tool-specific details in instrumentation and analysis routines. Building a basic block counting tool like Pixie with ATOM requires only a page of code.

ATOM, using OM link-time technology, organizes the final executable such that the application program and userâs analysis routines run in the same address space. Information is directly passed from the application program to the analysis routines through simple procedure calls instead of inter-process communication or files on disk. ATOM takes care that analysis routines do not interfere with the programâs execution, and precise information about the program is presented to the analysis routines at all times. ATOM uses no simulation or interpretation.

ATOM has been implemented on the Alpha AXP under OSF/1. It is efficient and has been used to build a diverse set of tools for basic block counting, memory recording, pipeline simulation, profiling, dynamic instruction and data cache simulation, evaluating branch prediction, and instruction scheduling.},
annote = {Static binary translation tool for the Alpha.  Unclear how well it would work with more evil architectures like x86.  Moderately cute trick of inlining analysis into translated binary (inspiration for pin).  Runtime overhead predictably very low.

        
Cute, but not particularly exciting (at least not nowadays; might have been in 1994).

      },
author = {Srivastava, Amitabh and Eustace, Alan and Equipment, Digital and Research, Western and Alto, Palo},
file = {:home/sos22/papers/dbt/p196-srivastava.pdf:pdf},
journal = {SIGPLAN},
number = {6},
pages = {196--205},
publisher = {ACM},
title = {{ATOM: A system for building customized program analysis tools}},
volume = {94},
year = {1994}
}
@inproceedings{Letko2008,
abstract = {The paper proposes a novel algorithm called AtomRace for a dynamic detection of data races. Data races are detected as a special case of atomicity violations on atomic sections specially defined to span just particular read/write instruc- tions and the transfer of control to and from them. A key ingredient allowing AtomRace to efficiently detect races on such short atomic sections is a use of techniques for a care- ful injection of noise into the scheduling of the monitored programs. The approach is very simple, fully automated, avoids false alarms, and allows for a lower overhead and bet- ter scalability than many other existing dynamic data race detection algorithms. We illustrate these facts by a set of experiments with a prototype implementation of AtomRace. Further, AtomRace can also be applied to detect atomicity violations on more general atomic sections than those used for the data race detection. They can be defined by the user or obtained by some static analysis.},
annote = {Terrible paper (structure, written English, choice of what to cover), but underlying idea is a bit better.  Somewhat Java-specific.  Detection algorithm is pretty much to turn every memory access event into a small window and then to detect if any windows overlap, with a noise injection system to increase the probability of overlap.  Straightforward extension to ``proper'' critical sections.  Use various (obvious) heuristics to guess at missing critical sections.  Attempt to auto-heal violations of inferred critical sections by just turning them into real critical sections.},
author = {Letko, Z. and Vojnar, T. and K$\backslash$vrena, B.},
booktitle = {Proceedings of the 6th workshop on Parallel and distributed systems: testing, analysis, and debugging},
file = {:home/sos22/papers/automatic\_bug\_fixing/a7-letko.pdf:pdf},
pages = {7},
publisher = {ACM},
title = {{AtomRace: data race and atomicity violation detector and healer}},
url = {http://portal.acm.org/citation.cfm?id=1390841.1390848},
year = {2008}
}
@article{Artho2003a,
abstract = {Data races are a common problem in concurrent programming. Ex- perience shows that the notion of data race is not powerful enough to capture certain types of inconsistencies occurring in practice. In this paper we investigate data races on a higher abstraction layer. This enables us to detect inconsistent uses of shared variables, even if no classical race condition occurs. For example, a data structure representing a coordinate pair may have to be treated atomically. By lifting the meaning of a data race to a higher level, such problems can now be covered. The paper defines the concepts view and view consistency to give a notation for this novel kind of property. It describes what kinds of errors can be detected with this new definition, and where its limitations are. It also gives a formal guideline for using data structures in a multi-threaded environment.},
annote = {Lots of very tedious verbiage telling us that low-level data race freedom is insufficient.  No shit.  Unclear what they propose instead, though; there's a lot of psuedo-maths disguising it.  Doesn't look like it's worth picking apart.  I don't think they really understood it when they wrote the paper; might explain why they couldn't get it published, although not why people seem to be citing it anyway.},
author = {Artho, Cyrille and Havelund, Klaus and Biere, Armin},
file = {:home/sos22/papers/beastiary/artho03highlevel.pdf:pdf},
journal = {Software Testing Verification and Reliability},
number = {4},
pages = {207--227},
publisher = {Citeseer},
title = {{High-level data races}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.45\&amp;rep=rep1\&amp;type=pdf},
volume = {13},
year = {2003}
}
@misc{Kirovski2007,
abstract = {Because races represent a hard-to-manage class of errors in concurrent programs, numerous approaches to detect them have been proposed and evaluated. We consider specifically asymmetric races, a subclass of race conditions, where a programmers thread correctly acquires and releases a lock for a specific variable, while another thread causes a race by improperly accessing this variable. We introduce ToleRace, an oracle that allows programs to either tolerate or detect asymmetric races based on local replication of shared state. ToleRace provides an approximation of atomicity in critical sections by creating local copies of shared variables when a critical section is entered and propagating the appropriate copy when the critical section is exited. We characterize the possible interleavings that can cause races and precisely describe the effect of ToleRace in each of these cases. We evaluate the theoretical aspects of the oracle and note that it could be implemented in hardware and/or software within a favorable range of overhead-to-benefit scenarios.},
address = {Redmond},
annote = {Only consider asymmetric races (i.e. one thread holds the ``right'' lock, and another one doesn't).  Works by duplicating data on entry to critical section and then merging at end.  Require the programmer to manually specify the lock <-> shared data binding.

        
A lot like ISOLATOR, really.  I'm not sure which one I prefer; this one has more scary-looking theory, but I didn't look at it closely enough to say whether it's any good.},
author = {Kirovski, Darko and Zorn, Benjamin and Nagpal, Rahul and Pattabiraman, Karthik},
file = {:home/sos22/papers/error-detecting/TR-2007-122.pdf:pdf;:home/sos22/papers/error-detecting/TR-2007-122.pdf:pdf},
institution = {Microsoft Research},
keywords = {tolerace},
mendeley-tags = {tolerace},
title = {{An Oracle for Tolerating and Detecting Asymmetric Races}},
year = {2007}
}
@article{Pan2005,
abstract = {Binary instrumentation has been widely used to observe dynamic program behavior, but current binary instrumentation systems do not allow the tool writer to alter the program execution path. This paper introduces some simple and general mechanisms for a binary instrumentation infrastructure to provide control over the application's execution path, allowing tools to replay or skip parts of the application, and to start or switch between threads. Specifically, the technique provides the following three functionalities for both single-threaded and multi-threaded applications: (1) checkpointing the execution state, (2) resuming execution at a checkpoint, and (3) starting execution at an arbitrary point in the program with a specified architectural state. We describe our implementation of these functionalities in Pin, a dynamic binary instrumentation infrastructure from Intel [5]. We demonstrate the usefulness of our mechanism by describing several binary instrumentation tools that have been built using this interface, including a transactional memory model and a thread scheduler.},
author = {Pan, Heidi and Asanovi\'{c}, Krste and Cohn, Robert and Luk, Chi-Keung},
issn = {0163-5964},
journal = {ACM SIGARCH Computer Architecture News},
number = {5},
title = {{Controlling program execution through binary instrumentation}},
url = {http://portal.acm.org/citation.cfm?id=1127577.1127587},
volume = {33},
year = {2005}
}
@article{Shavit1997,
address = {Ottowa},
author = {Shavit, N and Touitou, D},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shavit, Touitou - 1997 - Software transactional memory.pdf:pdf},
journal = {Distributed Computing},
number = {2},
pages = {99--116},
publisher = {ACM},
title = {{Software transactional memory}},
volume = {10},
year = {1997}
}
@inproceedings{Qin2007,
abstract = {Many applications demand availability. Unfortunately, software failures greatly reduce system availability. Prior work on surviving software failures suffers from one or more of the following limita- tions: Required application restructuring, inability to address deter- ministic software bugs, unsafe speculation on program execution, and long recovery time. This paper proposes an innovative safe technique, called Rx, which can quickly recover programs from many types of software bugs, both deterministic and non-deterministic. Our idea, inspired from allergy treatment in real life, is to rollback the program to a recent checkpoint upon a software failure, and then to re-execute the program in a modified environment. We base this idea on the observation that many bugs are correlated with the execution envi- ronment, and therefore can be avoided by removing the allergen from the environment. Rx requires few to no modifications to ap- plications and provides programmers with additional feedback for bug diagnosis. We have implemented Rx on Linux. Our experiments with four server applications that contain six bugs of various types show that Rx can survive all the six software failures and provide transparent fast recovery within 0.017-0.16 seconds, 21-53 times faster than the whole program restart approach for all but one case (CVS). In contrast, the two tested alternatives, a whole program restart ap- proach and a simple rollback and re-execution without environmen- tal changes, cannot successfully recover the three servers (Squid, Apache, and CVS) that contain deterministic bugs, and have only a 40\% recovery rate for the server (MySQL) that contains a non- deterministic concurrency bug. Additionally, Rxs checkpointing system is lightweight , imposing small time and space overheads.},
address = {Brighton},
annote = {Tries to hide bugs by noticing when they happen, and then rolling back and retrying with a slightly modified environment.  They claim to be ``safe'', but only by redefining the word.
        
Basic approach is the same as mine: run the program taking regular snapshots, and then following a bug roll back and try running forwards in a slightly different way.  The slight differences are pretty coarse, though: the only thread-related one is to change the scheduling parameters.  There's no real attempt to figure out an actual ``fix'' for the bug, it just makes it go away this time.  It might well reproduce later, in which case RX will need to go and re-do its search.
        
Replay is deliberately not fully deterministic, since they're just trying to avoid the problem this time around, and so it's actively helpful if the scheduling is slightly different.
        
Some of their recovery schemes require knowledge of the application-level protocol (and at least one is unsafe).  Most of them don't, though.  Client-transparency also requires application-level protocol knowledge.
        
Performance is pretty good: recovery usually within a few tens of milliseconds.  Eat a few MB/s or so on checkpoints during normal operation; causes a few percent hit on actual performance.},
author = {Qin, F and Tucek, J and Zhou, Y and Sundaresan, J},
booktitle = {SOSP},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qin et al. - 2007 - Rx Treating bugs as allergiesâa safe method to survive software failures.pdf:pdf},
keywords = {Availability,Bug,Reliability,Software Failure},
publisher = {ACM},
title = {{Rx: Treating bugs as allergiesa safe method to survive software failures}},
year = {2005}
}
@inproceedings{Zamfir2010,
abstract = {Debugging real systems is hard, requires deep knowledge of the code, and is time-consuming.Bug reports rarely provide sufficient information, thus forcing developers to turn into detectives searching for an explanation of how the program could have arrived at the reported failure point. Execution synthesis is a technique for automating this de- tective work: given a program and a bug report, it automat- ically produces an execution of the program that leads to the reported bug symptoms. Using a combination of static analysis and symbolic execution, it âsynthesizesâ a thread schedule and various required programinputs that cause the bug to manifest. The synthesized execution can be played back deterministically in a regular debugger, like gdb. This is particularly useful in debugging concurrency bugs. Our technique requires no runtime tracing or program modifications, thus incurring no runtime overhead and being practical for use in production systems.We evaluate ESDâ a debugger based on execution synthesisâon popular soft- ware (e.g., the SQLite database, ghttpdWeb server,HawkNL network library, UNIX utilities): starting from mere bug re- ports, ESD reproduces on its own several real concurrency and memory safety bugs in less than three minutes.},
address = {Paris},
author = {Zamfir, C. and Candea, G.},
booktitle = {European Conference on Computer Systems},
file = {:home/sos22/papers/debugging/esd.pdf:pdf;:home/sos22/papers/debugging/esd.pdf:pdf},
publisher = {ACM},
title = {{Execution Synthesis: A Technique for Automated Software Debugging}},
year = {2010}
}
@article{Perkins2009,
abstract = {We present ClearView, a system for automatically patching errors in deployed software. ClearView works on stripped Windows x86 binaries without any need for source code, debugging information, or other external information, and without human intervention. ClearView (1) observes normal executions to learn invariants thatcharacterize the application's normal behavior, (2) uses error detectors to distinguish normal executions from erroneous executions, (3) identifies violations of learned invariants that occur during erroneous executions, (4) generates candidate repair patches that enforce selected invariants by changing the state or flow of control to make the invariant true, and (5) observes the continued execution of patched applications to select the most successful patch. ClearView is designed to correct errors in software with high availability requirements. Aspects of ClearView that make it particularly appropriate for this context include its ability to generate patches without human intervention, apply and remove patchesto and from running applications without requiring restarts or otherwise perturbing the execution, and identify and discard ineffective or damaging patches by evaluating the continued behavior of patched applications. ClearView was evaluated in a Red Team exercise designed to test its ability to successfully survive attacks that exploit security vulnerabilities. A hostile external Red Team developed ten code injection exploits and used these exploits to repeatedly attack an application protected by ClearView. ClearView detected and blocked all of the attacks. For seven of the ten exploits, ClearView automatically generated patches that corrected the error, enabling the application to survive the attacks and continue on to successfully process subsequent inputs. Finally, the Red Team attempted to make Clear-View apply an undesirable patch, but ClearView's patch evaluation mechanism enabled ClearView to identify and discard both ineffective patches and damaging patches.},
author = {Perkins, Jeff H. and Kim, Sunghun and Larsen, Sam and Amarasinghe, Saman and Bachrach, Jonathan and Carbin, Michael and Pacheco, Carlos and Sherwood, Frank and Sidiroglou, Stelios and Sullivan, Greg and Wong, Weng-Fai and Zibin, Yoav and Ernst, Michael D. and Rinard, Martin},
journal = {ACM Symposium on Operating Systems Principles},
keywords = {self healing},
title = {{Automatically patching errors in deployed software}},
url = {http://portal.acm.org/citation.cfm?id=1629575.1629585},
year = {2009}
}
@inproceedings{Diniz1996,
abstract = {Atomic operations are a key primitive in parallel computing systems. The standard implementation mechanism for atomic operations uses mutual exclusion locks. In an object-based programming system the natural granularity is to give each object its own lock. Each operation can then make its execution atomic by acquiring and releasing the lock for the object that it accesses. But this fine lock granularity may have high synchronization overhead. To achieve good performance it may be necessary to reduce the overhead by coarsening the granularity at which the computation locks objects.

In this paper we describe a static analysis technique --- lock coarsening --- designed to automatically increase the lock granularity in object-based programs with atomic operations. We have implemented this technique in the context of a parallelizing compiler for irregular, object-based pro- grams. Experiments show these algorithms to be effective in reducing the lock overhead to negligible levels. },
author = {Diniz, Pedro and Rinard, Martin},
booktitle = {LCPC},
file = {:home/sos22/papers/rinard/lcpc96.pdf:pdf},
title = {{Lock Coarsening : Eliminating Lock Overhead in Automatically Parallelized Object-Based Programs}},
year = {1996}
}
@inproceedings{Diniz1997,
abstract = {As parallel machines become part of the mainstream computing environment, compilers will need to apply synchronization optimizations to deliver e cient parallel software.  new distributed computing environment with unprecedented This paper describes a new framework for synchronization optimizations and a new set of transformations for programs that implement critical sections using mutual exclusion locks. These transformations allow the compiler to move constructs that acquire and release locks both within and between procedures and to eliminate acquire and release constructs.                                                        The paper also presents a new synchronization algorithm, lock elimination, for reducing synchronization overhead. This optimization locates computations that repeatedly acquire and release the same lock, then uses the transformations to obtain equivalent computations that acquire and release the lock only once. Experimental results from a parallelizing compiler for object-based programs illustrate the practical utility of this optimization. For three benchmark programs the optimization dramatically reduces the number of times the computations acquire and release locks, which significantly reduces the amount of time processors spend acquiring and releasing locks. For one of the three benchmarks, the optimization always signi cantly improves the overall performance. Depending on the number of processors executing the computation, the optimized version runs between 2.11 and 1.83 times faster than the unoptimized version. For one of the other benchmarks, the optimized version runs between 1.13 and 0.96 times faster than the unoptimized version, with a mean of 1.08 times faster. For the nal benchmark, the optimization reduces the overall performance.              },
author = {Diniz, Pedro and Rinard, Martin},
booktitle = {POPL},
file = {:home/sos22/papers/rinard/popl97.pdf:pdf},
publisher = {ACM},
title = {{Synchronization Transformations for Parallel Computing Computation}},
year = {1997}
}
@inproceedings{Rugina1999,
abstract = {This paper presents a novel interprocedural, flow-sensitive, and context-sensitive pointer analysis algorithm for multi-threaded programs that may concurrently updated shared pointers.  For each pointer and each program point, the algorithm computes a conservative approximation of the memory locations to which that pointer may point.  The algorithm correctly handles a full range of constructs in multi-threaded programs, including recursive functions, function pointers, structures, arrays, nested structures and arrays, pointer arithmetic, casts between pointer variables of different types, heap and stack allocated memory, shared global variables, and thread-private global variables.

We have implemented the algorithm in the SUIF compiler system and used the implementation to analyze a sizable set of multithreaded programs written in the Cilk multi-threaded programming language.  Our experimental resuilts show that the analysis has good precision and converges quickly for our set of Cilk programs.
},
author = {Rugina, Radu and Rinard, Martin},
booktitle = {PLDI},
doi = {10.1145/301631.301645},
file = {:home/sos22/papers/rinard/pldi99.full.pdf:pdf},
issn = {03621340},
month = may,
number = {5},
publisher = {ACM},
title = {{Pointer analysis for multithreaded programs}},
url = {http://portal.acm.org/citation.cfm?doid=301631.301645},
volume = {34},
year = {1999}
}
@article{Salcianu2001,
abstract = {This paper presents a new combined pointer and escape analsysis for multithreaded programs.  The algorithm uses a new abstraction called parallel interaction graphs to analyze the interactions between threads and extract precise points-to, escape, and action ordering information for objects accessed by multiple threads.  The analysis is compositional, analyzing each method or thread once to extarct a parameterized analysis result that can be specialized for use in any context.  It is also capable of analyzing programs that use the unstructured form of multithreading present in languages such as Java and standard thread packages such as POSIX threads.

We have implementede the analysis in the MIT Flex compiled for Java and used the extracted information to 1) verify that the programs correctly use region-based allocation constructs, 2) eliminated synamic checks associated with the use of regions, and 3) eliminate unnecessary synchronization.  Our experimental results show that analyzing the interactions between threads significantly increases the effectiveness of the region analysis and region check elinination, but has little effect for synchronization elimination.
},
author = {Salcianu, Alexandru and Rinard, Martin},
file = {:home/sos22/papers/rinard/ppopp01.pdf:pdf},
journal = {ACM SIGPLAN Notices},
number = {7},
pages = {23},
publisher = {ACM},
title = {{Pointer and escape analysis for multithreaded programs}},
url = {http://portal.acm.org/citation.cfm?id=568014.379553},
volume = {36},
year = {2001}
}
@article{Rinard1999,
abstract = {As shared-memory multiprocessors become the dominant commodity source of computation, parallelizing compilers must support mainstream computations that manipulate irregular, pointer-based data structures such as lists, trees and graphs. Our experience with a parallelizing compiler for this class of applications shows that their synchronization requirements differ significantly from those of traditional parallel computations. Instead of coarse-grain barrier synchronization, irregular computations require synchronization primitives that support efficient fine-grain atomic operations. 
The standard implementation mechanism for atomic operations uses mutual exclusion locks. But the overhead of acquiring and releasing locks can reduce the performance. Locks can also consume significant amounts of memory. Optimisitic synchronization primitives such as load linked/store conditional are an attractive alternative. They require no additional memory and eliminate the use of heavyweight blocking synchronization constructs. 
This paper presents our experience using optimistic synchronization to implement fine-grain atomic operations in the context of a parallelizing compiler for irregular object-based programs. We have implemented two versions of the compiler. One version generates code that uses mutual exclusion locks to make operations execute atomically. The other version uses optimistic synchronization. 
},
author = {Rinard, Martin C.},
doi = {10.1145/329466.329486},
file = {:home/sos22/papers/rinard/ppopp97.pdf:pdf},
issn = {07342071},
journal = {ACM Transactions on Computer Systems},
month = nov,
number = {4},
pages = {337--371},
title = {{Effective fine-grain synchronization for automatically parallelized programs using optimistic synchronization primitives}},
url = {http://portal.acm.org/citation.cfm?doid=329466.329486},
volume = {17},
year = {1999}
}
@article{Rinard2001,
abstract = {The field of program analysis has focused primarily on sequential programming langueages. But multithreading is becoming increasingly important, both as a a program structuring mechanism and to support efficient parallel computations.  This paper surveys research in analysis for multithreaded programs, focusing on ways to improve the efficiency of analyzing interactions between threads, to detect data races, and to ameliorate the impact of weak memory consistency models.  We identify two distinct classes of multithreaded programs, and discuss how the structure of these kinds of programs leads to different solutions to these problems.  Specifically, we conclude that augmented type systems are the most promising approach for activity management programs, while targetted program analyses are the most promising approach for parallel computing programs.
},
author = {Rinard, Martin},
file = {:home/sos22/papers/rinard/sas01.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {1--19},
publisher = {Springer},
title = {{Analysis of multithreaded programs}},
url = {http://www.springerlink.com/index/0R9WUTB5CL5FT60T.pdf},
year = {2001}
}
@inproceedings{Engler2000,
author = {Engler, D. and Chelf, B. and Chou, A. and Hallem, S.},
booktitle = {Proceedings of the 4th conference on Symposium on Operating System Design $\backslash$\& Implementation-Volume 4},
file = {:home/sos22/papers/static-analysis/mc-osdi.pdf:pdf},
publisher = {USENIX Association Berkeley, CA, USA},
title = {{Checking system rules using system-specific, programmer-written compiler extensions}},
url = {http://portal.acm.org/citation.cfm?id=1251229.1251230\&amp;coll=GUIDE\&amp;d1=GUIDE\&amp;C},
year = {2000}
}
@article{Liu2007,
author = {Liu, C},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu - 2007 - Statistical debugging and automated program failure triage.pdf:pdf},
title = {{Statistical debugging and automated program failure triage}},
url = {http://www.ideals.uiuc.edu/bitstream/handle/2142/11348/Statistical Debugging and Automated Program Failure Triage.pdf?sequence=2},
year = {2007}
}
@phdthesis{Zheng2005,
author = {Zheng, Alice Ziaozhou},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jordan - 2005 - Statistical Software Debugging by Alice Xiaozhou Zheng BA (University of \ldots.pdf:pdf},
school = {UCB},
title = {{Statistical Software Debugging}},
type = {PhD},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.3845\&rep=rep1\&type=pdf},
year = {2005}
}
@article{Aiken1989,
author = {Aiken, A and Wimmers, EL and Williams, JH},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Edward, Wimmers - Unknown - Program Transformation in the Presence of Errors.pdf:pdf},
journal = {Proceedings of the 17th ACM  \ldots},
title = {{Program transformation in the presence of errors}},
url = {http://portal.acm.org/citation.cfm?id=96730\&dl=GUIDE,},
year = {1989}
}
@inproceedings{Edward,
author = {Aiken, A and Williams, JH and Wimmers, EL},
booktitle = {Proceedings of the 17th ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Edward, Wimmers - Unknown - Program Transformation in the Presence of Errors.pdf:pdf},
pages = {210--217},
publisher = {ACM New York, NY, USA},
title = {{Program Transformation in the Presence of Errors}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.3356\&rep=rep1\&type=pdf},
year = {1989}
}
@article{Liu2007a,
author = {Liu, C and Zhang, X and Han, J and Zhang, Y and Bhargava, BK},
file = {::},
title = {{Indexing noncrashing failures: A dynamic program slicing-based approach}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.7181\&rep=rep1\&type=pdf},
year = {2007}
}
@article{Ernst2000,
author = {Ernst, MD and Czeisler, A and Griswold, WG},
file = {::},
journal = {Proceedings of the 22nd  },
title = {{Quickly detecting relevant program invariants}},
url = {http://portal.acm.org/citation.cfm?id=337180.337240},
year = {2000}
}
@article{Zheng2003,
author = {Zheng, AX and Jordan, MI and Liblit, B and Aiken, A},
file = {::},
journal = {Advances in Neural Information  },
title = {{Statistical debugging of sampled programs}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.3279\&rep=rep1\&type=pdf},
year = {2003}
}
@article{Engler2001,
author = {Engler, D and Chen, DY and Hallem, S and Chou, A},
file = {::},
journal = {ACM SIGOPS Operating  },
title = {{Bugs as deviant behavior: A general approach to inferring errors in systems code}},
url = {http://portal.acm.org/citation.cfm?id=502041},
year = {2001}
}
@article{Ammons2002,
author = {Ammons, G and Bodik, R and Larus, JR},
file = {::},
journal = {  of the 29th ACM SIGPLAN-SIGACT },
title = {{Mining specifications}},
url = {http://portal.acm.org/citation.cfm?id=503272.503275},
year = {2002}
}
@article{Artho2003,
author = {Artho, C and Havelund, K and Biere, A},
file = {::},
journal = {Software Testing Verification and  \ldots},
title = {{High-level data races}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.45\&rep=rep1\&type=pdf},
year = {2003}
}
@article{Flanagan2008,
author = {Flanagan, C and Freund, SN},
file = {::},
journal = {Science of Computer Programming},
title = {{Atomizer: a dynamic atomicity checker for multithreaded programs}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167642308000051},
year = {2008}
}
@article{Engler2003,
author = {Engler, D and Ashcraft, K},
file = {::},
journal = {ACM SIGOPS Operating Systems Review},
title = {{RacerX: Effective, static detection of race conditions and deadlocks}},
url = {http://portal.acm.org/citation.cfm?id=945468},
year = {2003}
}
@article{Zamfir,
author = {Zamfir, C and Candea, G},
journal = {infoscience.epfl.ch},
title = {{ESD: A System Software Debugger Based On Execution Synthesis}},
url = {http://infoscience.epfl.ch/getfile.py?docid=25048\&name=main\&format=pdf\&version=1}
}
@article{Sahoo,
author = {Sahoo, SK and Criswell, J and Adve, V},
journal = {ideals.uiuc.edu},
title = {{Towards Automated Bug Diagnosis: An Empirical Study of Software Bugs in  }},
url = {http://www.ideals.uiuc.edu/bitstream/handle/2142/13697/paper\_submission.pdf?sequence=2}
}
@article{Andrica,
annote = {Research proposal, in essence.  Very similar to what I want to do.

        
They also consider performance issues, deadlocks, and wrong output, rather than just crashes.  Idea is to look at a bunch of test runs and hence build up a classifier which can say whether a schedule is good or bad, and then to enforce one of those schedules during production runs.  Looking at direct schedule enforcrement, rather than concurrency control inference, which is likely to be somewhat higher overhead.

        
I'm not convinced they have a good plan for dealing with input dependencies.

        
There's nothing really here, but I'd quite like to talk to the authors about what they're doing.},
author = {Andrica, S and Zamfir, C and Candea, G},
file = {:home/sos22/papers/random/goodrun\_wip\_eurosys09.pdf:pdf},
title = {{GoodRun: Enforcing Good Runs in Parallel Programs}},
url = {http://scholar.google.com/scholar?start=40\&hl=en\&as\_sdt=2000\&cites=9742784347491736142\#3}
}
@article{Lu2008a,
author = {Lu, S},
title = {{Understanding, Detecting and Exposing Concurrency Bugs}},
url = {http://scholar.google.com/scholar?start=10\&hl=en\&as\_sdt=2000\&cites=7528913852644717196\#8},
year = {2008}
}
@article{Chipounov2009,
author = {Chipounov, V and Georgescu, V and Zamfir, C},
file = {::},
journal = {Workshop on Hot  \ldots},
title = {{Selective Symbolic Execution}},
url = {http://infoscience.epfl.ch/getfile.py?docid=24251\&name=selsymbex\&format=pdf\&version=1},
year = {2009}
}
@article{Oberheide,
author = {Oberheide, J and Cooke, E and Jahanian, F},
journal = {usenix.org},
title = {{If It Ain't Broke, Don't Fix It: Challenges and New Directions for Inferring the  \ldots}},
url = {http://scholar.google.com/scholar?start=30\&hl=en\&as\_sdt=2000\&cites=9742784347491736142\#5}
}
@article{Hallem2002,
author = {Hallem, S and Chelf, B and Xie, Y and Engler, D},
file = {::},
journal = {  on Programming language  },
title = {{A system and language for building system-specific, static analyses}},
url = {http://portal.acm.org/citation.cfm?id=512539\&dl=},
year = {2002}
}
@article{Xu2005,
author = {Xu, M and Bod\'{\i}k, R and Hill, MD},
file = {::},
journal = {Proceedings of the 2005 ACM SIGPLAN  },
title = {{A serializability violation detector for shared-memory server programs}},
url = {http://portal.acm.org/citation.cfm?id=1064978.1065013},
year = {2005}
}
@article{Molnar2009,
author = {Molnar, D and Li, XC and Wagner, D},
file = {::},
journal = {USENIX Security Symposium},
title = {{Dynamic test generation to find integer bugs in X86 binary linux programs}},
url = {http://www.cs.berkeley.edu/\~{}dmolnar/metafuzz-tr-draft.pdf},
year = {2009}
}
@article{Olszewski2009,
abstract = {Although chip-multiprocessors have become the industry standard, developing parallel applications that target them remains a daunting task. Non-determinism, inherent in threaded applications, causes significant challenges for parallel programmers by hindering their ability to create parallel applications with repeatable results. As a consequence, parallel applications are significantly harder to debug, test, and maintain than sequential programs. This paper introduces Kendo: a new software-only system that provides deterministic multithreading of parallel applications. Kendo enforces a deterministic interleaving of lock acquisitions and specially declared non-protected reads through a novel dynamically load-balanced deterministic scheduling algorithm. The algorithm tracks the progress of each thread using performance counters to construct a deterministic logical time that is used to compute an interleaving of shared data accesses that is both deterministic and provides good load balancing. Kendo can run on today's commodity hardware while incurring only a modest performance cost. Experimental results on the SPLASH-2 applications yield a geometric mean overhead of only 16\% when running on 4 processors. This low overhead makes it possible to benefit from Kendo even after an application is deployed. Programmers can start using Kendo today to program parallel applications that are easier to develop, debug, and test.},
author = {Olszewski, Marek and Ansel, Jason and Amarasinghe, Saman},
journal = {Architectural Support for Programming Languages and Operating Systems},
keywords = {debugging,determinism,deterministic multithreading,multicore,parallel programming},
number = {3},
title = {{Kendo:Â efficient deterministic multithreading in software}},
url = {http://portal.acm.org/citation.cfm?id=1508244.1508256},
volume = {44},
year = {2009}
}
@article{Ernst1999,
author = {Ernst, MD and Cockrell, J},
file = {::},
journal = {  on Software  },
title = {{Dynamically discovering likely program invariants to support program evolution}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/ICSE.1999.841011},
year = {1999}
}
@article{Brumley2006,
author = {Brumley, D and Newsome, J and Song, D and Wang, H},
file = {::},
journal = {Proceedings of the 2006  },
title = {{Towards automatic generation of vulnerability-based signatures}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.74.6222\&rep=rep1\&type=pdf},
year = {2006}
}
@article{Wang2009,
author = {Wang, T and Wei, T and Lin, Z and Zou, W},
journal = {Network Distributed Security  },
title = {{Intscope: Automatically detecting integer overflow vulnerability in x86 binary  }},
url = {http://206.131.241.137/isoc/conferences/ndss/09/pdf/17.pdf},
year = {2009}
}
@article{Anand2008,
author = {Anand, S and Godefroid, P and Tillmann, N},
file = {::},
journal = {Lecture Notes in Computer Science},
title = {{Demand-driven compositional symbolic execution}},
url = {http://www.springerlink.com/index/7711m0qr285075n6.pdf},
year = {2008}
}
@article{Kremenek2006,
author = {Kremenek, T and Twohey, P and Back, G and Ng, A and Engler, D},
file = {::},
journal = {OSDI, Nov},
title = {{From uncertainty to belief: Inferring the specification within}},
url = {http://www.usenix.org/event/osdi06/tech/full\_papers/kremenek/kremenek.pdf},
year = {2006}
}
@misc{Malik,
abstract = {Automated debugging is becoming increasingly important as the size and complexity of software increases. This paper makes a case for using constraint-based data structure repair, a recently developed technique for fault recovery, as a basis for automated debugging. Data structure repair uses given structural integrity constraints for key data structures to monitor their correctness during the execution of a program. If a constraint violation is detected, repair performs mutations on the data structures, i.e., corrupt program state, and transforms it into another state, which satisfies the desired constraints. The primary goal of data structure repair is to transform an erroneous state into an acceptable state. Therefore, the mutations performed by repair actions provide a basis of debugging faults in code (assuming the errors are due to bugs). A key challenge to embodying this insight into a mechanical technique arises due to the difference in the concrete level of the program states and the abstract level of the program code: repair actions apply to concrete data structures that exist at runtime, whereas debugging applies to code. We observe that static structures (program variables) hold handles to dynamic structures (heap-allocated data), which allows bridging the gap between the abstract and concrete levels. We envision a tool-chain where a data structure repair tool generates repair logs that are used by a fault localization tool and a repair abstraction tool that apply in synergy to not only identify the location of fault(s) in code but also to synthesize debugging suggestions. An embodiment of our vision can significantly reduce the cost of developing reliable software.},
address = {Austin},
annote = {Assume the programmer provides a bunch of data structure validity constraints.  Every so often, go and check the constraints, and if they fail hit it with a hammer until it no longer fails.  Once you've done it once, embody the fix as a bit of fixup code which can resatisfy the same invariants.  Intended mostly as a debugging aid, rather than something which would be used in production.

        
Utterly terrifying.  Fortunately, they don't appear to have much idea how to actually do it, so it's all good.

        

      },
author = {Malik, MZ and Ghori, K and Elkarablieh, B and Khurshid, S},
booktitle = {users.ece.utexas.edu},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malik et al. - Unknown - A Case for Automated Debugging Using Data Structure Repair.pdf:pdf},
publisher = {University of Texas},
title = {{A Case for Automated Debugging Using Data Structure Repair}},
url = {http://users.ece.utexas.edu/\~{}khurshid/papers/2009/09ase-repair.pdf}
}
@article{Marzullo,
author = {Marzullo, K and Vin, H},
journal = {repositories.lib.utexas.edu},
title = {{Robust Multithreaded Applications}},
url = {http://scholar.google.com/scholar?start=290\&hl=en\&as\_sdt=2000\&cites=11403978972349890280\#1}
}
@article{He2004,
author = {He, H and Gupta, N},
file = {::},
journal = {Lecture notes in computer science},
title = {{Automated debugging using path-based weakest preconditions}},
url = {http://www.springerlink.com/index/WH2AW07FTQRG58VK.pdf},
year = {2004}
}
@phdthesis{Kent,
abstract = {Managed programming languages have improved the robustness of software by reducing some classes of errors. Despite these improvements, programs still contain errors. Even with help from existing static and dynamic analysis tools, the errors are often difficult to detect and can be even more difficult to suc- cessfully eliminate. Thiswork introduces a newapproach, called dynamic error remediation, that further improves the robustness of software, by tolerating some null pointer exceptions until a patch can be cre- ated to fix the exception. This paper describes dynamic error remediation, and its effectiveness, with different strategies for handling the exceptions. We show that dynamic error remediation is successful in allowing programs to continue execution and to do meaningful work in many cases. We then describe the bug suite created to test dynamic error remediation, as well as providing a methodology for creating other bug suites. Finally, we describe origin tracking, a JVM modification which exposes the origin of null values that cause null pointer exceptions. Because dynamic error remediation is successful in many cases, we assert that it is a viable strategy for increasing availability and reliability in many programs. While increasing availability over correctness is not ideal in all cases, we show that there are real exam- ples where it is preferred, and we provide dynamic error remediation as a tool for developers and users to survive null pointer exception bugs.},
annote = {Trying to get rid of null pointer exceptions in Java, essentially using the failure oblivious model.  Also do some bits with origin tracking to find out where the null came from.  Not as many references as I'd like.

        
Try a couple of different strategies for error remediation.  None are terribly exciting.

        
Origin tracking is essentially just using a redundant encoding for null values and putting more information in the extra bits.  Meh.

      },
author = {Kent, SW},
booktitle = {Citeseer},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kent - Unknown - Dynamic Error Remediation A Case Study with Null Pointer Exceptions.pdf:pdf},
school = {University of Texas at Austin},
title = {{Dynamic Error Remediation: A Case Study with Null Pointer Exceptions}},
type = {Honors},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.140.2544\&rep=rep1\&type=pdf},
year = {2008}
}
@article{Xie2002,
author = {Xie, T and Notkin, D},
file = {::},
journal = {FSE Poster Session},
title = {{Checking inside the black box: Regression fault exposure and localization  }},
url = {http://reference.kfupm.edu.sa/content/c/h/checking\_inside\_the\_black\_box\_\_regressio\_70283.pdf},
year = {2002}
}
@article{Perkins2004,
author = {Perkins, JH and Ernst, MD},
file = {::},
journal = {ACM SIGSOFT Software Engineering Notes},
title = {{Efficient incremental algorithms for dynamic detection of likely invariants}},
url = {http://portal.acm.org/citation.cfm?id=1041685.1029901},
year = {2004}
}
@inproceedings{Racunas,
abstract = {Fault screeners are a new breed of fault identification technique that can probabilistically detect if a transient fault has affected the state of a processor. We demonstrate that fault screeners function because of two key character- istics. First, we show that much of the intermediate data generated by a program inherently falls within certain con- sistent bounds. Second, we observe that these bounds are often violated by the introduction of a fault. Thus, fault screeners can identify faults by directly watching for any data inconsistencies arising in an application's behavior. We present an idealized algorithm capable of identify- ing over 85\% of injected faults on the SpecInt suite and over 75\% overall. Further, in a realistic implementation on a simulated Pentium-III-like processor, about half of the errors due to injected faults are identified while still in speculative state. Errors detected this early can be elimi- nated by a pipeline flush. In this paper, we present several hardware-based versions of this screening algorithm and show that flushing the pipeline every time the hardware screener triggers reduces overall performance by less than 1\%},
annote = {Tries to determine whether a soft error in a processor should be propagated up to the OS.

      },
author = {Racunas, P and Constantinides, K and Manne, S and Mukherjee, SS},
booktitle = {Proceedings of the 2007 IEEE 13th International Symposium on High Performance Computer Architecture},
file = {:home/sos22/papers/random/perturbation\_based\_fault\_screening.pdf:pdf},
pages = {169--180},
publisher = {IEEE COMPUTER SOCIETY},
title = {{Perturbation-based fault screening}},
url = {http://www.eecs.umich.edu/\~{}kypros/racunas-hpca07.pdf},
year = {2007}
}
@article{Tucek2007,
author = {Tucek, J and Lu, S and Huang, C and Xanthos, S},
file = {::},
journal = {ACM SIGOPS Operating  },
title = {{Triage: diagnosing production run failures at the user's site}},
url = {http://portal.acm.org/citation.cfm?id=1323293.1294275},
year = {2007}
}
@article{Lucia,
author = {Lucia, B and Ceze, L},
file = {::},
journal = {abstract.cs.washington.edu},
title = {{Finding Concurrency Bugs with Context-Aware Communication Graphs}},
url = {http://abstract.cs.washington.edu/\~{}blucia0a/pubs/micro09.pdf}
}
@article{Ronsse2000,
author = {Ronsse, M and Bosschere, K De},
journal = {  of Automated and  },
title = {{Non-intrusive on-the-fly data race detection using execution replay}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.9610\&rep=rep1\&type=pdf},
year = {2000}
}
@article{Jeffrey2008,
author = {Jeffrey, D and Gupta, N and Gupta, R},
file = {::},
journal = {IEEE International  },
title = {{Identifying the root causes of memory bugs using corrupted memory location  }},
url = {http://doi.ieeecomputersociety.org/10.1109/ICSM.2008.4658084},
year = {2008}
}
@inproceedings{Narayanasamy2007,
abstract = {Many concurrency bugs in multi-threaded programs are due to data races. There have been many efforts to develop static and dynamic mechanisms to automatically find the data races. Most of the prior work has focused on finding the data races and eliminating the false positives. In this paper, we instead focus on a dynamic analysis technique to automatically classify the data races into two categories  the data races that are potentially benign and the data races that are po- tentially harmful. A harmful data race is a real bug that needs to be fixed. This classification is needed to focus the triaging effort on those data races that are potentially harmful. Without prioritiz- ing the data races we have found that there are too many data races to triage. Our second focus is to automatically provide to the de- veloper a reproducible scenario of the data race, which allows the developer to understand the different effects of a harmful data race on a programs execution. To achieve the above, we record a multi-threaded programs execution in a replay log. The replay log is used to replay the multi- threaded program, and during replay we find the data races using a happens-before based algorithm. To automatically classify if a data race that we find is potentially benign or potentially harmful, we replay the execution twice for a given data race  one for each possible order between the conflicting memory operations. If the two replays for the two orders produce the same result, then we classify the data race to be potentially benign. We discuss our experiences in using our replay based dynamic data race checker on several Microsoft applications.},
address = {San Diego},
annote = {Log a run under a deterministic replay system, then do an offline check for races, and then try flipping the race.  Any race where flipping it over doesn't make any (much?) difference is dismissed as benign.  Any replay failure is considered to be evidence of malignancy.  Any deviation in memory state (modulo things like free() and stack pointer changes) is considered evidence of malignancy.  So they have a pretty generous definition of malignancy.

        
Massive overheads: factor of several hundred.  Okay in a dev tool.

        
Looks like they eliminate about half of the races in their test corpus.  Pretty good.

        
Potentially useful information for the search phase of the replay system?

      },
author = {Narayanasamy, S and Wang, Z and Tigani, J and Edwards, A and Calder, B},
booktitle = {PLDI},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Narayanasamy, Wang, Tigani - 2007 - Automatically classifying benign and harmful data racesallusing replay analysis.pdf:pdf},
keywords = {Benign Data Races,Concurrency Bugs,Replay},
title = {{Automatically classifying benign and harmful data races using replay analysis}},
url = {http://portal.acm.org/citation.cfm?id=1250738},
year = {2007}
}
@article{Bond2007,
author = {Bond, MD and Nethercote, N and Kent, SW},
file = {::},
journal = {Proceedings of the  },
title = {{Tracking bad apples: reporting the origin of null and undefined value errors}},
url = {http://portal.acm.org/citation.cfm?id=1297027.1297057},
year = {2007}
}
@article{Mariani2009,
author = {Mariani, L and Pastore, F and Pezze, M},
file = {::},
journal = {Proceedings of the  },
title = {{A toolset for automated failure analysis}},
url = {http://doi.ieeecomputersociety.org/10.1109/ICSE.2009.5070556},
year = {2009}
}
@article{Taniguchi2005,
author = {Taniguchi, K and Ishio, T and Kamiya, T},
file = {::},
journal = {  on Principles of  },
title = {{Extracting sequence diagram from execution trace of Java program}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/IWPSE.2005.19},
year = {2005}
}
@article{Xiao2007,
author = {Xiao, C},
title = {{Performance enhancements for a dynamic invariant detector}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.5689\&rep=rep1\&type=pdf},
year = {2007}
}
@article{Jeffrey2008a,
author = {Jeffrey, D and Gupta, N and Gupta, R},
file = {::},
journal = {Proceedings of the 2008 international  },
title = {{Fault localization using value replacement}},
url = {http://portal.acm.org/citation.cfm?id=1390652},
year = {2008}
}
@article{Bond2010,
author = {Bond, MD and Baker, GZ and Guyer, SZ},
journal = {Under submission to ACM  },
title = {{Breadcrumbs: Efficient Context Sensitivity for Dynamic Bug Detection  }},
url = {http://scholar.google.com/scholar?cites=16180793081970582461\&hl=en\&as\_sdt=2000\#3},
year = {2010}
}
@article{Lo,
author = {Lo, D and Khoo, SC and Liu, C},
journal = {Program Comprehension through Dynamic Analysis},
title = {{Mining temporal rules from program execution traces}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6160\&rep=rep1\&type=pdf\#page=30}
}
@article{Pytlik2003,
author = {Pytlik, B},
file = {::},
title = {{Automatic Debugging Using Potential Invariants}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.7795\&rep=rep1\&type=pdf},
year = {2003}
}
@article{Narayanasamy2005,
author = {Narayanasamy, S and Pokam, G and Calder, B},
file = {::},
journal = {Proceedings of the 32nd  },
title = {{Bugnet: Continuously recording program execution for deterministic replay  }},
url = {http://portal.acm.org/citation.cfm?id=1069807.1069994},
year = {2005}
}
@article{Zeller2002,
author = {Zeller, A},
file = {::},
journal = {ACM SIGSOFT Software Engineering Notes},
title = {{Isolating cause-effect chains from computer programs}},
url = {http://portal.acm.org/citation.cfm?id=605468},
year = {2002}
}
@article{Ball2003,
author = {Ball, T and Naik, M and Rajamani, SK},
file = {::},
journal = {  of the 30th ACM SIGPLAN-SIGACT  },
title = {{From symptom to cause: Localizing errors in counterexample traces}},
url = {http://portal.acm.org/citation.cfm?id=604131.604140},
year = {2003}
}
@article{Savage1997,
author = {Savage, S and Burrows, M and Nelson, G and Sobalvarro, P. and Anderson, T.},
file = {::},
journal = {ACM Transactions on Computer Systems (TOCS)},
number = {4},
pages = {391--411},
publisher = {ACM New York, NY, USA},
title = {{Eraser: A dynamic data race detector for multithreaded programs}},
url = {http://portal.acm.org/citation.cfm?id=265924.265927\&amp;dl=portal\&amp;dl=ACM\&amp;idx=J774\&amp;part=periodical\&amp;WantType=periodical\&amp;title=ACM Transactions on Computer Systems (TOCS)},
volume = {15},
year = {1997}
}
@article{Burrows2004,
author = {Burrows, M and Leino, KRM},
file = {::},
journal = {Concurrency and Computation Practice and  },
title = {{Finding stale-value errors in concurrent programs}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.492\&rep=rep1\&type=pdf},
year = {2004}
}
@article{Serebryany2009,
author = {Serebryany, K and Iskhodzhanov, T},
file = {::},
journal = {WBIA'09},
title = {{ThreadSanitizerdata race detection in practice}},
url = {http://pintool.org/wbia09.pdf\#page=62},
year = {2009}
}
@article{Yang2006,
author = {Yang, J and Evans, D and Bhardwaj, D and Bhat, T},
file = {::},
journal = {Proceedings of the 28th  },
title = {{Perracotta: Mining temporal API rules from imperfect traces}},
url = {http://portal.acm.org/citation.cfm?id=1134325\&dl=},
year = {2006}
}
@article{Ayewah2007,
author = {Ayewah, N and Pugh, W and Morgenthaler, JD},
journal = {Companion to the  },
title = {{Using FindBugs on production software}},
url = {http://portal.acm.org/citation.cfm?id=1297897},
year = {2007}
}
@article{Chen,
author = {Chen, Q and Wang, L},
journal = {doi.ieeecomputersociety.org},
title = {{An Integrated Framework for Checking Concurrency-related Programming Errors}},
url = {http://doi.ieeecomputersociety.org/10.1109/COMPSAC.2009.105}
}
@article{Agarwal2005,
author = {Agarwal, R and Sasturkar, A and Wang, L},
journal = {Proceedings of the 20th  },
title = {{Optimized run-time race detection and atomicity checking using partial discovered  }},
url = {http://portal.acm.org/citation.cfm?id=1101944},
year = {2005}
}
@article{Tian2008,
author = {Tian, C and Nagarajan, V and Gupta, R},
file = {::},
journal = {Proceedings of the 2008  },
title = {{Dynamic recognition of synchronization operations for improved data race  }},
url = {http://portal.acm.org/citation.cfm?id=1390630.1390649},
year = {2008}
}
@article{Mittal2004,
author = {Mittal, N and Garg, VK},
file = {::},
journal = {Distributed Computing},
title = {{Finding missing synchronization in a distributed computation using controlled re- }},
url = {http://www.springerlink.com/index/GJRWUWUVFR491L6X.pdf},
year = {2004}
}
@article{Henzinger2004,
author = {Henzinger, TA and Jhala, R and Majumdar, R},
journal = {Proceedings of the ACM  },
title = {{Race checking by context inference}},
url = {http://portal.acm.org/citation.cfm?id=996841.996844},
year = {2004}
}
@article{Hruba2009,
author = {Hrub\'{a}, V and Kfena, B and Vojnar, T},
journal = {  on Computer Aided Systems Theory, Las  },
title = {{Self-healing Assurance Based on Bounded Model Checking}},
url = {http://www.springerlink.com/index/m23x648n5h0r67w6.pdf},
year = {2009}
}
@article{Thakur,
author = {Thakur, A and Sen, R and Liblit, B and Lu, S},
file = {::},
journal = {pages.cs.wisc.edu},
title = {{Cooperative Crug Isolation}},
url = {http://pages.cs.wisc.edu/\~{}liblit/woda-2009/woda-2009.pdf}
}
@misc{Letko,
abstract = {Data races and atomicity violation are a common problem in concurrent programming. This article describes a technology capable to detect atomicity violation and data races in Java pro- grams and heal them at run-time. The architecture expects dynamic analysis to be used for detecting and healing data races and atomicity violations. Correct atomicity can be specified manually or obtained by static analysis.},
annote = {A research proposal, essentially.  Trying to automatically fix races, but they don't seem to give any indication of how they're planning on doing it.

        
Basically null.

      },
author = {Letko, Z},
booktitle = {feec.vutbr.cz},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Letko - Unknown - AN ARCHITECTURE FOR SELF-HEALING OF DATA RACES AND.pdf:pdf},
publisher = {Brno University of Technology},
title = {{AN ARCHITECTURE FOR SELF-HEALING OF DATA RACES AND ATOMICITY VIOLATIONS FOR JAVA}},
url = {http://www.feec.vutbr.cz/EEICT/2008/sbornik/02-Magisterske projekty/10-Inteligentni systemy/05-xletko00.pdf}
}
@inproceedings{Nir-Buchbinder2008,
abstract = {Deadlocks are possibly the best known bug pattern in com- puter systems in general; certainly they are the best known in concurrent programming. Numerous articles, some dating back more than 40 years, have been dedicated to the questions of how to design deadlock free pro- grams, how to statically or dynamically detect possible deadlocks, how to avoid deadlocks at runtime, and how to resolve deadlocks once they hap- pen.We start the paper with an investigation on how to exhibit potential deadlocks. Exhibiting deadlocks is very useful in testing, as verifying if a potential deadlock can actually happen is a time-consuming debugging activity. There was recently some very interesting research in this direc- tion; however, we believe our approach is more practical, has no scaling issues, and in fact is already industry-ready. The second contribution of our paper is in the area of healing multi- threaded programs so they do not get into deadlocks. This is an entirely new approach, which is very different from the approaches in the liter- ature that were meant for multi-process scenarios and are not suitable (and indeed not used) in multithreaded programming. While the basic ideas are fairly simple, the details here are very important as any mistake is liable to actually create new deadlocks. The paper describes the basic healing idea and its limitations, the pitfalls and how to overcome them, and experimental results.},
annote = {Precursor to Dimmunix, Gadara, and CHESS.  Use something very similar to the CHESS algorithm to find deadlocks, and the Dimmunix one to invent fixes by providing new guardian locks.

        
Really quite cute (and makes Dimmunix look much less impressive), but not directly relevant.  It's very sad that both Dimmunix and Gadara got more citations than this one.},
author = {Nir-Buchbinder, Y and Tzoref, R and Ur, S},
booktitle = {Proc. 8th Workshop on Runtime Verification},
file = {:home/sos22/papers/random/deadlocks\_from\_exhibiting\_to\_healing.pdf:pdf},
title = {{Deadlocks: from exhibiting to healing}},
url = {http://www.springerlink.com/index/c0q57141u4542t30.pdf},
year = {2008}
}
@article{Lee2010,
author = {Lee, D and Wester, B and Veeraraghavan, K and Narayanasamy, S},
file = {::},
title = {{Respec: Efficient Online Multiprocessor Replay via Speculation and External }},
url = {http://www.eecs.umich.edu/\~{}kaushikv/papers/respec\_asplos10.pdf},
year = {2010}
}
@article{Huang,
author = {Huang, J and Liu, P and Zhang, C and Kim, S},
file = {::},
journal = {cse.ust.hk},
title = {{CLAP: Concurrent Lightweight Crash Reproduction}},
url = {http://www.cse.ust.hk/\~{}charlesz/clap.pdf}
}
@article{Ayers2005,
author = {Ayers, A and Schooler, R and Metcalf, C and Agarwal, A},
journal = {ACM SIGPLAN  },
title = {{TraceBack: first fault diagnosis by reconstruction of distributed control flow}},
url = {http://portal.acm.org/citation.cfm?id=1064978.1065035},
year = {2005}
}
@article{Jula2008a,
author = {Jula, H and Candea, G},
journal = {Proc. 8th Workshop on Runtime Verification},
title = {{A scalable, sound, eventuallycomplete algorithm for deadlock immunity}},
url = {http://www.springerlink.com/index/l8476552682k5724.pdf},
year = {2008}
}
@article{Roy2009,
author = {Roy, A and Hand, S and Harris, T},
journal = {Proceedings of the 4th ACM European  },
title = {{A runtime system for software lock elision}},
url = {http://portal.acm.org/citation.cfm?id=1519094},
year = {2009}
}
@article{Boyapati2002,
author = {Boyapati, C and Lee, R and Rinard, M},
journal = {  on Object-oriented programming,  },
title = {{Ownership types for safe programming: Preventing data races and deadlocks}},
url = {http://portal.acm.org/citation.cfm?id=582440\&dl=GUIDE,},
year = {2002}
}
@article{Tallam2008,
author = {Tallam, S and Tian, C and Gupta, R},
file = {::},
journal = {International Conference on Software  },
title = {{Dynamic slicing of multithreaded programs for race detection}},
url = {http://www.cs.ucr.edu/\~{}gupta/research/Publications/Comp/icsm08b.pdf},
year = {2008}
}
@article{Cook,
author = {Cook, B and Podelski, A and Rybalchenko, A},
file = {::},
journal = {mpi-sws.org},
title = {{Proving program termination}},
url = {http://www.mpi-sws.org/\~{}rybal/papers/cacm09-proving-program-termination.pdf}
}
@article{Chandra1998,
author = {Chandra, S and Chen, PM},
journal = {FTCS},
title = {{How fail-stop are faulty programs?}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.74.5182\&rep=rep1\&type=pdf},
year = {1998}
}
@article{Prvulovic2003,
author = {Prvulovic, M and Torrellas, J},
journal = {ANNUAL INTERNATIONAL  \ldots},
title = {{ReEnact: Using thread-level speculation mechanisms to debug data races in  \ldots}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/ISCA.2003.1206993},
year = {2003}
}
@article{Flanagan2005a,
author = {Flanagan, C and Freund, SN},
journal = {Synchronization and Concurrency in Object- },
title = {{Automatic synchronization correction}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.1908\&rep=rep1\&type=pdf},
year = {2005}
}
@phdthesis{Chew2009,
abstract = {Multi-core machines have become common and have led to an increase in multithreaded software. In turn, the number of concurrency bugs has also increased. Such bugs are elusive and remain difficult to solve, despite existing research. Thus, this thesis proposes a system which detects, prevents and optionally helps expose concurrency bugs. Specifically, we focus on bugs caused by atomicity violations, which occur when thread interleaving violates the programmers assumption that a code section executes atomically. At compile-time, our system performs static analysis to identify code sections where violations could occur. At run-time, we use debug registers to monitor these sections for interleaving thread accesses which would cause a violation. If detected, we undo their effects and thus prevent the violation. Optionally, we help expose atomicity violations by perturbing thread scheduling during execution. Our results demonstrate that the system is effective and imposes low overhead.},
annote = {Only consider atomicity violations.  Approach is to do an initial static analysis, and guess from that where the critical sections should have been.  If a critical section doesn't have a lock around it, arrange to check at run-time whether the section was actually atomic, and roll back if it wasn't.

        
Only consider short critical sections: at most two accesses.

        
Critical regions are monitored using debug registers.

        
Have a load of CHESS-like bits for trying to provoke bugs which are irrelevant to us.

        
Overhead on the order of tens of percent.

        
Pretty damn good, for a masters thesis.},
author = {Chew, L},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chew - 2009 - A System for Detecting, Preventing and Exposing Atomicity Violations in \ldots.pdf:pdf},
school = {University of Toronto},
title = {{A System for Detecting, Preventing and Exposing Atomicity Violations in Multithreaded Programs}},
type = {Master of Applied Science},
url = {http://www.eecg.toronto.edu/\~{}lie/papers/lchew\_msthesis.pdf},
year = {2009}
}
@article{Balakrishnan2004,
author = {Balakrishnan, G and Reps, T},
journal = {Lecture notes in computer science},
title = {{Analyzing memory accesses in x86 executables}},
url = {http://www.springerlink.com/index/MWXDBUCPQDMXRQ61.pdf},
year = {2004}
}
@article{Xin2008,
author = {Xin, B and Sumner, WN and Zhang, X},
file = {::},
journal = {Proceedings of the 2008 ACM  },
title = {{Efficient program execution indexing}},
url = {http://portal.acm.org/citation.cfm?id=1375611},
year = {2008}
}
@article{Gao2009,
author = {Gao, Q and Zhang, W and Tang, Y and Qin, F},
file = {::},
journal = {Proceedings of the fourth ACM  },
title = {{First-aid: surviving and preventing memory management bugs during production  }},
url = {http://portal.acm.org/citation.cfm?id=1519083\&dl=ACM},
year = {2009}
}
@article{Farchi2003,
author = {Farchi, E and Nir, Y and Ur, S},
journal = {  of the 17th International Symposium on  },
title = {{Concurrent bug patterns and how to test them}},
url = {http://scholar.google.com/scholar?hl=en\&q=concurrent+bug+patterns+and+how+to+test+them\&btnG=Search\&as\_sdt=2000\#0},
year = {2003}
}
@article{Rajwar2001,
author = {Rajwar, R and Goodman, JR},
journal = {Proceedings of the 34th annual ACM/IEEE },
title = {{Speculative lock elision: Enabling highly concurrent multithreaded execution}},
url = {http://portal.acm.org/citation.cfm?id=564036\&dl=},
year = {2001}
}
@article{Mittermayr,
author = {Mittermayr, R and Blieberger, J},
file = {::},
journal = {Proc. of the 3rd International Symposium on  },
title = {{Static partial-order reduction of concurrent systems in polynomial time}},
url = {http://www.springerlink.com/index/r8698kq31171g7r5.pdf}
}
@article{Flanagan2005,
author = {Flanagan, C and Godefroid, P},
journal = {ACM SIGPLAN Notices},
title = {{Dynamic partial-order reduction for model checking software}},
url = {http://portal.acm.org/citation.cfm?id=1040315},
year = {2005}
}
@inproceedings{Berger2006,
abstract = {Applications written in unsafe languages like C and C++ are vul- nerable to memory errors such as buffer overflows, dangling point- ers, and reads of uninitialized data. Such errors can lead to pro- gram crashes, security vulnerabilities, and unpredictable behavior. We present DieHard, a runtime system that tolerates these errors while probabilistically maintaining soundness. DieHard uses ran- domization and replication to achieve probabilistic memory safety by approximating an infinite-sized heap. DieHardâs memory man- ager randomizes the location of objects in a heap that is at least twice as large as required. This algorithm prevents heap corruption and provides a probabilistic guarantee of avoiding memory errors. For additional safety, DieHard can operate in a replicated mode where multiple replicas of the same application are run simulta- neously. By initializing each replica with a different random seed and requiring agreement on output, the replicated version of Die- Hard increases the likelihood of correct execution because errors are unlikely to have the same effect across all replicas.We present analytical and experimental results that show DieHardâs resilience to a wide range of memory errors, including a heap-based buffer overflow in an actual application.},
annote = {Core idea is to expand the heap so as to hide buffer overflows and dangling pointer errors.  Also look at running multiple instances of the program with differently randomised heaps, for further resilience.

        
Aim is to probabilistically emulate an infinite heap by just putting bloody great gaps between every allocation and only reusing stuff after a large random delay.

        
Kind of assume that their randomisation is the only source of non-determinancy in the protected application.

        
They try to derive some probabilities, but they need to make some highly implausible assumptions about value independence in order to do so, and I don't think that the results are terribly worthwhile.

      },
author = {Berger, ED and Zorn, BG},
booktitle = {PLDI},
file = {:home/sos22/papers/random/p158-berger.pdf:pdf},
publisher = {ACM},
title = {{DieHard: probabilistic memory safety for unsafe languages}},
year = {2006}
}
@article{Trainin2009,
author = {Trainin, E and Nir-Buchbinder, Y and Tzoref-Brill, R},
journal = {Proceedings of the  },
title = {{Forcing small models of conditions on program interleaving for detection of  }},
url = {http://portal.acm.org/citation.cfm?id=1639622.1639629},
year = {2009}
}
@article{Chew2010,
author = {Chew, L and Lie, D},
file = {::},
title = {{Kivati: Fast Detection and Prevention of Atomicity Violations}},
url = {http://www.eecg.toronto.edu/\~{}lie/papers/chew-eurosys2010-web.pdf},
year = {2010}
}
@article{Weeratunge2010,
author = {Weeratunge, D and Zhang, X and Jagannathan, S},
file = {::},
title = {{Analyzing Multicore Dumps to Facilitate Concurrency Bug Reproduction}},
url = {http://www.cs.purdue.edu/homes/suresh/papers/asplos10.pdf},
year = {2010}
}
@article{Jeffrey2009,
author = {Jeffrey, DB},
file = {::},
title = {{Dynamic State Alteration Techniques for Automatically Locating Software  }},
url = {http://www.cs.ucr.edu/\~{}gupta/Thesis/dennis.pdf},
year = {2009}
}
@article{Zhou,
author = {Zhou, SPSLY},
file = {::},
journal = {opera.ucsd.edu},
title = {{CTrigger: Exposing Atomicity Violation Bugs from Their Hiding Places}},
url = {http://opera.ucsd.edu/paper/asplos092-zhou.pdf}
}
@inproceedings{Kiriansky,
abstract = {We introduce program shepherding, a method for moni- toring control flowtransfers during program execution to enforce a security policy. Program shepherding provides three techniques as building blocks for security policies. First, shepherding can restrict execution privileges on the basis of code origins. This distinction can ensure that malicious code masquerading as data is never exe- cuted, thwarting a large class of security attacks. Sec- ond, shepherding can restrict control transfers based on instruction class, source, and target. For example, shep- herding can forbid execution of shared library code ex- cept through declared entry points, and can ensure that a return instruction only targets the instruction after a call. Finally, shepherding guarantees that sandboxing checks placed around any type of program operation will never be bypassed. We have implemented these capabilities efficiently in a runtime system with minimal or no per- formance penalties. This system operates on unmodified native binaries, requires no special hardware or operat- ing system support, and runs on existing IA-32 machines under both Linux andWindows.},
address = {San Francisco},
annote = {Apply privileges based on RIP, classified according to which binary it came from and whether it's been modified since then.  Also enforce that only certain pre-approved control flow transfers can happen.  Done dynamically with DynamoRIO.  Handles dlopen() quite badly.  Does a little bit better with indirect calls, but not much.

        
Seems to be pretty effective at stopping attacks.  Overhead is pretty reasonable: RIO imposes a few tens of percent, and the additional overhead is usually only a couple of percent above that.  Quite a long tail, though, with one benchmark showing 80\% increase in run time.

      },
author = {Kiriansky, V and Bruening, D and Amarasinghe, S},
booktitle = {Proceedings of the 11th USENIX Security Symposium},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiriansky, Bruening, Amarasinghe - Unknown - Secure execution via program shepherding.pdf:pdf},
publisher = {USENIX Association},
title = {{Secure execution via program shepherding}},
url = {http://www.usenix.org/events/sec02/full\_papers/kiriansky/kiriansky.pdf},
year = {2002}
}
@inproceedings{Sidiroglou2005,
abstract = {We propose a new approach for reacting to a wide variety of software failures, ranging from remotely exploitable vulnerabilities to more mundane bugs that cause abnormal program termination (e.g., il- legal memory dereference). Our emphasis is in cre- ating âself-healingâ software that can protect itself against a recurring fault until a more comprehensive fix is applied. Our system consists of a set of sensors that mon- itor applications for various types of failure and an instruction-level emulator that is invoked for se- lected parts of a programâs code. Use of such an em- ulator allows us to predict recurrences of faults, and recover program execution to a safe control flow. Using the emulator for small pieces of code, as di- rected by the sensors, allows us to minimize the per- formance impact on the immunized application. We discuss the overall system architecture and a prototype implementation for the x86 platform. We evaluate the efficacy of our approach against a range of attacks and other software failures and investigate its performance impact on several server-type appli- cations. We conclude that our system is effective in preventing the recurrence of a wide variety of soft- ware failures at a small performance cost.},
annote = {Observe crashes and try to make them not happen.  Fixes are just to make functions return with error values, and hence to trigger existing error recovery logic.  Essentially the same error virtualisation approach as rescue points.

        
They claim their approach works for non-crash bugs, but you have to be able to detect the error on the instruction where it happens, which pretty much means crash bugs unless you're doing lots of extra validation, which they don't do.

        
Error recovery is at function granularity, and involves emulating that function instruction-by-instruction.  Heuristic for figuring out error return values is quite stupid: just look at the return type, and use -1 for int, 0 for unsigned, or NULL for pointers.  Minimal filtering to ensure that those are vaguely sane.

        
Needs source code.

        
Eval looks pretty thorough.  Performance overhead of emulation is predictably massive (factor of several hundred).  They argue that you don't actually need to emulate very much, but don't provide much evidence to substantiate that claim.},
author = {Sidiroglou, S and Locasto, ME and Boyd, SW},
booktitle = {Proceedings of the USENIX Annual Technical Conference},
file = {:home/sos22/papers/random/10.1.1.59.1007.pdf:pdf},
keywords = {reactive immune system},
mendeley-tags = {reactive immune system},
pages = {149--161},
publisher = {Usenix},
title = {{Building a reactive immune system for software services}},
url = {http://portal.acm.org/citation.cfm?id=1247371},
year = {2004}
}
@misc{Locasto,
abstract = {One promising technique for defending software systems against vulnerabilities involves the use of selfâhealing. Such efforts, however, carry a great deal of risk because they largely bypass the cycle of humanâdriven patching and testing used to vet both vendor and internally de- veloped patches. In particular, it is difficult to predict if a repair will keep the behavior of the system consistent with ânormalâ behavior. Assuring that postârepair be- havior does not deviate from normal behavior is a major challenge to which no satisfactory solutions exist. We investigate the feasibility of automatically mea- suring behavioral deviations in software that has under- gone a selfâhealing repair. We provide a first exami- nation of the problem of assessing a repairâs impact on execution behavior, and we define a model for represent- ing postârepair behavior using machineâlevel intraproce- dural control flow. In essence, we advocate performing anomaly detection on an applicationâs execution after it has been attacked and subsequently repaired. Our goal, however, is not to detect an attack, but rather to provide a tool for assisting a system administrator to perform vul- nerability triage. Our system can help them discover the relative impact of a repair so that they can begin to track down and analyze the cause of postârepair execu- tion anomalies.},
address = {Fairfax, VA},
annote = {Look at how semantics-preserving self healing techniques are.  Propose techniques for doing so, rather than evaluating existing systems.

        
Their basic technique is just to do anomaly detection on a control flow trace, which is a bit damp.  Don't consider the effects of code relocation due to e.g. linker artifacts or recompilation.

        
Use strace to collect control flow information?

        
Eval is odd; cherry-picked subsets of a couple of benchmarks, with no indication of why that subset was chosen.

        
Pretty weak.

        

      },
author = {Locasto, ME and Stavrou, A and Cretu, GF},
booktitle = {cs.gmu.edu},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Locasto, Stavrou, Cretu - Unknown - Life After Self-Healing Assessing Post-Repair Program Behavior.pdf:pdf},
institution = {George Mason University},
title = {{Life After Self-Healing: Assessing Post-Repair Program Behavior}},
url = {http://cs.gmu.edu/\~{}mlocasto/papers/GMU-CS-TR-2008-3.pdf},
year = {2008}
}
@article{Demsky2003,
author = {Demsky, B and Rinard, M},
journal = {Proceedings of the 18th annual ACM  },
title = {{Automatic detection and repair of errors in data structures}},
url = {http://portal.acm.org/citation.cfm?id=949314},
year = {2003}
}
@article{Ramaswamy2010,
author = {Ramaswamy, A and Bratus, S and Locasto, ME},
file = {::},
journal = {Proceedings of the 4th  },
title = {{Katana: A Hot Patching Framework for ELF Executables}},
url = {http://cs.gmu.edu/\~{}mlocasto/papers/secse-main-final.pdf},
year = {2010}
}
@inproceedings{Sidiroglou2009,
abstract = {Software failures in server applications are a significant problem for preserving system availability. We present AS- SURE, a system that introduces rescue points that recover software from unknown faults while maintaining both sys- tem integrity and availability, by mimicking system behav- ior under known error conditions. Rescue points are loca- tions in existing application code for handling a given set of programmer-anticipated failures, which are automatically repurposed and tested for safely enabling fault recovery from a larger class of (unanticipated) faults. When a fault occurs at an arbitrary location in the program, ASSURE restores execution to an appropriate rescue point and in- duces the program to recover execution by virtualizing the programs existing error-handling facilities. Rescue points are identified using fuzzing, implemented using a fast co- ordinated checkpoint-restart mechanism that handles multi- process and multi-threaded applications, and, after testing, are injected into production code using binary patching.We have implemented an ASSURE Linux prototype that oper- ates without application source code and without base op- erating system kernel changes. Our experimental results on a set of real-world server applications and bugs show that ASSURE enabled recovery for all of the bugs tested with fast recovery times, has modest performance overhead, and provides automatic self-healing orders of magnitude faster than current human-driven patch deployment methods.},
annote = {Try to repurpose existing error handling in programs to handle lager classes of errors (e.g. recovering from a race by pretending that a file access failed).  Error handling points are referred to as recue points, and are discovered using fuzz testing.  When a fault happens, they roll back to a checkpoint and force the application to go through a failure point, using some amount of binary patching (?).

        
They're considering multi-process as well as multi-thread applications, which is a nice touch.

        
They only consider bugs which have been seen, and have a semi-offline process for finding fixes for observed bugs.

        
Their fixes are always of the form:

        
-- Take snapshot every time you go through point X.
-- Insert check at point Y.
-- If check fails, roll back to snapshot of X and make it return an error.

        
Always pick the closest rescue point for rollback.

        
Limited discussion of multi-threading issues?  All of their test bugs are deterministic.

        
There's an assumption that their fuzzer only generates error runs, which isn't completely unreasonable.

        
Identification of error values is simple but apparently effective: just use NULL or special integer markers.

        
Do some amount of testing to check that rollback is semantically valid, but it's not clear how much.

        
Performance results are good: usually recover from faults in a few tens of milliseconds, and take a minute or two to generate the fixes.  Overhead is on the order of five to ten percent when no faults present.  Overhead with patches in place was higher; \~{}5-20\%.},
author = {Sidiroglou, S and Laadan, O and Perez, C and Viennot, N and Nieh, J and Keromytis, AD},
booktitle = {ASPLOS09},
file = {:home/sos22/papers/random/p37-sidiroglou.pdf:pdf},
keywords = {Binary Patching,Checkpoint Restart,Error Recovery,Reli- able Software,Software Self-healing},
pages = {37--48},
publisher = {ACM},
title = {{Assure: automatic software self-healing using rescue points}},
url = {http://portal.acm.org/citation.cfm?id=1508250},
year = {2009}
}
@article{Rinard2005,
author = {Rinard, M and Cadar, C and Nguyen, HH},
journal = { to the 20th annual ACM SIGPLAN },
title = {{Exploring the acceptability envelope}},
url = {http://portal.acm.org/citation.cfm?id=1094855.1094866},
year = {2005}
}
@inproceedings{Nagarajan2009,
abstract = {It is important that long running server programs retain availabil- ity amidst software failures. However, server programs do fail and one of the important causes of failures in server programs is due to memory errors. Software bugs in the server code like buffer over- flows, integer overflows, etc. are exposed by certain user requests, leading to memory corruption, which can often result in crashes. One safe way of recovering from these crashes is to periodically checkpoint program state and rollback to the most recent check- point on a crash. However, checkpointing program state periodi- cally can be quite expensive. Furthermore, since recovery can in- volve the rolling back of considerable state information in addition to replay of several benign user requests, the throughput and re- sponse time of the server can be reduced significantly during roll- back recovery. In this paper, we first conducted a detailed study to see how memory corruption propagates in server programs. Our study shows that memory locations that are corrupted during the pro- cessing of an user request, generally do not propagate across user requests. On the contrary, the memory locations that are corrupted are generally cleansed automatically, as memory (stack or the heap) gets deallocated or when memory gets overwritten with uncor- rupted values. This self cleansing property in server programs led us to believe that recovering from crashes does not necessarily re- quire the expensive roll back of state for recovery. Motivated by this observation, we propose SRS, a technique for self recovery in server programs which takes advantage of self-cleansing to recover from crashes. Those memory locations that are not fully cleansed are restored in a demand driven fashion, which makes SRS very efficient. Thus in SRS, when a crash occurs instead of rolling back to a safe state, the crash is suppressed and the program is made to execute forwards past the crash; we employ a mechanism called crash suppression, to prevent further crashes from recurring as the execution proceeds forwards. Experiments conducted on realworld server programs with real bugs, show that in each of the cases the server program could efficiently recover from the crash and the faulty user request was isolated from future benign user requests. Categories and Subject Descriptors D.4.5 [Operating Systems]},
annote = {Look at memory corruption bugs, and discover that in server processes most memory errors don't propagate very far at all, and get cleansed quite quickly.  Present a way of recovering from memory errors based on suppressing certain accesses.

        
Use a dynamic taint analysis to figure out how corruptions actually propagate: pick some store instruction and taint whatever it writes to, then track how the taint propagates in the usual way.  Result is that for \~{}90\% of stores the taint eventually goes away, but the few stores which do cause lasting taint tend to cause a lot of it: \~{}5\% taint more than a thousand locations at the end of the test.  Non-trivial application dependence, though: Apache shows \~{}25\% of stores cause lasting taint.  Note that very few memory locations are shared between requests in any of their programs, and that the programs with the most sharing show the most taint propagation.

        
No real indication of how important these corruptions are, but nevermind.

        
Their crash recovery scheme works like this: when you get a bad load, flag the target register as being corrupt and suppress the instruction, then suppress any following instruction which uses the corrupt register but mark its target as corrupt as well.  Indirect jumps to corrupt locations are resolved by making something up based on profiling information.  Most locations then self-cleanse at the end of the request.  Discover the remainder by profiling beforehand and handle them by explicit snapshotting.

        
Overhead is amazingly low, less than ten percent, in a prototype based on dynamoRIO.  They only tested with four bugs, but they recovered successfully from all of them.  So that's pretty cool.

        
I like this paper.

      },
author = {Nagarajan, V and Jeffrey, D and Gupta, R},
booktitle = {ISMM09},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagarajan, Jeffrey, Gupta - 2009 - Self-recovery in server programs.pdf:pdf},
keywords = {memory propagation,self cleansing,self recovery},
publisher = {ACM},
title = {{Self-recovery in server programs}},
url = {http://portal.acm.org/citation.cfm?id=1542439},
year = {2009}
}
@inproceedings{Tallam2008a,
abstract = {We present an online framework to capture and recover from program failures and prevent them from occurring in the future through safe execution perturbations. The per- turbations are safe as they respect the semantics of the pro- gram. We use a checkpointing/logging mechanism to cap- ture a programexecution to an event log. If the execution re- sults in a failure, the framework automatically searches for perturbation of the execution by altering the event log and replaying the execution using the altered log to avoid the failure. If found, the perturbation is recorded as a dynamic patch, which is later applied by all future executions of this application to prevent the failure from occurring again. Our experiments show that the proposed framework is very ef- fective in avoiding concurrency faults, heap memory over- flow faults, and malicious requests. The entailed overhead for normal execution is very low (2-18\%).},
annote = {Consider atomicity violations, buffer overflows, and malformed user requests.

        
Use checkpoint/logging to capture failures, then tries a bunch of different schedules to find one which doesn't crash.  Actually, they go a bit beyond just checking for crash by letting the user provide an oracle of correctness.  Fixes just consist of upping the priority of some thread for a few instructions; not sure how that's supposed to work.

        
Ah ha: there's a throwaway line at the end of 3.0 that they only consider uniprocessor execution, which is a bit damp.

        
The multithreading bits are kind of stupid because of the uniprocessor consideration, and their fixes really aren't very clever.  The rest of it is a little bit better.},
author = {Tallam, S and Tian, C and Gupta, R},
booktitle = {Proceedings of the 2008 32nd Annual IEEE International Computer Software and Applications Conference},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tallam, Tian, Gupta - 2008 - Avoiding program failures through safe execution perturbations.pdf:pdf},
pages = {152----159},
publisher = {IEEE Computer Society},
title = {{Avoiding program failures through safe execution perturbations}},
url = {http://doi.ieeecomputersociety.org/10.1109/COMPSAC.2008.23},
year = {2008}
}
@article{Novark2007,
abstract = {Programs written in C and C++ are susceptible to memory er- rors, including buffer overflows and dangling pointers. These er- rors, which can lead to crashes, erroneous execution, and security vulnerabilities, are notoriously costly to repair. Tracking down their location in the source code is difficult, even when the full memory state of the program is available. Once the errors are finally found, fixing them remains challenging: even for critical security-sensitive bugs, the average time between initial reports and the issuance of a patch is nearly one month. We present Exterminator, a system that automatically corrects heap-based memory errors without programmer intervention. Ex- terminator exploits randomization to pinpoint errors with high precision. From this information, Exterminator derives runtime patches that fix these errors both in current and subsequent execu- tions. In addition, Exterminator enables collaborative bug correc- tion by merging patches generated by multiple users. We present analytical and empirical results that demonstrate Exterminators ef- fectiveness at detecting and correcting both injected and real faults.},
annote = {Exterminator: detect and correct heap-based memory errors.  Works on the binary, and is completely automated.  Tries to detect overflows and premature free().  Fixes programs with automatically generated binary patches.  Refinement of DieHard.

        
Interesting approach to canaries: fill memory with one of a small number of random values, rather than using a fixed canary, so that you can guess the source of a wild read post mortem.

        
There's apparently enough information in the heap to make a pretty good stab at guessing how large a buffer you actually needed in order to fix an overflow, and so it can make up a patch which adds a few more bytes of trailer to the allocations.
Fixing premature free()s is a bit more dodgy: just double the time for which the allocation remains live.  Not completely unreasonable, I guess.

        
Not sure why they're doing this with patches; it'd be really easy to do it in their allocator library.

        
Overhead is pretty high.  It looks like maybe a factor of five on allocator operations themselves, but much less on realistic benchmarks.

        
Kind of nice.  It's basically what I was going to do when I was thinking about failure hiding memory allocators.

      },
author = {Novark, G and Berger, ED and Zorn, BG},
file = {:home/sos22/papers/random/p1-novark.pdf:pdf},
journal = {Proceedings of the 2007 ACM SIGPLAN conference on Programming Language Design and Implementation},
keywords = {DieFast,Exterminator,dynamic memory allocation,error correction,memory errors,probabilistic memory safety,ran- domized algorithms},
title = {{Exterminator: Automatically correcting memory errors with high probability}},
year = {2007}
}
@article{Costa2007,
author = {Costa, M and Castro, M and Zhou, L and Zhang, L},
journal = {Proceedings of twenty- },
title = {{Bouncer: Securing software by blocking bad input}},
url = {http://portal.acm.org/citation.cfm?id=1294274},
year = {2007}
}
@article{Rinard2007,
abstract = {We present a set of automated techniques that enable software systems to survive otherwise fatal errors such as memory leaks, infinite loops, and addressing errors.},
annote = {Short position paper.  Give a couple of quite simple techniques with a flavour of failure-obliviousness, and a two-sentence description of each.  Nothing very exciting here.

        

      },
author = {Rinard, M},
file = {:home/sos22/papers/random/suriving\_fatal\_errors.pdf:pdf},
journal = {Electronic Notes in Theoretical Computer Science},
title = {{Automated Techniques for Surviving (Otherwise) Fatal Software Errors}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1571066107001995},
year = {2007}
}
@inproceedings{Keromytis2007,
abstract = {The introduction of self-healing capabilities to software systems could offer a way to alter the current, unfavorable imbalance in the software security arms race. Consequently, self-healing software systems have emerged as a re- search area of particular interest in recent years. Motivated by the inability of traditional techniques to guarantee software integrity and availability, especially against motivated human adversaries, self-healing approaches are meant to com- plement existing approaches to security. In this paper, we provide a first attempt to characterize self-healing software sys- tems by surveying some of the existingwork in the field.We focus on systems that effect structural changes to the software under protection, as opposed to block- level system reconfiguration. Our goal is to begin mapping the space of software self-healing capabilities. We believe this to be a necessary first step in explor- ing the boundaries of the research space and understanding the possibilities that such systems enable, as well as determining the risks and limitations inherent in automatic-reaction schemes.},
annote = {Many typesetting errors.  Lots of verbiage.  Seems to be the usual software engineering garbage.  Moderately useful summary of the existing software healing techniques in section 4.1.

        
Deeply unclear what the actual contribution of this paper is.},
author = {Keromytis, AD},
booktitle = {4th International Conference on Mathematical Methods, Models and Architectures for Computer Networks Security},
file = {:home/sos22/papers/random/10.1.1.71.391.pdf:pdf},
keywords = {Self-healing,availability,reliability,software security},
title = {{Characterizing self-healing software systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.391\&rep=rep1\&type=pdf},
year = {2007}
}
@article{Rinard2006,
author = {Rinard, M},
journal = {Proceedings of the 20th annual international  },
title = {{Probabilistic accuracy bounds for fault-tolerant computations that discard tasks}},
url = {http://portal.acm.org/citation.cfm?id=1183447},
year = {2006}
}
@article{Park2009a,
author = {Park, S and Lu, S and Zhou, Y},
journal = {ACM SIGPLAN Notices},
title = {{Ctrigger: Exposing atomicity violation bugs from their hiding places}},
url = {http://portal.acm.org/citation.cfm?id=1508284.1508249},
year = {2009}
}
@article{Sen2008,
author = {Sen, K},
file = {::},
journal = {ACM SIGPLAN Notices},
title = {{Race directed random testing of concurrent programs}},
url = {http://portal.acm.org/citation.cfm?id=1375584},
year = {2008}
}
@article{Ceze2007,
author = {Ceze, L and Tuck, J and Montesinos, P},
journal = {ACM SIGARCH Computer  },
title = {{BulkSC: bulk enforcement of sequential consistency}},
url = {http://portal.acm.org/citation.cfm?id=1273440.1250697},
year = {2007}
}
@article{Muzahid2009,
author = {Muzahid, A and Su\'{a}rez, D and Qi, S and Torrellas, J},
file = {::},
title = {{SigRace: signature-based data race detection}},
url = {http://portal.acm.org/citation.cfm?id=1555815.1555797},
year = {2009}
}
@article{Ronsse1999,
author = {Ronsse, M and Bosschere, K De},
file = {::},
journal = {ACM Transactions on Computer  },
title = {{RecPlay: A fully integrated practical record/replay system}},
url = {http://portal.acm.org/citation.cfm?id=312203.312214\&dl=portal\&dl=ACM\&idx=J774\&part=periodical\&WantType=periodical\&title=ACM Transactions on Computer Systems (TOCS)},
year = {1999}
}
@article{Narayanasamy2006,
author = {Narayanasamy, S and Pereira, C and Calder, B},
journal = {Proceedings of the 12th  },
title = {{Recording shared memory dependencies using strata}},
url = {http://portal.acm.org/citation.cfm?id=1168886},
year = {2006}
}
@article{Sack2006,
author = {Sack, P and Bliss, BE and Ma, Z and Petersen, P},
journal = {Proceedings of the 1st  },
title = {{Accurate and efficient filtering for the intel thread checker race detector}},
url = {http://portal.acm.org/citation.cfm?id=1181309.1181315},
year = {2006}
}
@inproceedings{Huang1995,
annote = {Considering long-running server processes, primarily in a telecoms context.  Assert that processes age over time as memory leaks etc. build up.  Fix this by periodically restarting the application process from a known-good snapshot.

        
There's some slightly dubious statistics showing that preemptive maintenance is sometimes useful, but I don't believe their underlying model enough to put much faith in it.

        
Meh.

        
Bazillions of cites, but they're all just trying to calculate optimum reboot schedules, which is monumentally tedious.

      },
author = {Huang, Y and Kintala, C and Kolettis, N and Fulton, ND},
booktitle = {FTCS},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 1995 - Software rejuvenation Analysis, module and applications.pdf:pdf},
pages = {381--381},
publisher = {IEEE Computer Society Press},
title = {{Software rejuvenation: Analysis, module and applications}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/FTCS.1995.466961},
year = {1995}
}
@article{Chandra2000a,
author = {Chandra, S and Chen, PM},
journal = {Proceedings of the 2000 IEEE },
title = {{Whither generic recovery from application faults? A fault study using open-source  }},
url = {http://doi.ieeecomputersociety.org/10.1109/ICDSN.2000.857521},
year = {2000}
}
@article{Rinard2003,
abstract = {We discuss a new approach to the construction of software systems. Instead of attempting to build a system that is as free of errors as possible, the designer instead identifies key properties that the execution must satisfy to be accept- able to its users. Together, these properties define the ac- ceptability envelope of the system: the region that it must stay within to remain acceptable. The developer then aug- ments the system with a layered set of components, each of which enforces one of the acceptability properties. The potential advantages of this approach include more flexible, resilient systems that recover from errors and behave accept- ably across a wide range of operating environments, an ap- propriately prioritized investment of engineering resources, and the ability to productively incorporate unreliable com- ponents into the final software system.},
annote = {Argue that the aim should not be to produce perfect software, but instead to produce acceptable software.  This means definining what, precisely, acceptable means.  Basic idea is to structure the system in rings, with the innermost ring being a conventionally-engineered implementation.  Outer rings assume that inner rings are buggy, and try to filter their interactions with the world to enforce some basic sanity constraint.  All of this has to be done manually.

        
Very close to the privsep idea, but applied to arbitrary correctness properties rather than just security things.

        
His examples don't really match up with his stated plan of action: fix specific bugs in specific ways, rather than enforcing correct partially correct behaviour.  A lot of his ideas look pretty crazy, and have the potential to induce a lot of archeology.

        
Not entirely convinced by this one.  Lots of software engineering and not much computer science.

      },
author = {Rinard, M},
file = {:home/sos22/papers/random/p57-rinard.pdf:pdf},
journal = {ACM SIGPLAN Notices},
keywords = {Acceptability Properties,Monitoring,Rectificatio,Repair},
number = {12},
pages = {57--75},
title = {{Acceptability-oriented computing}},
url = {http://portal.acm.org/citation.cfm?id=966060},
volume = {38},
year = {2003}
}
@article{Choi1991,
author = {Choi, JD and Miller, BP and Netzer, RHB},
file = {::},
journal = {ACM Transactions on  },
title = {{Techniques for debugging parallel programs with flowback analysis}},
url = {http://portal.acm.org/citation.cfm?id=115324\&dl=},
year = {1991}
}
@article{Brumley2007a,
author = {Brumley, D and Wang, H and Jha, S and Song, D},
journal = {Proceedings of the IEEE Computer  },
title = {{Creating vulnerability signatures using weakest pre-conditions}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.4800\&rep=rep1\&type=pdf},
year = {2007}
}
@article{Sidiroglou2005a,
author = {Sidiroglou, S and Giovanidis, G and Keromytis, AD},
journal = {Lecture notes in computer  },
title = {{A dynamic mechanism for recovering from buffer overflow attacks}},
url = {http://www.springerlink.com/index/91uvhk8d0y8h69tx.pdf},
year = {2005}
}
@inproceedings{Flanagan2005b,
abstract = {Multithreaded programs are notoriously prone to synchronization errors.  Much prior work has tackled the problem of detecting sych errors.  This paper focuses on the subsequent problem of sycnrhonization correction.  We present a constraint-based analysis that, given an erroneous program, automatically infers (where possible) what additional locking operations should be inserted in order to yield a correctly-synchronized program.  For performance reasons, our algorithm also attempts to minimize the number of additional lock acquires and the duration for which the acquired locks are held.  We present experimental results that validate this approach on a number of standard Java library classes.},
annote = {Requires the programmer to flag some methods as atomic, and then adds enough additional synchronization to make it so.  Rely on the programmer to manually check that the suggested fix is correct.  Works at the source level and is specific to Java.  Kind of assume that atomic blocks are always whole functions.

        
They've randomly invented a new syntax for Java, just to keep things interesting.

        
Algorithm for doing this is kind of stupid: essentially unification on synchronisation variables with a little bit of analysis to find boundaries.  Seems to be entirely static; no dynamic analysis at all.  Lots of typing rules and all that crap.  Not very intresting, if we're honest.  Pretty much obsoleted by STM.},
author = {Flanagan, C. and Freund, S.N.},
booktitle = {Synchronization and Concurrency in Object-Oriented Languages (SCOOL)},
file = {:home/sos22/papers/random/10.1.1.133.1908.pdf:pdf},
publisher = {ACM},
title = {{Automatic synchronization correction}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.1908\&amp;rep=rep1\&amp;type=pdf},
volume = {19},
year = {2005}
}
@article{Cunningham2008a,
author = {Cunningham, D and Gudka, K and Eisenbach, S},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cunningham, Gudka, Eisenbach - 2008 - Keep off the grass Locking the right path for atomicity.pdf:pdf},
journal = {Lecture Notes in Computer  },
title = {{Keep off the grass: Locking the right path for atomicity}},
url = {http://www.springerlink.com/index/4844g510834v4085.pdf},
year = {2008}
}
@inproceedings{Locasto2008,
abstract = {Current embryonic attempts at software selfâhealing produce mech- anisms that are often oblivious to the semantics of the code they supervise. We believe that, in order to help inform runtime repair strategies, such systems re- quire a more detailed analysis of dynamic application behavior.We describe how to profile an application by analyzing all function calls (including library and system) made by a process.We create predictability profiles of the return values of those function calls. Selfâhealing mechanisms that rely on a transactional ap- proach to repair (that is, rolling back execution to a known safe point in control flow or slicing off the current function sequence) can benefit from these return value predictability profiles. Profiles built for the applications we tested can pre- dict behavior with 97\% accuracy given a context window of 15 functions. We also present a survey of the distribution of actual return values for real software as well as a novel way of visualizing both the macro and micro structure of the return value distributions. Our system helps demonstrate the feasibility of com- bining binaryâlevel behavior profiling with selfâhealing repairs.},
annote = {Essentially a repeat of the Lugrind paper by the same authors, but using Pin rather than Valgrind, and two years later.

      },
author = {Locasto, ME and Stavrou, A and Cretu, GF and Keromytis, A.D. and Stolfo, S.J.},
booktitle = {Proceedings of the 3rd International Workshop on Security: Advances in Information and Computer Security},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Locasto, Stavrou, Cretu - 2008 - Return Value Predictability Profiles for SelfHealing.pdf:pdf},
keywords = {Lugrind,anonamly detection,behavior profiling,self-healing},
mendeley-tags = {Lugrind},
pages = {166},
publisher = {Springer},
title = {{Return Value Predictability Profiles for SelfâHealing}},
url = {http://www.springerlink.com/index/k317274503q56h81.pdf},
year = {2008}
}
@inproceedings{Chandra2002,
abstract = {Recovery systems must save state before a failure occurs to enable the system to recover from the failure. However, recovery will fail if the recovery system saves any state corrupted by the fault. The frequency and comprehensive- ness of how a recovery system saves state has a major effect on how often the recovery system inadvertently saves corrupted state. This paper explores and measures that effect. We measure how often software faults in the application and operating system cause real applications to save corrupted state when using different types of recov- ery systems.We find that generic recovery techniques, such as checkpointing and logging, work well for faults in the operating system. However, we find that they do not work well for faults in the application because the very actions taken to enable recovery often corrupt the state upon which successful recovery depends.},
annote = {It's another bloody micro-reboot/software rejuvenation thing.

      },
author = {Chandra, S and Chen, PM},
booktitle = {Proceedings of the 13th International Symposium on Software Reliability Engineering},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandra, Chen - 2002 - The impact of recovery mechanisms on the likelihood of saving corrupted state.pdf:pdf},
pages = {91},
publisher = {IEEE Computer Society},
title = {{The impact of recovery mechanisms on the likelihood of saving corrupted state}},
url = {http://doi.ieeecomputersociety.org/10.1109/ISSRE.2002.1173219},
year = {2002}
}
@article{Liu2005,
author = {Liu, C and Yan, X and Fei, L and Han, J and Midkiff, SP},
file = {::},
journal = {ACM SIGSOFT Software  },
title = {{SOBER: statistical model-based bug localization}},
url = {http://portal.acm.org/citation.cfm?id=1095430.1081753},
year = {2005}
}
@misc{Locasto2006,
abstract = {The increasing sophistication of software attacks has created the need for increasingly finer-grained intrusion and anomaly detection systems, both at the network and the host level. We believe that the next generation of defense mechanisms will require a much more detailed dynamic analysis of application behavior than is currently done. We also note that the same type of behavior analysis is needed by the current embryonic attempts at self-healing systems. Because such mechanisms are currently perceived as too expensive in terms of their performance impact, questions relating to the feasibility and value of such analysis remain unexplored and unanswered. We present a new mechanism for profiling the behavior space of an application by analyzing all function calls made by the process, including regular functions and library calls, as well as system calls. We derive behavior from aspects of both control and data flow. We show how to build and check profiles that contain this information at the binary level â that is, without making changes to the applicationâs source, the operating system, or the compiler. This capability makes our system, Lugrind, applicable to a variety of software, including COTS applications. Profiles built for the applications we tested can predict behavior with 97\% accuracy given a context window of 15 functions. Lugrind demonstrates the feasibility of combining binary-level behavior profiling with detection and automated repair.},
annote = {I'm not awake enough to eval this properly.

        
Hack up Valgrind to gather a trace of some interesting events as the application executes.  This allows them to build variant of a backtrace with another time-like dimension (can see what happened previously in each call frame, in addition to the call frame stack itself).  Discover that these are pretty good predictors of return values.  Doesn't look immensely relevant to anything.

        
There's another variant of this paper also tagged Lugrind.

      },
author = {Locasto, ME and Stavrou, A and Cretu, GF and Stolfo, SJ and Keromytis, AD},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Locasto et al. - 2006 - Quantifying Application Behavior Space for Detection and Self-Healing.pdf:pdf},
institution = {Columbia University},
keywords = {Lugrind},
mendeley-tags = {Lugrind},
publisher = {Citeseer},
title = {{Quantifying Application Behavior Space for Detection and Self-Healing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.3003\&amp;rep=rep1\&amp;type=pdf},
year = {2006}
}
@inproceedings{Li2007,
abstract = {Traditionally, fault tolerance researchers have required architectural state to be numerically perfect for programex- ecution to be correct. However, in many programs, even if execution is not 100\%numerically correct, the programcan still appear to execute correctly from the userâs perspec- tive. Hence, whether a fault is unacceptable or benign may depend on the level of abstraction at which correctness is evaluated, with more faults being benign at higher levels of abstraction, i.e. at the user or application level, compared to lower levels of abstraction, i.e. at the architecture level. The extent to which programs are more fault resilient at higher levels of abstraction is application dependent. Pro- grams that produce inexact and/or approximate outputs can be very resilient at the application level. We call such pro- grams soft computations, and we find they are common in multimedia workloads, as well as artificial intelligence (AI) workloads. Programs that compute exact numerical outputs offer less error resilience at the application level. However, we find all programs studied in this paper exhibit some en- hanced fault resilience at the application level, including those that are traditionally considered exact computationsâ e.g., SPECInt CPU2000. This paper investigates definitions of program correct- ness that view correctness from the applicationâs stand- point rather than the architectureâs standpoint. Under application-level correctness, a programâs execution is deemed correct as long as the result it produces is ac- ceptable to the user. To quantify user satisfaction, we rely on application-level fidelity metrics that capture user- perceived program solution quality. We conduct a detailed fault susceptibility study that measures how much more fault resilient programs are when defining correctness at the application level compared to the architecture level. Our re- sults show for 6 multimedia and AI benchmarks that 45.8\% of architecturally incorrect faults are correct at the appli- cation level. For 3 SPECInt CPU2000 benchmarks, 17.6\% of architecturally incorrect faults are correct at the appli- cation level. We also present a lightweight fault recovery mechanism that exploits the relaxed requirements on nu- merical integrity provided by application-level correctness to reduce checkpoint cost. Our lightweight fault recovery mechanismsuccessfully recovers 66.3\%of programcrashes in our multimedia and AI workloads, while incurring mini- mum runtime overhead.},
annote = {Looking at applications' ability to tolerate and mask hardware faults automatically.  Only really consider numerical errors, and look at the magnitude of errors induced in outputs by a small error in computation.  This requires domain-specific knowledge of what output fidelity means.

        
Results show that, most of the time, most architecturally-visible processor faults lead to unacceptable behaviour, which is kind of sad, although there are a few applications which can tolerate up to about 50\% of errors (mostly multimedia programs).},
author = {Li, X and Yeung, D},
booktitle = {Proceedings of the 13th International Symposium on High Performance Computer Architecture},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Yeung - 2007 - Application-level correctness and its impact on fault tolerance.pdf:pdf},
pages = {181--192},
publisher = {Citeseer},
title = {{Application-level correctness and its impact on fault tolerance}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.1661\&amp;rep=rep1\&amp;type=pdf},
year = {2007}
}
@misc{Livshits,
abstract = {A great deal of research in the last several years has focused on securing sever-side software systems, which are a common target to buffer overruns, format string violations, and other similar types of attacks. A variety of techniques to protect server-side software have been suggested, ranging from hardware-level mechanisms [12, 36] to static analysis [23, 34, 37]. However, most of the code being written is ap- plication software, which has a totally different set of security requirements. In particular, application software is often written by programmers who not trained in security, suffer from a rushed development schedule, and exist in environment where features are emphasized over software quality. Automatic solu- tions such as static analysis may be too intrusive and out of place at organizations that lack a well- established development process. Moreover, rushed development schedules that favor features over ro- bustness leave little time to ensure the security of the system, which is often considered ânice to haveâ, but not a necessity. As a result, it is often impracti- cal to expect developers to take the responsibility for securing their applications. The reality of todayâs ap- plication development suggests that fully automatic security solutions that relieve applications develop- ers from the responsibility of securing their code are highly desirable. In this paper we argue for a generalized approach to protecting application software from a variety of exploits with minimal programmer intervention.},
annote = {Position paper.  Reads like a very early draft.  In fact, is a very early draft.},
author = {Livshits, B},
booktitle = {HotSec09},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Livshits - Unknown - Towards Seamless Prevention \& Recovery from Application-Level.pdf:pdf},
title = {{Towards Seamless Prevention \& Recovery from Application-Level}},
url = {http://research.microsoft.com/users/livshits/papers/pdf/hotsec06.pdf},
year = {2009}
}
@misc{Sumner,
abstract = {In this paper, we propose an automated debugging technique that explains a failure by computing its causal path leading from the root cause to the failure. Given a failing execution, the technique first searches for a dynamic patch. Fine-grained execution comparison be- tween the failing run and the patched run is performed to isolate the causal path. We introduce a formal sys- tem, wherein the corrected version of a faulty program is assumed so that the concept of ideal failure inducing chain (FIC) can be defined by comparing the failing run and the run on the corrected program using the same input. Properties of such chains are studied. A product of the formal system is a metric that serves in the ob- jective evaluation of the proposed technique. We iden- tify a key enabling technique called execution indexing, whose goal is to establish a mapping between equivalent points in two different executions so that comparison can be meaningfully performed. We show that a control structure based indexing scheme, when integrated into the formal system, demonstrates very nice properties that can be exploited to develop an effective and effi- cient debugging algorithm. The evaluation shows that the metric lives up to its promise, computing desired FICs, and the proposed approach is able to compute high quality FICs. The results of our technique signif- icantly supercede the state of the art.},
annote = {Trying to find out the causal chain of an observed failure.  Do this by looking at the failing run, generating a patch, and then comparing the patched run to the failing one.

        
There's a lot of formalism here which I'm not awake enough to understand.  I strongly suspect that it's there in large part to hide the fact that at the bottom it's all a bit woolly.

        
Their scheme for producing patches is really quite stupid: just flipping a branch predicate a few instructions back.  It's also not particularly well-described in this paper.

        
I don't know why I bothered reading this one.

      },
author = {Sumner, WN and Zhang, X},
booktitle = {cs.purdue.edu},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sumner, Zhang - Unknown - Automatic Failure Inducing Chain Computation through Aligned Execution.pdf:pdf},
institution = {Purdue University},
title = {{Automatic failure inducing chain computation through aligned execution comparison (Ammended)}},
url = {http://www.cs.purdue.edu/homes/wsumner/submission.pdf},
year = {2008}
}
@inproceedings{Nakajima,
abstract = {Device drivers are the major cause of operating system failure. Prior research proposed frameworks to improve the reliability of device drivers by means of driver restart. While avoiding any instrumentation of the driver, this ap- proach does not always allow graceful recovery. In this pa- per, we propose a framework for self-healing device drivers that lets the driver developer consider and implement the failure recovery of device drivers. For this purpose, our framework provides easy to use and light-weight persistent memory that preserves the state of the driver needed to suc- cessfully recover. We developped a prototype on top of the L4 microkernel, and were able to achieve full recovery of crashed drivers as fast as 0.2 ms for different device drivers. In all cases, recovery was totally transparent for the user.},
annote = {L4-based.  Mostly interested in embedded systems.  Programmer manually flags certain bits of data as critical, and then on a crash you copy those bits into a new instantiation of the driver and let it go again.  The paper has lots of engineering and implementation bits, and a few big ups for L4 and ArcOS, but nothing very useful.

        
Eval is pretty basic, using drivers which they wrote and artificial fault injection.

        
Not very exciting, really.},
author = {Ishikawa, H. and Courbot, A. and Nakajima, T.},
booktitle = {Second IEEE International Conference on Self-Adaptive and Self-Organizing Systems, 2008},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nakajima - Unknown - A Framework for Self-healing Device Drivers.pdf:pdf},
keywords = {P3},
mendeley-tags = {P3},
pages = {277--286},
publisher = {IEEE Comput. Soc},
title = {{A framework for self-healing device drivers}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:A+Framework+for+Self-healing+Device+Drivers\#0},
year = {2008}
}
@article{Hicks2006,
author = {Hicks, M and Foster, JS and Pratikakis, P},
file = {::},
journal = {TRANSACT'06},
title = {{Lock inference for atomic sections}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.3900\&rep=rep1\&type=pdf},
year = {2006}
}
@inproceedings{Chandra2000b,
abstract = {This paper tests the hypothesis that generic recovery techniques, such as process pairs, can survive most appli- cation faults without using application-specific informa- tion. We examine in detail the faults that occur in three, large, open-source applications: the Apache web server, the GNOME desktop environment, and the MySQL data- base. Using information contained in the bug reports and source code, we classify faults based on how they depend on the operating environment. We find that 72-87\% of the faults are independent of the operating environment and are hence deterministic (non-transient). Recovering from the failures caused by these faults requires the use of application-specific knowledge. Half of the remaining faults depend on a condition in the operating environment that is likely to persist on retry, and the failures caused by these faults are also likely to require application-specific recovery. Unfortunately, only 5-14\% of the faults were triggered by transient conditions, such as timing and syn- chronization, that naturally fix themselves during recovery. Our results indicate that classical application-generic recovery techniques, such as process pairs, will not be suf- ficient to enable applications to survive most failures caused by application faults.},
annote = {Partially contradicted by later work on fault healing.

        
Look at a bunch of bug reports from open-source projects and classify them as either deterministic or non-deterministic, and the deterministic ones as being environment-dependent or environment-independent.  The specifically exclude the workload from the environment, which is an odd decision.

        
Summary: most bugs depend primarily on the workload, with very few actually dependent on environmental factors.  Of environmentally-dependent bugs, about half were non-deterministic.  Not clear whether all of that's still true; things have changed a bit in the past ten years.

      },
author = {Chandra, S. and Chen, P.M.},
booktitle = {Proceeding International Conference on Dependable Systems and Networks. DSN 2000},
doi = {10.1109/ICDSN.2000.857521},
file = {:home/sos22/papers/random/whither\_generic\_recovery\_from\_application\_faults.pdf:pdf},
isbn = {0-7695-0707-7},
pages = {97--106},
publisher = {IEEE Comput. Soc},
title = {{Whither generic recovery from application faults? A fault study using open-source software}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=857521},
year = {2000}
}
@inproceedings{Xua,
abstract = {Software vulnerabilities have been the main contributing factor to the Internet security problems such as fast spread- ing worms. Among these software vulnerabilities, memory corruption vulnerabilities such as buffer overflow and for- mat string bugs have been the most common ones exploited by network-based attacks. Many security countermeasures (e.g., patching, automatic signature generation for intrusion detection systems) require vulnerability information to func- tion correctly. However, despite many years of research, automatically identifying unknown software vulnerabilities still remains an open problem. In this paper, we present the development of a security de- bugging tool named MemSherlock, which can automatically identify unknown memory corruption vulnerabilities upon the detection of malicious payloads that exploit such vulner- abilities. MemSherlock provides critical information for un- known memory corruption vulnerabilities, including (1) the corruption point in the source code (i.e., the statement that allows the exploitation of memory corruption vulnerability), (2) the slice of source code that helps the malicious input to reach the corruption point, and (3) the description of how the malicious input exploits the unknown vulnerability. We evaluate MemSherlock with a set of 11 real-world applica- tions that have buffer overflow, heap overflow, and format string vulnerabilities. The evaluation results indicate that MemSherlock is a useful tool to facilitate the automatic vul- nerability analysis process.},
address = {Alexandria, Virginia},
annote = {Want to look at an exploited program and automatically derive siganture for the vulnerability which was exploited.  Intended primarily as a debugging aide.  They claim to have disproved Rice's Theorem, which is unlikely.

        
Basic approach is to track who writes to each variable during normal operation, and then look for writes which happen during exploitation which aren't in the normal set.  Sounds pretty useless.  Also does some amount of equally useless static analysis.

        
Yet Another Valgrind Skin.

      },
author = {Sezer, C and Ning, P and Kil, C and Xu, J},
booktitle = {CCS07},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu - Unknown - MemSherlock An Automated Debugger for Unknown Memory Corruption \ldots.pdf:pdf},
keywords = {Debugging,Memory corruption,Vulnerability analysis},
publisher = {ACM},
title = {{MemSherlock: An Automated Debugger for Unknown Memory Corruption Vulnerabilities}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.7487\&rep=rep1\&type=pdf},
year = {2007}
}
@inproceedings{Pattabiraman2006,
abstract = {This paper proposes a novel technique for preventing a wide range of data errors from corrupting the execution of applications. The proposed technique enables automated derivation of fine-grained, application-specific error detectors. An algorithm based on dynamic traces of application execution is developed for extracting the set of error detector classes, parameters, and locations in order to maximize the error detection coverage for a target application. The paper also presents an automatic framework for synthesizing the set of detectors in hardware to enable low-overhead run- time checking of the application execution. Coverage (evaluated using fault injection) of the error detectors derived using the proposed methodology, the additional hardware resources needed, and performance overhead for several benchmark programs are also reported.},
annote = {Looking at picking up hardware-induced errors quickly, so not directly relevant.  There's some interesting stuff on how you place your assertions to catchthe maximum number of bugs for the minimum overhead, but most of its pushed to a cite.  Detectors are basically Daikon-style invariant discovery.

        
Lots of hardware bits and bobs, but not much of relevance to me here.

      },
author = {Pattabiraman, K and Saggese, GP and Chen, D and Kalbarczyk, Z. and Iyer, RK},
booktitle = {Proceedings of the Sixth European Dependable Computing Conference},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pattabiraman, Saggese, Chen - 2006 - Dynamic derivation of application-specific error detectors and their \ldots.pdf:pdf},
pages = {97--108},
publisher = {IEEE Computer Society},
title = {{Dynamic derivation of application-specific error detectors and their implementation in hardware}},
url = {http://www.crhc.illinois.edu/DEPEND/pubs/papers/EDCC06-final.pdf},
year = {2006}
}
@inproceedings{Fetzer,
abstract = {HEALERS is a practical, high-performance toolkit that can enhance the robustness and security of existing applications. For any shared library, it can find all func- tions defined in that library and automatically derives properties for those functions. Through automated fault- injection experiments, it can detect arguments that cause the library to crash and derive safe argument types for each function. The toolkit can prevent heap and stack buffer overflows that are a common cause of security breaches. The nice feature of the HEALERS approach is that it can protect existing applications without ac- cess to the source code.},
annote = {Present HEALERS, which is trying to robust-ify libraries automatically.  Essentially works by doing a kind of fault injection on the library to figure out what the actual requirements on its arguments are, and then wrapping them to do something.  They don't really describe what the wrappers do, though.  Not sure what the point of this paper was.

      },
author = {Fetzer, C and Xiao, Z},
booktitle = {Proceedings of the IEEE International Conference on Dependable Systems and Networks},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fetzer, Xiao - Unknown - HEALERS A toolkit for enhancing the robustness and security of existing \ldots.pdf:pdf},
keywords = {fault tol- erance,reliability,robustness,security,wrapper},
publisher = {IEEE},
title = {{HEALERS: A toolkit for enhancing the robustness and security of existing \ldots}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/DSN.2003.1209942},
year = {2003}
}
@inproceedings{Zhou2007,
annote = {Authors observe that it's sometimes useful to run two versions of a program and compare the results.  If the two versions are very similar, this is inefficient, so they suggest running the shared bits only once and doing a moderately complicated partial-merge to avoid redundant computations.  They use the word ``synergistic'', which has to count against them.

        
Not very relevant to what I'm doing.

      },
author = {Zhou, Y and Marinov, D and Sanders, W and Zilles, C and D'Amorim, M and Lauterberg, S and Lefever, RM and Tucek, J},
booktitle = {HOTDEP07},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2007 - Delta execution for software reliability.pdf:pdf},
publisher = {ACM},
title = {{Delta execution for software reliability}},
url = {http://portal.acm.org/citation.cfm?id=1323140.1323156},
year = {2007}
}
@inproceedings{Chiueh,
abstract = {Control-hijacking attacks exploit vulnerabilities in net- work services to take control of them and eventually their un- derlying machines. Although much work has been done on detection and prevention of control-hijacking attacks, most of them did not address the problem of repairing the attacked network services so as to prevent the same attacks from re- curring. Ideally, post-attack repair should consist of an at- tack signature generation component that creates a filtering rule for front-end firewall or intrusion prevention system to block the detected attack and its variants, and a patch gener- ation component that creates a fix to permanently eliminate the vulnerabilities that the detected attack exploits. This pa- per describes the design, implementation and evaluation of a program transformation and execution trace analysis system called PASAN that can automatically instrument the source code of network service programs in such a way that it can detect control-hijacking attacks and automatically generate patches to seal the vulnerability being exploited by the de- tected attack. We have implemented the first PASAN pro- totype as a GNU C compiler extension that aims at stack- based buffer overflow attacks but could be easily general- ized to accommodate other control-hijacking attacks. Test- ing this prototype with seven network daemon programs with known vulnerabilities show that the automatically generated patches can successfully fix the vulnerability. In addition, these patches are similar in their structure to those that are manually created. The run-time performance overhead of application programs instrumented by PASAN is between 10\% and 23\%, except two programs, whose CPU consump- tion is low.},
annote = {Want to generate patches for specific stack buffer overflow attacks.  Use a standard buffer overflow detector, plus some dynamic data+control flow dependency tracking.  All of that gets inserted by the compiler, rather than via DBT, which is a nice touch.

        
Their patches replace unsafe operations like strcpy() with safer ones like strncpy(), but that requires knowledge of the size of the destination buffer.  They claim to be able to find that using the array allocation table, but that'll just tell you what size it was in the captured trace, rather than what size it is dynamically, so isn't really enough to safely generate a patch.

        
Overheads are pretty high; factor rather than percentage for CPU-bound tasks (which is what's interesting).  The evaluation is pretty poor, because most of their tests are IO-bound, so artificially flatter them.  They do seem to catch a reasonable number of bugs, though.

        
Not convinced by this one.  No cites.

      },
author = {Smirnov, A. and Chiueh, T.},
booktitle = {Information Assurance and Security, 2007. IAS 2007. Third International Symposium on},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chiueh - Unknown - Automatic Patch Generation for Buffer Overflow Attacks.pdf:pdf},
keywords = {PASAN},
mendeley-tags = {PASAN},
pages = {165--170},
title = {{Automatic patch generation for buffer overflow attacks}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Automatic+Patch+Generation+for+Buffer+Overflow+Attacks\#0},
year = {2007}
}
@inproceedings{Locasto2008a,
abstract = {Most attacks on computing systems occur rapidly enough to frus- trate manual defense or repair. It appears, therefore, that defense systems must include some degree of autonomy. Recent advances have led to an emerging interest in selfâhealing software as a so- lution to this problem. It is not clear, however, if the effort to create selfâhealing mechanisms is actually worth the cost in terms of development effort, deployment complexity, or runtime supervi- sion and monitoring. Furthermore, no general purpose selfâhealing mechanisms have been shown to be achievable for general systems; it is hard to know beforehand exactly what an application should do in response to an arbitrary vulnerability. A number of very hard problems remain for researchers to explore before selfâhealing can be reliably applied to real computing systems. This paper provides a critique of the current state of the art and offers the position that selfâhealing as a concept should be relegated to the status of au- tonomic computing: a goal worth aiming for as a way to push the boundary of the possible rather than an achievable end in itself. Along the way, we identify a number of important but unsolved research problems in this space.},
annote = {Position paper.  Basically pointing out that the general case of self-healing is impossible, which is unsurprising.  Nothing really exciting here.

      },
author = {Locasto, ME},
booktitle = {Proceedings of the 2007 Workshop on New Security Paradigms},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Locasto - 2008 - Self-healing science, engineering, and fiction.pdf:pdf},
keywords = {automatic re- pair,autonomic computing,self-* systems,self-healing},
publisher = {ACM},
title = {{Self-healing: science, engineering, and fiction}},
url = {http://portal.acm.org/citation.cfm?id=1600183},
year = {2008}
}
@inproceedings{Lin2007,
abstract = {Software patch generation is a critical phase in the life-cycle of a software vulnerability. The longer it takes to generate a patch, the higher the risk a vulnerable system needs to take to avoid from being compromised. However, in practice, it is a rather lengthy process to generate and release software patches. For example, the analysis on 10 recent Microsoft patches (MS06-045 to MS06-054) shows that, for an iden- tified vulnerability, it took 75 days on average to generate and release the patch. In this paper, we present the design, implementation, and evaluation of AutoPaG, a system that aims at reducing the time needed for software patch generation. In our current work, we mainly focus on a common and serious type of software vulnerability: the out-of-bound vulnerability which includes buffer overflows and general boundary condition er- rors. Given a working out-of-bound exploit which may be previously unknown, AutoPaG is able to catch on the fly the out-of-bound violation, and then, based on data flow analysis, automatically analyzes the program source code and identifies the root cause â vulnerable source-level pro- gram statements. Furthermore, within seconds, AutoPaG generates a fine-grained source code patch to temporarily fix it without any human intervention. We have built a proof-of-concept system in Linux and the preliminary results are promising: AutoPaG is able to successfully identify the root cause and generate a source code patch within seconds for every vulnerability test in the Wilanderâs buffer over- flow benchmark test-suite. In addition, the evaluation with a number of real-world out-of-bound exploits also demon- strates its effectiveness and practicality in automatically iden- tifying (vulnerable) source code root causes and generating corresponding patches.},
address = {Singapore},
annote = {Want to automatically generate source-level patches for array out of bounds errors, given an example exploit.  Use a standard memory error detector to find the actual instruction which overflows the buffer, and then uses static analysis (mostly dataflow analysis) to find useful information.  Finally, generate a patch which suppresses out of bounds writes and wraps out of bounds reads to be within bounds.  Use a limited kind of dynamic checking based on CCured to fix problems which they can't handle by static analysis.

        
Results seem to be pretty good: fix all of the Wilander bugs, plus five wild ones.  For some of the Wilander bugs, they introduce extra crashes, but that's pretty much unavoidable due to the nature of the tests.  None of the wild bug patches introduced new bugs.  Performance hit is low - a few percent with one patch applied.

        
Future work says they want to look at race patching, but they don't give any indication of how they want to do it.

        
I like this work, although the paper isn't as clear as it could be.

      },
author = {Lin, Z and Jiang, X and Xu, D and Mao, B and Xie, L},
booktitle = {ASIACCS07},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2007 - AutoPaG towards automated software patch generation with source code root \ldots.pdf:pdf},
keywords = {Automated Patch Generation,Data Flow Analysis,Out-of-boundVulnerability,Software Security},
publisher = {ACM},
title = {{AutoPaG: towards automated software patch generation with source code root cause identification and repair}},
year = {2007}
}
@article{MICHAEL2006,
author = {MICHAEL, M and LEVY, NBHM},
journal = {ACM Transactions on Computer  \ldots},
title = {{Recovering Device Drivers}},
url = {http://d.wanfangdata.com.cn/NSTLQK\_NSTL\_QK13376826.aspx},
year = {2006}
}
@article{Prasad,
author = {Prasad, M},
journal = {usenix.org},
title = {{Disassembly Engine Implementation}},
url = {http://www.usenix.org/events/usenix03/tech/full\_papers/prasad/prasad\_html/node6.html}
}
@article{Rinard2007a,
author = {Rinard, MC},
file = {::},
journal = {Proceedings of the 22nd annual ACM SIGPLAN  \ldots},
title = {{Living in the comfort zone}},
url = {http://portal.acm.org/citation.cfm?id=1297027.1297072},
year = {2007}
}
@inproceedings{Sidiroglou2007,
abstract = {Testing vendor-issued patches remains one of the major hurdles to their speedy deployment. Studies have shown that administrators remain reluctant to quickly patch their systems, even when they have the capability to do so, partly because security patches in particular are often incomplete or altogether non-functional. We propose Band-aid Patching, a new approach for con- currently testing application patches. Using binary runtime injection techniques, we patch binaries such that when pro- gram execution reaches a program segment that has been affected by an issued patch, two (or more) program ex- ecution threads are created. These threads speculatively execute both parts of the code (patched and unpatched). Our system then retroactively selects one of the execution threads based on a variety of criteria, including obvious faultiness, prior history, and user input. We believe this ap- proach to offer significant advantages to accelerating de- ployment of hot fixes while providing some assurance to system administrators. In this paper, we describe our ini- tial thoughts on the system architecture, and provide some preliminary indications on the feasibility and performance impact of our scheme.},
annote = {Trying to reduce the risk of patches having unintended consequences.  Idea is it to fork the world when you get to the start of a patched section, and then when you get to the end of the section use some heuristic to decide which fork to take.  Skimmed but not properly read.

      },
author = {Sidiroglou, S and Ioannidis, S and Keromytis, A.D.},
booktitle = {Proceedings of the 3rd workshop on on Hot Topics in System Dependability},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sidiroglou, Ioannidis - 2007 - Band-aid patching.pdf:pdf},
pages = {6},
publisher = {USENIX Association},
title = {{Band-aid patching}},
url = {http://portal.acm.org/citation.cfm?id=1323140.1323146},
year = {2007}
}
@article{Cowan1998,
author = {Cowan, C and Pu, C and Maier, D and Hintony, H},
file = {::},
journal = {Proceedings of the  \ldots},
title = {{StackGuard: Automatic adaptive detection and prevention of buffer-overflow attacks}},
url = {http://portal.acm.org/citation.cfm?id=1267554},
year = {1998}
}
@phdthesis{Bond,
annote = {tl;dr.

        
Looking at tricks for debugging production systems with suitably low overheads.  Main tricks are:

        
-- Some light-weight anomaly detection on call stacks
-- Statistical correlation between data and program locations
-- Origin tracking, using spare bits in e.g. null pointers.

        
Also provide some limited techniques for tolerating memory leaks in GC'd languages.  First, add a pager (duh).  More interestingly, reclaim stuff which hasn't been touched in a while, and defer the out-of-memory exception until the program touches something which was reclaimed.

        
Cute, but not relevant to what I'm doing right now, and too long to read for interest (at least past skimming the introduction).},
author = {Bond, MD and Pingali, K and Stone, P and Witchel, E and Horn, AH},
booktitle = {Citeseer},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bond et al. - Unknown - Diagnosing and tolerating bugs in deployed systems.pdf:pdf},
school = {University of Texas at Austin},
title = {{Diagnosing and tolerating bugs in deployed systems}},
type = {Doctor of Philosophy},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.140.2862\&rep=rep1\&type=pdf},
year = {2008}
}
@article{Lowell,
author = {Lowell, DE and Chen, SCPM},
journal = {usenix.org},
title = {{Exploring failure transparency and the limits of generic recovery}},
url = {http://scholar.google.com/scholar?cites=17164444154164928774\&hl=en\&as\_sdt=2000\#4}
}
@article{Aroraa2005,
abstract = {Distributed systems have to deal with the following scenarios in practice: bugs in components; incorrect specifications of components and, therefore, incorrect use of components; unanticipated faults due to complex interactions or to not containing the effects of faults in lower-level components; and evolution of components. Extant fault tolerance models deal with such scenarios in only a limited manner. particular, we point out that state corruption In is inevitable in practice and that therefore one must accept it and seek to correct it. The well-known concepts of detectors and correctors can be used to find and repair However, these concepts have state misbehaving system corruption. employed to immediately detect and correct errors caused by Immediate detection and correction is often too expensive to perform and hence we consider the implications of running detectors and correctors only intermittently. More specifically, we address issues that must be dealt with when state corruption may persist within a system for a period of time. We show how to both detect and correct state corruption caused by infrequently occurring âtransientâ errors despite the ability for it to actively spread to other parts of the system. We also show how to eventually detect all state corruption, even in cases where continually recurring errors are constantly introducing new state corruption. Finally, we discuss the minimum set of capabilities needed from a trusted base of software in order to guarantee the correctness of our algorithms.},
annote = {Assume that state corruption is inevitable in any large system, and seek to constrain or model the potential damage.  Further assume that it's too expensive to run state corruption detectors at all times.  System model has no shared state; all communication through controllable RMI.  Assume that correctness is per-component i.e. components can be correct independently of other components.

        
I've no idea what this is trying to say.

      },
author = {Arora, A and Theimer, M},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aroraa, Theimerb - 2005 - On modeling and tolerating incorrect software.pdf:pdf},
journal = {Journal of High Speed Networks},
number = {2},
pages = {109--134},
publisher = {IOS Press},
title = {{On modeling and tolerating incorrect software}},
url = {http://iospress.metapress.com/index/AA8V9WTTUNU4MNFF.pdf},
volume = {14},
year = {2005}
}
@article{Sidiroglou2005b,
abstract = {The ability of worms to spread at rates that effectively preclude human-directed reaction has elevated them to a first-class security threat to distributed systems. We propose an architecture for automatically repairing software flaws that are exploited by zero-day worms. Our approach relies on source code transformations to quickly apply automatically-created (and tested) localized patches to vulnerable segments of the targeted application. To deter- mine these susceptible portions, we use a sandboxed instance of the application as a âclean roomâ laboratory that runs in parallel with the production system and exploit the fact that a worm must reveal its infection vector to achieve its goal (i.e., further infection). We believe our approach to be the first end-point solution to the problemofmalicious self-replicating code. The primary benefits of our approach are ability to respond to attacks without human intervention, and î¨ î© î¨î¡î© its low impact on application performance, î¨î¢î© its its capacity to deal with zero-day worms (for which no known patches exist). Furthermore, our approach does not depend on a centralized update repository, which can be the target of a concerted attack similar to the Blaster worm. Finally, our approach can also be used to protect against lower intensity attacks, such as intrusion (âhack-inâ) attempts. To experimentally evaluate the efficacy of our approach, we use our prototype implementation to test a number of applications with known vulnerabilities. Our preliminary results indicate a success rate of 82\%, and a maximum repair time of 8.5 seconds.},
annote = {Trying to automatically fix stack-based buffer overflow attacks.  Fixes are pretty stupid, for the most part: just move offending buffer to the heap, allocate guard pages, and then trap SIGSEGV to longjmp to a pre-identified rescue point.  Program is compiled with ProPolice to detect errors, then transformed using TXL to include the error mitigator.

        

        
Not terribly inspiring, overall.

      },
author = {Sidiroglou, S and Keromytis, AD},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sidiroglou, Keromytis - 2005 - Countering network worms through automatic patch generation.pdf:pdf},
journal = {IEEE Security $\backslash$\& Privacy},
pages = {41--50},
publisher = {IEEE Computer Society},
title = {{Countering network worms through automatic patch generation}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/MSP.2005.144},
year = {2005}
}
@article{Sultan,
author = {Sultan, F and Bohra, A and Neamtiu, I and Iftode, L},
file = {::},
journal = {Citeseer},
title = {{Backdoor: Nonintrusive Remote Healing with Remote Memory  \ldots}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.2368\&rep=rep1\&type=pdf}
}
@inproceedings{Susskraut2006,
abstract = {Bad error handling is the cause of many service outages. We address this problem by a novel approach to detect and patch bad error handling automatically. Our approach uses error injection to detect bad error handling and static anal- ysis of binary code to determine which type of patch can be instantiated. We describe several measurements regarding the effectiveness of our approach to detect and patch bad error handling in several open source programs.},
annote = {Use error injection to provoke bugs in error handling code, then try to map error handling paths with bugs to error handling paths without bugs using a small set of hand-crafted patch patterns.  Also consider some kinds of preallocation to move bugs around.  Do some minimal static analysis to eliminate a few false positives (i.e. bits when the function could crash under some circumstances, but those circumstances never happen).  This is very incomplete.

        
They find some crashes using fault injection, but it's not clear how many of these are actual honest-to-goodness bugs which might hit in the real world (most look like malloc() failures, which will never happen under Linux).  They fix about 80\% of them.  Overhead seems to be small but quite random: from -5\% to 10\%.

        
It's not a bad attempt at solving the problem; it's just a pity the problem is so stupid.

      },
author = {S\"{u}\ss kraut, M. and Fetzer, C},
booktitle = {Proceedings of the Sixth European Dependable Computing Conference},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S\"{u}\ss kraut, Fetzer - 2006 - Automatically finding and patching bad error handling.pdf:pdf},
pages = {22},
publisher = {IEEE Computer Society},
title = {{Automatically finding and patching bad error handling}},
url = {http://wwwse.inf.tu-dresden.de/papers/preprint-suesskraut2006edcc1.pdf},
year = {2006}
}
@inproceedings{Fetzer2002,
abstract = {As our reliance on computers increases, so does the need for robust software. Previous studies have shown that many C libraries exhibit robustness problems due to exceptional inputs. This paper describes the HEALERS system that uses an automated approach to increasing the robustness of C li- braries without source code access. The system extracts the C type information for a shared library using header files and manual pages. Then it generates for each global func- tion a fault-injector to determine a ârobustâ argument type for each argument. Based on this information and option- ally, some manual editing, the system generates a robust- ness wrapper that performs careful argument checking be- fore invoking C library functions. A robustness evaluation using Ballista tests has shown that our wrapper can prevent crash, hang, and abort failures. Moreover, the wrapper gen- eration process is highly automated and can easily adapt to new library releases.},
annote = {HEALERS again -- trying to make library functions more robust by auto-generating robustness wrappers.  Still not sure quite what the point of this is.},
author = {Fetzer, C and Xiao, Z},
booktitle = {International Conference on Dependable Systems and Networks},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fetzer, Xiao - 2002 - An automated approach to increasing the robustness of C libraries.pdf:pdf},
keywords = {HEALERS},
mendeley-tags = {HEALERS},
pages = {155--164},
title = {{An automated approach to increasing the robustness of C libraries}},
url = {http://doi.ieeecomputersociety.org/10.1109/DSN.2002.1028896},
year = {2002}
}
@article{Brumley2007b,
author = {Brumley, D and Newsome, J and Song, D},
file = {::},
journal = {Malware Detection. Springer},
title = {{Sting: An end-to-end self-healing system for definding against internet worms}},
url = {http://www.springerlink.com/index/pn3874h50g587170.pdf},
year = {2007}
}
@article{Lee2007,
abstract = {To reason about whole-program behavior, dynamic optimiz- ers and analysis tools collect a dynamic call graph using sampling. Previ- ous approaches have not achieved high accuracy with low runtime over- head, and this problem is likely to become more challenging as object- oriented programmers increasingly compose complex programs. This paper demonstrates how to use static and dynamic control-flow graph (CFG) constraints to improve the accuracy of the dynamic call graph (DCG). We introduce the frequency dominator (FDOM) which is a novel CFG relation that extends the dominator relation to expose rel- ative execution frequencies of basic blocks. We combine conservation of flow and dynamic CFG basic block profiles to further improve the accu- racy of the DCG. Together these approaches add minimal overhead (1\%) and achieve 85\% accuracy compared to a perfect call graph for SPEC JVM98 and DaCapo benchmarks. Compared to sampling alone, accuracy improves by 12 to 36\%. These results demonstrate that static and dy- namic control-flow information offer accurate information for efficiently improving the DCG.},
annote = {Compiler optimisations.

      },
author = {Lee, B and Resnick, K and Bond, MD and McKinley, KS},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2007 - Correcting the dynamic call graph using control-flow constraints.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {80},
publisher = {Springer},
title = {{Correcting the dynamic call graph using control-flow constraints}},
url = {http://www.springerlink.com/index/5004q722u58315g3.pdf},
volume = {4420},
year = {2007}
}
@article{Herzog,
annote = {Not quite sure what this is },
author = {Herzog, D},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Herzog - Unknown - HEALERS-A Toolkit.pdf:pdf},
journal = {wwwse.inf.tu-dresden.de},
title = {{HEALERS-A Toolkit}},
url = {http://wwwse.inf.tu-dresden.de/wiki/images/b/b4/SEMINAR-DanielHerzog-Handin.pdf}
}
@article{Goldstein2007,
author = {Goldstein, M and Shehory, O and Weinsberg, Y},
journal = {\ldots  workshop on Software  \ldots},
title = {{Can self-healing software cope with loitering?}},
url = {http://portal.acm.org/citation.cfm?id=1295074.1295076},
year = {2007}
}
@article{Meulen,
author = {van der Meulen, M and Riddle, S and Strigini, L and Jefferson, N},
journal = {\ldots Spain, X. Franch and D. Port \ldots},
title = {{Protective wrapping of off-the-shelf components}},
url = {http://www.springerlink.com/index/5R27A8341AH1QVUF.pdf}
}
@inproceedings{Sultan2003,
abstract = {In this paper, we propose a remote healing approach for computer systems based on backdoors, a system architecture that supports monitoring and repair actions on a remote operating system or application memory image without using the processors of the target machine. A backdoor can be implemented using the remote memory communication technology provided by communication standards like Virtual Interface Architecture or InfiniBand, specifi- cally its support for remote DMA read and write operations. We discuss the potential and challenges of backdoor-based remote healing and describe a preliminary prototype we developed as proof of concept.},
annote = {Suggest that system healing should be done remotely i.e. from another machine using e.g. RDMA.  Very, very, stupid.  And yet they still manage to get citations.  Clearly, the world is broken.

      },
author = {Sultan, F and Bohra, A and Neamtiu, I and Iftode, L},
booktitle = {Proc. 1st Workshop on Algorithms and Architectures for Self-Managing Systems},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sultan et al. - 2003 - Nonintrusive remote healing using backdoors.pdf:pdf},
publisher = {Citeseer},
title = {{Nonintrusive remote healing using backdoors}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.4429\&amp;rep=rep1\&amp;type=pdf},
year = {2003}
}
@inproceedings{Locasto2006a,
abstract = {Software monocultures are usually considered danger- ous because their size and uniformity represent the poten- tial for costly and widespread damage. The emerging con- cept of collaborative security provides the opportunity to re-examine the utility of software monoculture by exploit- ing the homogeneity and scale that typically define large software monocultures. Monoculture can be leveraged to improve an applicationâs overall security and reliability. We introduce and explore the concept of Application Com- munities: collections of large numbers of independent in- stances of the same application. Members of an applica- tion community share the burden of monitoring for flaws and attacks, and notify the rest of the community when such are detected. Appropriate mitigation mechanisms are then deployed against the newly discovered fault. We explore the concept of an application community and determine its fea- sibility through analytical modeling and a prototype imple- mentation focusing on software faults and vulnerabilities. Specifically, we identify a set of parameters that define application communities and explore the tradeoffs between the minimal size of an application community, the marginal overhead imposed on each member, and the speed with which new faults are detected and isolated. We demon- strate the feasibility of the scheme using Selective Trans- actional EMulation (STEM) as both the monitoring and remediation mechanism for low-level software faults, and provide some preliminary experimental results using the Apache web server as the protected application. Our ex- periments show that ACs are practical and feasible for cur- rent applications: an AC of 15,000 members can collabo- ratively monitor Apache for new faults and immunize all members against them with only a 6\% performance degra- dation for each member.},
annote = {Trying to do a collective self-immunisation thing, so that once some fraction of hosts in the community have been hit with some attack the others become immune.  Sounds like Vigilante.

        
Kind of tedious.

      },
author = {Locasto, ME and Sidiroglou, S and Keromytis, AD},
booktitle = {Internet Society (ISOC) Symposium on Network and Distributed Systems Security},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Locasto, Sidiroglou, Keromytis - 2006 - Software self-healing using collaborative application communities.pdf:pdf},
pages = {95--106},
publisher = {Citeseer},
title = {{Software self-healing using collaborative application communities}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.2699\&amp;rep=rep1\&amp;type=pdf},
year = {2006}
}
@inproceedings{Fetzera,
abstract = {Improving the dependability of computer systems is in- creasingly important as more and more of our lives depend on the availability of such systems. Wrapping dynamic link libraries is an effective approach for improving the reli- ability and security of computer software without source code access. In this paper we describe a flexible frame- work to generate a rich set of software wrappers for shared libraries. We describe the architecture of the wrapper gen- erator, the problems of how to generate wrappers efficiently, and our solutions to these problems. Based on a set of prop- erties declared for a function, the generator can create a va- riety of wrappers to suit the diverse requirements of appli- cation programs. Performance measurements indicate that the overhead of the generated wrappers is small.},
annote = {HEALERS again.  Assume that you have some dynamically linked binary which contains bugs, and you want to replace the dynamic libraries so that it doesn't  hit any bugs any more.  Approach involves parsing up header files and man pages (???) to extract prototypes, and then generating an LD\_PRELOAD library which overloads appropriate weak symbols.  Main healing strategy seems to be replay, which is a bit sucky.  Also have a resource manager thingy which can tell processes to release e.g. file descriptors, which is both cute and useless.

        
Meh.  Very, very meh.

      },
author = {Fetzer, C and Xiao, Z},
booktitle = {Proceedings of the 13th International Symposium on Software Reliability Engineering (ISSREâ02)},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fetzer, Xiao - Unknown - A flexible generator architecture for improving software dependability.pdf:pdf},
keywords = {HEALERS,middleware,reliability,security,software fault- tolerance,wrapper generator,wrappers},
mendeley-tags = {HEALERS},
pages = {102--113},
title = {{A flexible generator architecture for improving software dependability}},
url = {http://doi.ieeecomputersociety.org/10.1109/ISSRE.2002.1173221},
year = {2002}
}
@inbook{Strigini,
abstract = {This chapter surveys techniques for tolerating the effects of design defects in computer systems, paying special attention to software. Design faults are a major cause of failure in modern computer systems, and their relative importance is growing as techniques for tolerating physical faults gain wider acceptance. Although design faults could in principle be eliminated, in practice they are inevitable in many categories of systems, and designers need to apply fault tolerance for mitigating their effects. Limited degrees of fault tolerance in software â âdefensive programmingâ â are common, but systematic application of fault tolerance for design faults is still rare and mostly limited to highly critical systems. However, the increasing dependence of system designers on off-the-shelf components often makes fault tolerance a necessary, feasible and probably cost-effective solution for achieving modest dependability improvements at affordable cost. This chapter introduces techniques and principles, outlines similarities and differences with fault tolerance against physical faults, provides a structured description of the space of design solutions, and discusses some design issues and trade-offs.},
author = {Strigini, L},
booktitle = {Dependable Computing Systems: Paradigms, Performance Issues, and Applications},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strigini - Unknown - Fault tolerance against design faults.pdf:pdf},
pages = {213--241},
publisher = {Wiley},
title = {{Fault tolerance against design faults}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.4340\&amp;rep=rep1\&amp;type=pdf}
}
@article{Locastoa,
author = {Locasto, ME and Stavrou, A and Cretu, GF and Keromytis, AD},
journal = {usenix.org},
title = {{From STEM to SEAD: Speculative execution for automated defense}},
url = {http://scholar.google.com/scholar?hl=en\&q=speculative+execution+for+automatic+defence\&btnG=Search\&as\_sdt=2000\#0}
}
@article{Necula2005,
author = {Necula, GC and Condit, J and Harren, M and McPeak, S},
file = {::},
journal = {ACM Transactions on  \ldots},
title = {{CCured: Type-safe retrofitting of legacy software}},
url = {http://portal.acm.org/citation.cfm?id=1065892},
year = {2005}
}
@inproceedings{Li2002,
author = {Li, L and Vaidyanathan, K and Trivedi, KS},
booktitle = {Proceedings of the International Symposium on Empirical Software Engineering, ISESE 2002},
title = {{An approach for estimation of software aging in a web server}},
url = {http://shannon.ee.duke.edu/Rejuv/isese02.ps},
year = {2002}
}
@inproceedings{Vaidyanathan2001,
author = {Vaidyanathan, K and Harper, RE and Hunter, SW and Trivedi, K.S.},
booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
file = {::},
pages = {62--71},
publisher = {ACM New York, NY, USA},
title = {{Analysis and implementation of software rejuvenation in cluster systems}},
url = {http://portal.acm.org/citation.cfm?id=378420.378434\&amp;type=series},
year = {2001}
}
@inproceedings{Garg1995,
author = {Garg, S and Puliafito, A and Trivedi, KS},
booktitle = {In Proceedings of the 6th International Symposium on Software Reliability Engineering},
file = {::},
title = {{Analysis of software rejuvenation using Markov regenerative stochastic Petri net}},
url = {http://eprints.kfupm.edu.sa/25715/},
year = {1995}
}
@article{Castelli2001,
author = {Castelli, V and Harper, RE and Heidelberger, P and Hunter, S.W. and Trivedi, K.S. and Vaidyanathan, K. and Zeggert, W.P.},
file = {::},
journal = {IBM Journal of Research and Development},
number = {2},
pages = {311--332},
publisher = {Citeseer},
title = {{Proactive management of software aging}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.7273\&amp;rep=rep1\&amp;type=pdf},
volume = {45},
year = {2001}
}
@inproceedings{Trivedi2000,
author = {Trivedi, KS and Vaidyanathan, K and Goseva-Popstojanova, K.},
booktitle = {ANNUAL SIMULATION SYMPOSIUM},
file = {::},
pages = {270--282},
publisher = {IEEE Computer Society; 1999},
title = {{Modeling and analysis of software aging and rejuvenation}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/SIMSYM.2000.844925},
volume = {33},
year = {2000}
}
@inproceedings{Garg1998a,
author = {Garg, S and van Moorsel, A. and Vaidyanathan, K. and Trivedi, K.S.},
booktitle = {Proceedings of the 9th International Symposium on Software Reliability Engineering},
pages = {283--292},
publisher = {Paderborn, Germany},
title = {{A methodology for detection and estimation of software aging}},
url = {http://doi.ieeecomputersociety.org/10.1109/ISSRE.1998.730892},
year = {1998}
}
@inproceedings{Vaidyanathan1999,
author = {Vaidyanathan, K and Trivedi, KS},
booktitle = {Proceedings of the 10th International Symposium on Software Reliability Engineering},
pages = {84},
publisher = {IEEE Computer Society Washington, DC, USA},
title = {{A measurement-based model for estimation of resource exhaustion in operational software systems}},
url = {http://doi.ieeecomputersociety.org/10.1109/ISSRE.1999.809313},
year = {1999}
}
@article{Garg1998,
author = {Garg, S and Puliafito, A and Telek, M and Trivedi, K},
file = {::},
journal = {IEEE Transactions on Computers},
number = {1},
pages = {96--107},
publisher = {Citeseer},
title = {{Analysis of preventive maintenance in transactions based software systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.467\&amp;rep=rep1\&amp;type=pdf},
volume = {47},
year = {1998}
}
@inproceedings{Candea,
author = {Candea, G and Kawamoto, S and Fujiki, Y and Friedman, G and Fox, A},
booktitle = {OSDI04},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Candea et al. - Unknown - Microrebootâa technique for cheap recovery.pdf:pdf},
publisher = {USENIX Association},
title = {{Microrebootâa technique for cheap recovery}},
url = {http://www.usenix.org/events/osdi04/tech/full\_papers/candea/candea.pdf},
year = {2004}
}
@inproceedings{Candea2001,
author = {Candea, G and Fox, A},
booktitle = {Proceedings of the 8th Workshop on Hot Topics in Operating Systems (HotOS-VIII)},
file = {::},
publisher = {Citeseer},
title = {{Recursive restartability: Turning the reboot sledgehammer into a scalpel}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.3331\&amp;rep=rep1\&amp;type=pdf},
year = {2001}
}
@misc{Patterson2002,
author = {Patterson, D and Brown, A and Broadwell, P and Candea, G and Chen, M. and Cutler, J. and Enriquez, P. and Fox, A. and Kiciman, E. and Merzbacher, M. and Others},
institution = {Citeseer},
title = {{Recovery-oriented computing (ROC): Motivation, definition, techniques, and case studies}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.582\&amp;rep=rep1\&amp;type=pdf},
year = {2002}
}
@inproceedings{Candea2002,
author = {Candea, G and Cutler, J and Fox, A and Doshi, R. and Garg, P. and Gowda, R.},
booktitle = {Proceedings of the 2002 International Conference on Dependable Systems and Networks},
file = {::},
pages = {605--614},
publisher = {IEEE Computer Society Washington, DC, USA},
title = {{Reducing recovery time in a small recursively restartable system}},
url = {http://doi.ieeecomputersociety.org/10.1109/DSN.2002.1029006},
year = {2002}
}
@inproceedings{Rinard2005a,
address = {Singapore},
author = {Rinard, M},
booktitle = {Singapore-MIT Alliance 2005 Symposium},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rinard - 2005 - Failure-Oblivious Computing and Boundless Memory Blocks.pdf:pdf},
publisher = {MIT},
title = {{Failure-Oblivious Computing and Boundless Memory Blocks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.9762\&amp;rep=rep1\&amp;type=pdf},
year = {2005}
}
@inproceedings{Elkarablieh2007,
author = {Elkarablieh, B and Garcia, I and Suen, YL and Khurshid, S.},
booktitle = {Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elkarablieh et al. - 2007 - Assertion-based repair of complex data structures.pdf:pdf},
pages = {64--73},
publisher = {ACM New York, NY, USA},
title = {{Assertion-based repair of complex data structures}},
url = {http://portal.acm.org/citation.cfm?id=1321643\&amp;dl=GUIDE,},
year = {2007}
}
@article{Viswanadham1990,
author = {Viswanadham, N and Narahari, Y and Johnson, T.L.},
file = {::},
journal = {IEEE Transactions on Robotics and Automation},
number = {6},
pages = {713--723},
publisher = {IEEE},
title = {{Deadlock prevention and deadlock avoidance in flexible manufacturing systems using Petri net models}},
url = {http://eprints.iisc.ernet.in/3840/1/deadlock.pdf},
volume = {6},
year = {1990}
}
@article{Dijkstra2004,
author = {Dijkstra, EW},
doi = {10.1016/S0969-4765(04)00066-9},
file = {:home/sos22/papers/random/ewd623.pdf:pdf},
issn = {09694765},
journal = {Selected Writings on Computing: A Personal Perspective},
month = mar,
number = {3},
pages = {308--312},
title = {{The mathematics behind the Banker's algorithm}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:The+mathematics+behind+the+banker's+algorithm\#0},
volume = {12},
year = {1977}
}
@misc{CWE758,
annote = {undefined},
author = {Security, U.S. Department Of Homeland},
booktitle = {Common Weakness Enumeration},
file = {::},
title = {{CWE-758: Reliance on Undefined, Unspecified, or Implementation-Defined Behavior}},
url = {http://cwe.mitre.org/data/definitions/758.html}
}
@inproceedings{Jula2008b,
abstract = {We introduce the concept of deadlock immunityâa programâs ability to avoid all deadlocks that match patterns of deadlocks expe- rienced in the past. We present here an algorithm for enabling large software systems to automatically acquire such immunity without any programmer assistance. We prove that the algorithm is sound and com- plete with respect to the immunity property. We implemented the algo- rithm as a tool for Java programs, and measurements show it introduces only modest performance overhead in real, large applications like JBoss. Deadlock immunity is as useful as complete freedom from deadlocks in many practical cases, so we see the present algorithm as a pragmatic step toward ridding complex concurrent programs of their deadlocks.},
annote = {Essentially another variant of the Dimmunix paper.  Seems to be a slightly earlier variant.
Marginally more formal, although I'm not convinced the extra formality actually buys you anything.},
author = {Jula, Horatiu and Candea, George},
booktitle = {Proc. 8th Workshop on Runtime Verification},
file = {:home/sos22/papers/random/dimmunix-algo.pdf:pdf},
publisher = {Springer},
title = {{A scalable, sound, eventuallycomplete algorithm for deadlock immunity}},
url = {http://www.springerlink.com/index/l8476552682k5724.pdf},
year = {2008}
}
@inproceedings{Saib1977a,
abstract = {Two preprocessors, one for FORTRAN and one for PASCAL, have been implemented to allow "executable assertions" to be added to the source code. These assertions make it possible to carry out certain static and dynamic checks on the semantics of a program. They can detect errors in input data and prevent error propagation. It is possible to in- clude remedial instructions to compensate for detected errors and provide fault tolerance. The assertions can he applied to all the data types available in the languages.  Executable assertions can also be used in a proof of correctness. This paper describes the syntax of the assertions, which contain first-order logical expressions. Examples are given of their use in a simple but practical example, the calculation of the time that a ballistic projectile travels. },
annote = {Appears to be the origin of the assert() statement.

      },
author = {Saib, SH},
booktitle = {11th Asilomar Conference on Circuits, Systems and Computers},
file = {:home/sos22/papers/random/exec\_assertions.pdf:pdf},
keywords = {assert},
mendeley-tags = {assert},
number = {C},
pages = {277--281},
title = {{Executable assertions-an aid to reliable software}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Executable+assertions:+AN+AID+TO+RELIABLE+SOFTWARE\#0},
volume = {75},
year = {1977}
}
@article{Saib1977,
author = {Saib, SH},
journal = {Circuits, Systems and Computers, 1977. Conference  \ldots},
title = {{Executable assertions-an aid to reliable software}},
url = {http://scholar.google.com/scholar?q=executable+assertions\#3},
year = {1977}
}
@article{Pattabiraman2005,
abstract = {The goal of this study is to provide low-latency detection and prevent error propagation due to value errors. This paper introduces metrics to guide the strategic placement of detectors and evaluates (using fault injection) the coverage provided by ideal detectors embedded at program locations selected using the computed metrics. The computation is represented in the form of a Dynamic Dependence Graph (DDG), a directed-acyclic graph that captures the dynamic dependencies among the values produced during the course of program execution. The DDG is employed to model error propagation in the program and to derive metrics (e.g., value fanout or lifetime) for detector placement. The coverage of the detectors placed is evaluated using fault injections in real programs, including two large SPEC95 integer benchmarks (gcc and perl). Results show that a small number of detectors, strategically placed, can achieve a high degree of detection coverage.},
author = {Pattabiraman, Karthik and Kalbarczyk, Zbigniew and Iyer, Ravishankar K.},
journal = {PRDC},
title = {{Application-Based Metrics for Strategic Placement of Detectors}},
url = {http://portal.acm.org/citation.cfm?id=1128016.1128371},
year = {2005}
}
@article{Hiller2002,
author = {Hiller, M and Jhumka, A and Suri, N},
file = {::},
journal = {Proceedings of the  \ldots},
title = {{On the placement of software mechanisms for detection of data errors}},
url = {http://doi.ieeecomputersociety.org/10.1109/DSN.2002.1028894},
year = {2002}
}
@article{Akers1978,
author = {Akers, SB},
file = {::},
journal = {IEEE Transactions on computers},
title = {{Binary decision diagrams}},
url = {http://www.ee.pdx.edu/\~{}shahd/BDD\_Akers.pdf},
year = {1978}
}
@article{LeBlanc1987,
author = {LeBlanc, TJ and Mellor-Crummey, JM},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeBlanc, Mellor-Crummey - 1987 - Debugging parallel programs with instant replay.pdf:pdf},
journal = {IEEE Transactions on Computers},
number = {4},
pages = {471--482},
title = {{Debugging parallel programs with instant replay}},
url = {http://pages.cs.wisc.edu/\~{}bart/739/papers/instantreplay.pdf},
volume = {36},
year = {1987}
}
@article{Yi2010,
abstract = {Reasoning about the correctness of multithreaded programs is complicated by the potential for unexpected interference between threads. Previous work on controlling thread interference focused on verifying race freedom and/or atomicity. Unfortunately, race freedom is insufficient to prevent unintended thread interference. The notion of atomic blocks provides more semantic guarantees, but offers limited benefits for non-atomic code and it requires bi-modal sequential/multithreaded reasoning (depending on whether code is inside or outside an atomic block). This paper proposes an alternative strategy that uses yield annotations to control thread interference, and we present an effect system for verifying the correctness of these yield annotations. The effect system guarantees that for any preemptively-scheduled execution of a well-formed program, there is a corresponding cooperative execution with equivalent behavior in which context switches happen only at yield annotations. This effect system enables cooperative reasoning: the programmer can adopt the simplifying assumption of cooperative scheduling, even though the program still executes with preemptive scheduling and/or true concurrency on multicore processors. Unlike bimodal sequential/multithreaded reasoning, cooperative reasoning can be applied to all program code.},
author = {Yi, Jaeheon and Flanagan, Cormac},
journal = {Types In Languages Design And Implementation},
keywords = {atomicity,effect system,race conditions,yield},
title = {{Effects for cooperable and serializable threads}},
url = {http://portal.acm.org/citation.cfm?id=1708019},
year = {2010}
}
@book{Demaine2007,
abstract = {The edit distance between two ordered rooted trees with vertex labels is the minimum cost of transforming one tree into the other by a sequence
  of elementary operations consisting of deleting and relabeling existing nodes, as well as inserting new nodes. In this paper,
  we present a worst-case O(n
  3)-time algorithm for this problem, improving the previous best O(n
  3logn)-time algorithm\&nbsp;[7]. Our result requires a novel adaptive strategy for deciding how a dynamic program divides into subproblems,
  together with a deeper understanding of the previous algorithms for the problem. We prove the optimality of our algorithm
  among the family of decomposition strategy algorithmsâwhich also includes the previous fastest algorithmsâby tightening the known lower bound of $\Omega$(n
  2log2
  n)\&nbsp;[4] to $\Omega$(n
  3), matching our algorithmâs running time. Furthermore, we obtain matching upper and lower bounds of 
  
  when the two trees have sizes m and n where mâ\&lt;ân.},
address = {Berlin, Heidelberg},
author = {Demaine, Erik and Mozes, Shay and Rossman, Benjamin and Weimann, Oren},
booktitle = {Automata, Languages and Programming},
doi = {10.1007/978-3-540-73420-8},
isbn = {978-3-540-73419-2},
issn = {0302-9743},
pages = {146--157},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Automata, Languages and Programming}},
url = {http://www.springerlink.com/content/fn7r1804170q2056},
volume = {4596},
year = {2007}
}
@inproceedings{Makatos2010,
abstract = {Flash-based solid state drives (SSDs) offer superior perfor- mance over hard disks for manyworkloads.Aprominent use of SSDs in modern storage systems is to use these devices as a cache in the I/O path. In this work, we examine how transparent, online I/O compression can be used to increase the capacity of SSD-based caches, thus increasing the cost- effectiveness of the system.We present FlaZ, an I/O system that operates at the block-level and is transparent to existing file-systems. To achieve transparent, online compression in the I/O path and maintain high performance, FlaZ provides support for variable-size blocks, mapping of logical to phys- ical blocks, block allocation, and cleanup. FlaZ mitigates compression and decompression overheads that can have a significant impact on performance by leveraging modern multicore CPUs. We implement FlaZ in the Linux kernel and evaluate it on a commodity server withmulticore CPUs, using TPC-H, PostMark, and SPECsfs.Our results showthat compressed caching trades off CPU cycles for I/O perfor- mance and enhancesSSD efficiency as a cache by up to 99\%, 25\%, and 11\% for each workload, respectively.},
address = {Paris},
annote = {(Presentation at EuroSys10)

        
SSDs as cache in front of spinning rust
Block level compression
Pretty damn obvious
Use a compacting GC

        
Qs: What hardware in eval?
Should perhaps only compress some bits?
GC only in idle time -> why such high overhead?

      },
author = {Makatos, Thanos and Flouris, Michail D},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p1.pdf:pdf},
keywords = {Eval- uation,I/O performance,Online block-level compression,Solid state disk caches},
pages = {1--14},
publisher = {ACM},
title = {{Using Transparent Compression to Improve SSD-based I / O Caches}},
year = {2010}
}
@inproceedings{Lupei2010,
abstract = {In this paper, we study parallelization of multiplayer games using software Transactional Memory (STM) support. We show that the STM provides not only ease of programming, but also better performance than that achievable with state- of-the-art lock-based programming, for this realistic high impact application. For this purpose, we use a game benchmark, SynQuake, that extracts the main data structures and the essential fea- tures of the popular game Quake. SynQuake can be driven with a synthetic workload generator that flexibly emulates client game actions and various hot-spot scenarios in the game world. We implement, evaluate and compare the STM version of SynQuake with a state-of-the-art lock-based paralleliza- tion of Quake, which we ported to SynQuake. While in STM-SynQuake support for maintaining the consistency of each complex game action is automatic, conservative lock- ing of surrounding objects within a bounding box, for the duration of the game action is inherently needed in lock- based SynQuake. This leads to higher scalability of STM- SynQuake versus lock-based SynQuake, due to a higher degree of false sharing in the latter. Task assignment to threads has a second-order effect on the scalability of STM- SynQuake, due to its impact on the applicationâs true sharing patterns.We showthat a dynamic locality-aware task assign- ment to threads provides the best trade-off between load bal- ancing and conflict reduction.},
address = {Paris},
annote = {(From presentation at EuroSys10)

        
Speaker pitched too low.
Net clear why this is interesting
Articial workload based loosely on quake (``synquake'')

        
Basically a re-run of atomic quake; pointed out during q\&a.  Poor bastard.},
author = {Lupei, Daniel and Simion, Bogdan and Pinto, Don and Misler, Matthew and Burcea, Mihai and Krick, William and Amza, Cristiana},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p41.pdf:pdf},
keywords = {load balancing,multiplayer games,scalability,software transactional memory,synchronization},
pages = {41--54},
publisher = {ACM},
title = {{Transactional Memory Support for Scalable and Transparent Parallelization of Multiplayer Games}},
year = {2010}
}
@inproceedings{Christie2010,
abstract = {AMDâs Advanced Synchronization Facility (ASF) is an x86 instruction set extension proposal intended to simplify and speed up the synchronization of concurrent programs. In this paper, we report our experiences using ASF for imple- menting transactional memory. We have extended a C/C++ compiler to support language-level transactions and generate code that takes advantage of ASF. We use a software fall- back mechanism for transactions that cannot be committed within ASF (e. g., because of hardware capacity limitations). Our evaluation uses a cycle-accurate x86 simulator that we have extended with ASF support. Building a complete ASF- based software stack allows us to evaluate the performance gains that a user-level program can obtain from ASF. Our measurements on a wide range of benchmarks indicate that the overheads traditionally associated with software transac- tional memories can be significantly reduced with the help of ASF.},
address = {Paris},
annote = {(Presentation at EuroSys10)

        
HTM = Hardware TM
Near-cycle accurate-sim (PTLSim) -- how accurate?
ASF -- transactional memory accesses need explicit annotation -- default non-transactional.
Using an explicit locked line buffer
Implement on Dresden TM conmpiler -- link-time optimisation
Need for explicit annotations complicates use of existing libraries.
Summary seems to be that TM is a bad idea.  Pity.},
author = {Christie, Dave and Chung, Jae-Woong and Disetelhorst, Stephan and Hohmuth, Michael and Pohlack, Martin and Fetzer, Christof and Nowack, Martin and Riegel, Trovald and Felber, Pascal and Marlier, Patrick and Riviere, Etienne},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p27.pdf:pdf},
keywords = {transactional memory},
pages = {27--40},
publisher = {ACM},
title = {{Evaluation of AMD â s Advanced Synchronization Facility Within a Complete Transactional Memory Stack}},
year = {2010}
}
@inproceedings{Balakrishnan2010,
abstract = {SSDs exhibit very different failure characteristics compared to hard drives. In particular, the Bit Error Rate (BER) of an SSD climbs as it receives more writes. As a result, RAID arrays composed from SSDs are subject to correlated fail- ures. By balancing writes evenly across the array, RAID schemes can wear out devices at similar times. When a de- vice in the array fails towards the end of its lifetime, the high BER of the remaining devices can result in data loss. We propose Diff-RAID, a parity-based redundancy solution that creates an age differential in an array of SSDs. Diff-RAID distributes parity blocks unevenly across the array, leverag- ing their higher update rate to age devices at different rates. To maintain this age differential when old devices are re- placed by new ones, Diff-RAID reshuffles the parity distri- bution on each drive replacement.We evaluate Diff-RAIDâs reliability by using real BER data from 12 flash chips on a simulator and show that it is more reliable than RAID-5, in some cases by multiple orders of magnitude. We also eval- uate Diff-RAIDâs performance using a software implemen- tation on a 5-device array of 80 GB Intel X25-M SSDs and show that it offers a trade-off between throughput and relia- bility.},
address = {Paris},
annote = {(Presentation at EuroSys10)

        
SSDRAID:
more erasure -> higher BER
10\^{}-4 after 10\^{}4 cycles, before ECC
In RAID4/5 parity updated more often than data -> if the parity is evenly distributed, the drives age at the same rate -> correlated failure.
Answer: unbalance parity so that drives age differently.  Need to rebalance after replacement.

        
Seems to help quite a bit.

        
Q\&A: future work includes rebalancing during normal operation (i.e. not in response to failure),},
author = {Balakrishnan, Mahesh and Kadav, Asim and Prabhakaran, Vijayan and Malkhi, Dahlia},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p15.pdf:pdf},
keywords = {flash,raid,ssd},
pages = {15--26},
publisher = {ACM},
title = {{Differential RAID : Rethinking RAID for SSD Reliability}},
year = {2010}
}
@inproceedings{Cucinotta2010,
abstract = {We present an approach for adaptive scheduling of soft real- time legacy applications (for which no timing information is exposed to the system). Our strategy is based on the com- bination of two techniques: 1) a real-time monitor that ob- serves the sequence of events generated by the application to infer its activation period, 2) a feedback mechanism that adapts the scheduling parameters to ensure a timely execu- tion of the application. By a thorough experimental evalua- tion of an implementation of our approach, we show its per- formance and its efficiency.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Real-time scheduling.

        
Posx RT misnamed.

        
Want to ``fix'' legacy RT apps to use RT APIs where available rather than just asking for high priority.

        
Classic monitor-infer-modify loop.

        
SLI but for timing rather than scheduling.

        
 -- Actually, no, that's not what they're doing.  Seem to be trying to infer EDF params by monitoring CPU usage? syscall trace?

        
Not actually using EDF schedule, but a CPU throttle.

        
Lots of room for symbolic exec \& ESD-style synthesis.

        
Identification of period confused the crap out of me.

        
Talk generally confusing, but worth reading paper properly.

        
App. specific?
What happens on slow processor?
Multi-threading hard?

      },
author = {Cucinotta, Tommaso and Checconi, Fabio and Abeni, Luca and Palopoli, Luigi},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p55.pdf:pdf},
keywords = {Experimentation,Measure- ment,Performance},
pages = {55--67},
publisher = {ACM},
title = {{Self-tuning Schedulers for Legacy Real-Time Applications}},
year = {2010}
}
@inproceedings{Pizlo2010,
abstract = {While managed languages such as C\# and Java have become quite popular in enterprise computing, they are still consid- ered unsuitable for hard real-time systems. In particular, the presence of garbage collection has been a sore point for their acceptance for low-level system programming tasks. Real- time extensions to these languages have the dubious distinc- tion of, at the same time, eschewing the benefits of high- level programming and failing to offer competitive perfor- mance. The goal of our research is to explore the limita- tions of high-level managed languages for real-time systems programming. To this end we target a real-world embed- ded platform, the LEON3 architecture running the RTEMS real-time operating system, and demonstrate the feasibility of writing garbage collected code in critical parts of embed- ded systems.We show that Java with a concurrent, real-time garbage collector, can have throughput close to that ofCpro- grams and comes within 10\% in the worst observed case on realistic benchmark.We provide a detailed breakdown of the costs of Java features and their execution times and compare to real-time and throughput-optimized commercial Java vir- tual machines.},
address = {Paris},
annote = {(From presentation at EuroSys10)

        
Hard RT system -- RTEMS.
Want to use Java.
No JIT -- custom static compiler.  Quite simple.
Dedicated GC thread at low priority -- pause for out-of-memory.
Talk was really boring
Not sure what the point of this thing was.
Assert that GC schedulability can be checked easily.

        

      },
author = {Pizlo, Filip and Ziarek, Lukasz and Blanton, Ethan and Maj, Petr and Vitek, Jan},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p69.pdf:pdf},
keywords = {Java virtual machine,Memory management,Real-time systems},
pages = {69--82},
publisher = {ACM},
title = {{High-level Programming of Embedded Hard Real-Time Devices}},
url = {http://www.cs.purdue.edu/homes/jv/pubs/eurosys10.pdf},
year = {2010}
}
@inproceedings{Oliveira2010,
abstract = {In this paper, we propose a management framework for pro- tecting large computer systems against operator mistakes. By detecting and confining mistakes to isolated portions of the managed system, our framework facilitates correct oper- ation even by inexperienced operators.We built a prototype management system called Barricade based on our frame- work.We evaluate Barricade by deploying it for two differ- ent systems, a prototype Internet service and an enterprise computer infrastructure, and conducting experiments with 20 volunteer operators. Our results are very promising. For example, we show that Barricade can detect and contain 39 out of the 43 mistakes that we observed in 49 live operator experiments performed with our Internet service.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Protecting against operator errors

        
Claim to block a useful subset of mistakes.
Monitor action, estimate cost of an operation if it turns out to be a mistake, force testing of high-risk changes.

        
Lots of UML-style block diagrams in place of thought -- policy engine in a box.

        
Seems to relu on operator setting a pre-canned list of things they want to do => why not just script those things?

        
Blocks on mistakes are pretty much just freezing unrelated machines when you start changine one machine.

        
Relies on a lot of manually estimated probabilities.

        
Presenter feld the need to explain Bayes' theorem to us despite not really understanding it himself.

        
          
Very stupid cost model.

        
Useful bit is improved system visibility.

      },
author = {Oliveira, F. and Tjang, Andrew and Bianchini, R. and Martin, R.P. and Nguyen, T.},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p83.pdf:pdf},
keywords = {manageability,operator mistakes},
organization = {EuroSys},
pages = {83--96},
publisher = {ACM},
title = {{Barricade: Defending Systems Against Operator Mistakes}},
url = {http://vivo.cs.rutgers.edu/publications/euro036-oliveira.pdf},
year = {2010}
}
@inproceedings{Bodik2010,
abstract = {Contemporary datacenters comprise hundreds or thousands of machines running applications requiring high availability and responsiveness. Although a performance crisis is easily detected by monitoring key end-to-end performance indica- tors (KPIs) such as response latency or request throughput, the variety of conditions that can lead to KPI degradation makes it difficult to select appropriate recovery actions. We propose and evaluate a methodology for automatic classification and identification of crises, and in particular for detecting whether a given crisis has been seen before, so that a known solution may be immediately applied. Our ap- proach is based on a new and efficient representation of the datacenterâs state called a fingerprint, constructed by statis- tical selection and summarization of the hundreds of perfor- mance metrics typically collected on such systems. Our eval- uation uses 4 months of trouble-ticket data from a produc- tion datacenter with hundreds of machines running a 24x7 enterprise-class user-facing application. In experiments in a realistic and rigorous operational setting, our approach pro- vides operators the information necessary to initiate recov- ery actions with 80\% correctness in an average of 10 min- utes, which is 50 minutes earlier than the deadline provided to us by the operators. To the best of our knowledge this is the first rigorous evaluation of any such approach on a large- scale production installation.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Figuring out what's gone wrong following a performance problem.

        
Crises recur with similar state.

        
Contributions:
-- Fingerprint system state such that 1:(1+bottom) mapping from fingerprints to crisis causes.
-- Use fingerprints to detect crashes more quickly and accurately.
-- Eval on data for real commercial service

        
Crisis == violation of SLA
Metric == arbitrary time series.  Pre-selected by experienced operators.

        
--Picking features: logistic regression with L1 constraints?  Automatic, apparently.
--Summarise metrics to 3 quantiles: appropriate sensitivity to outliers
--Map metrics to hot/normal/cold i.e. count how many servers in each quantile and then see if there are more than expected number in a given cell.

        
I have no idea what this man is trying to tell me.

      },
author = {Bodik, Peter and Goldszmidt, Moises and Fox, Armando and Woodard, Dawn B and Andersen, Hans},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p111.pdf:pdf},
keywords = {datacenters,performance,web applications},
pages = {111--124},
publisher = {ACM},
title = {{Fingerprinting the Datacenter : Automated Classification of Performance Crises}},
year = {2010}
}
@inproceedings{Ding2010,
abstract = {The benefits of virtualized IT environments, such as com- pute clouds, have drawn interested enterprises to migrate their applications onto new platforms to gain the advantages of reduced hardware and energy costs, increased flexibility and deployment speed, and reduced management complex- ity. However, the process of migrating a complex application takes a considerable amount of effort, particularly when per- forming post-migration testing to verify that the application still functions correctly in the target environment. The tra- ditional approach of test case generation and execution can take weeks and synthetic test cases may not adequately re- flect actual application usage. In this paper, we propose and evaluate a black-box ap- proach for post-migration testing ofWeb applications with- out manually creating test cases. AWeb proxy is put in front of the production application to intercept all requests from real users, and these requests are simultaneously sent to the production and migrated applications. Results generated by both applications are then compared, and mismatches due to migration problems can be easily detected and presented to testing teams for resolution.We implement this approach in Splitter, a software module that is deployed as a reverse Web proxy. Through our evaluation using a number of real applications, we show that Splitter can effectively automate post-migration testing while also reduce the number of mis- matches that must be manually inspected. Equally impor- tant, it imposes a relatively small performance overhead on the production environment.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Service migration.

        
Yet another reverse HTTP proxy

        
Requests go to known-good app and app under test.  Compare and contract responses.

        
Some amount of request munging necessaqry to fix up non-determinism.  Also on responses.  Seems to be done ad-hoc.  Requires MITM on SSL.

        
Keep per-session state in splitter to aid munging.

        
Non-identical responses stashed for later off-line analysis.

        
Later analysis uses an ad-hoc canonicalisation, per file format.

        
Testing against actual live production data + validation against known-good implementation is kind of cute.

        
Apparently more stuff in paper, incliding de-non-determinism.},
author = {Ding, Xiaoning and Huang, Hai and Ruan, Yaoping and Shaikh, Anees and Peterson, Brian and Zhang, Xiaodong},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p97.pdf:pdf},
keywords = {Design,Experimentation,Management,Verification},
pages = {97--110},
publisher = {ACM},
title = {{Splitter : A Proxy-based Approach for Post-Migration Testing of Web Applications}},
year = {2010}
}
@inproceedings{Koufaty2010,
abstract = {Heterogeneous architectures that integrate a mix of big and small cores are very attractive because they can achieve high single-threaded performance while enabling high per- formance thread-level parallelism with lower energy costs. Despite their benefits, they pose significant challenges to the operating system software. Thread scheduling is one of the most critical challenges. In this paper we propose bias scheduling for heteroge- neous systems with cores that have different microarchitec- tures and performance.We identify key metrics that charac- terize an application bias, namely the core type that best suits its resource needs. By dynamically monitoring application bias, the operating systemis able tomatch threads to the core type that can maximize system throughput. Bias scheduling takes advantage of this by influencing the existing scheduler to select the core type that bests suits the application when performing load balancing operations. Bias scheduling can be implemented on top of most ex- isting schedulers since its impact is limited to changes in the load balancing code. In particular, we implemented it over the Linux scheduler on a real system that models microar- chitectural differences accurately and found that it can im- prove system performance significantly, and in proportion to the application bias diversity present in theworkload.Unlike previouswork, bias scheduling does not require sampling of CPI on all core types or offline profiling.We also expose the limits of dynamic voltage/frequency scaling as an evaluation vehicle for heterogeneous systems.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Heterogeneous multicore scheduling.

        
Bias -> win moving a thread from small core to large one.  Dynamic: varies over time, depends on input.

        
CPU divided into execution, stall for core-internal resources, stall for external resources -- mereasure with hw perf counter.
-- External dominates -> run on small core
-- Exec dominates -> run on big core
-- Internal dominates -> run on small core.  Not sure about that.

        
Trend is vaguely right, but very noisy.

        
Use bias to tweak kernel scheduler load balancer.  When migrating from core X to core Y, pick the thread with the ``best'' bias.  Also do a periodic affinity sanity check.

        
Evail: restruct hardware to single execution reture at a time.  Different results to freq. scaling.

        
Eval 2B, 1B2S, 1B3S, all at same frequency.

        
Max win about 18\% over stock scheduler for very heterogeneous workload -> probably not worth doing.

        
Homogeneous workload -> up to 9\%, average 3 -> more worthwhile.

      },
author = {Koufaty, David and Reddy, Dheeraj and Hahn, Scott},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p125.pdf:pdf},
keywords = {Algorithms,Performance},
pages = {125--138},
publisher = {ACM},
title = {{Bias Scheduling in Heterogeneous Multi-core Architectures}},
year = {2010}
}
@inproceedings{Saez2010,
abstract = {Symmetric-ISA (instruction set architecture) asymmetric- performance multicore processors were shown to deliver higher performance per watt and area for applications with diverse architectural requirements, and so it is likely that future multicore processors will combine a few fast cores characterized by complex pipelines, high clock frequency, high area requirements and power consumption, and many slow ones, characterized by simple pipelines, low clock frequency, low area requirements and power consumption. Asymmetric multicore processors (AMP) derive their effi- ciency from core specialization. Efficiency specialization ensures that fast cores are used for âCPU-intensiveâ appli- cations, which efficiently utilize these coresâ âexpensiveâ features, while slow cores would be used for âmemory- intensiveâ applications, which utilize fast cores inefficiently. TLP (thread-level parallelism) specialization ensures that fast cores are used to accelerate sequential phases of parallel applications, while leaving slow cores for energy-efficient execution of parallel phases. Specialization is effected by an asymmetry-aware thread scheduler, which maps threads to cores in consideration of the properties of both. Previous asymmetry-aware schedulers employed one type of special- ization (either efficiency or TLP), but not both. As a result, they were effective only for limited workload scenarios. We propose, implement, and evaluate CAMP, a Compre- hensive AMP scheduler, which delivers both efficiency and TLP specialization. Furthermore, we propose a new light- weight technique for discovering which threads utilize fast cores most efficiently. Our evaluation in the OpenSolaris op- erating system demonstrates that CAMP accomplishes an efficient use of an AMP system for a variety of workloads, while existing asymmetry-aware schedulers were effective only in limited scenarios.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
I'm not awake enough to understand this.

        
``Trick'' seems to be running tetss several times and only counting last result i.e. can do unrealistic training.

        
Using last-level-cache miss rate as proxy for stall rate.

        
They seem to mostly be thinking about static scheduling, which is surprising.

        
Results are, we're assured, good, but I've no idea what they actually mean.

      },
author = {Saez, Juan Carlos and Preiro, Manuel and Fedorova, Alexandra and Bagodurov, Sergey},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p139.pdf:pdf},
keywords = {asymmetric multicore,operating systems,scheduling},
pages = {139--152},
publisher = {ACM},
title = {{A Comprehensive Scheduler for Asymmetric Multicore Systems}},
year = {2010}
}
@inproceedings{Depoutovitch2010,
abstract = {The default behavior of all commodity operating systems to- day is to restart the system when a critical error is encoun- tered in the kernel. This terminates all running applications with an attendant loss of âwork in progressâ that is non- persistent. Otherworld is a mechanism that microreboots the operat- ing system kernel when a critical error is encountered in the kernel, and it does so without clobbering the state of the run- ning applications. After the kernel microreboot, Otherworld attempts to resurrect the applications that were running at the time of failure. It does so by restoring the application memory spaces, open files and other resources. In the default case it then continues executing the processes from the point at which they were interrupted by the failure. Optionally, ap- plications can have user-level recovery procedures registered with the kernel, in which case Otherworld passes control to these procedures after having restored their process state. Recovery procedures might check the integrity of applica- tion data and restore resources Otherworld was not able to restore. We implemented Otherworld in Linux, but we believe that the technique can be applied to all commodity operating systems. In an extensive set of experiments on real-world applications (MySQL, Apache/PHP, Joe, vi), we show that Otherworld is capable of successfully microrebooting the kernel and restoring the applications in over 97\% of the cases. In the default case, Otherworld adds zero overhead to normal execution. In an enhanced mode, Otherworld can provide extra application memory protection with overhead of between 4\% and 12\%.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Use a kdump kernel and some signals to userspace.

        
After a crash, kexec new kernel, which slurps old processes out of memory dump and send them the crashed-recovered signal.

        
Doesn't restore sockets, pipe, ptys.

        
Essentially unexec for kernels.

        
All syscalls aborted on k crash.

        
4G/4G kernel-user split.

        
Most injected crashes can be recovered from.  1 in 800 gave application data error.

        
The demo worked.

        
The audience seemed to like it.

        
Won best student paper.

      },
author = {Depoutovitch, Alex and Stumm, Michael},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p181.pdf:pdf},
keywords = {crash kernel,kernel,microreboot,recovery},
pages = {181--194},
publisher = {ACM},
title = {{Otherworld - Giving Applications a Chance to Survive OS Kernel Crashes}},
year = {2010}
}
@inproceedings{Merkel2010,
abstract = {In multicore systems, shared resources such as caches or the memory subsystem can lead to contention between applica- tions running on different cores, entailing reduced perfor- mance and poor energy efficiency. The characteristics of in- dividual applications, the assignment of applications to ma- chines and execution contexts, and the selection of processor frequencies have a dramatic impact on resource contention, performance, and energy efficiency. We employ the concept of task activity vectors for char- acterizing applications by resource utilization. Based on this characterization, we apply migration and co-scheduling policies that improve performance and energy efficiency by combining applications that use complementary resources, and use frequency scaling when scheduling cannot avoid contention owing to inauspicious workloads. We integrate the policies into an operating system sched- uler and into a virtualization system, allowing placement de- cisions to be made both within and across physical nodes, and reducing contention both for individual tasks and com- plete applications. Our evaluation based on the Linux op- erating system kernel and the KVM virtualization environ- ment shows that resource-conscious scheduling reduces the energy delay product considerably.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Energy-efficiency in symmetrical multicore systems)

        
Memory is a bottleneck.

        
Use Bulpin's trick of co-scheduling heterogenous applications to better use available resources.

        
Frequency scal processor down.

        
Freq. scaling domains quite coarse, which forces us to combine similar applications.

        
Can't do both -> have to pick one.

        
Reducing freq a good idea for memory-bound apps UNLESS you have a lot run running: contention -> freq. scaling ba.

        
Implementatin: Sort some run queues in ascending order of memory-intensiveness, and some in descending order.  Play with timeslice length and offsets to get right kind of synchronisation.

        
Use VMs to do cross-host balancing.

        
Results look decide, but hard to interpret.},
author = {Merkel, Andreas and Stoess, Jan and Bellos, Frank},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p153.pdf:pdf},
keywords = {CMP,activity vectors,energy-aware scheduling,frequency scaling,migration,resources,task characteristics,virtualization},
pages = {153--166},
publisher = {ACM},
title = {{Resource-conscious Scheduling for Energy Efficiency on Multicore Processors}},
year = {2010}
}
@inproceedings{Chipounov2010,
abstract = {This paper presents a technique that helps automate the re- verse engineering of device drivers. It takes a closed-source binary driver, automatically reverse engineers the driverâs logic, and synthesizes new device driver code that imple- ments the exact same hardware protocol as the original driver. This code can be targeted at the same or a different OS. No vendor documentation or source code is required. Drivers are often proprietary and available for only one or two operating systems, thus restricting the range of de- vice support on all other OSes. Restricted device support leads to low market viability of new OSes and hampers OS researchers in their efforts to make their ideas available to the âreal world.â Reverse engineering can help automate the porting of drivers, as well as produce replacement drivers with fewer bugs and fewer security vulnerabilities. Our technique is embodied in RevNIC, a tool for reverse engineering network drivers.We use RevNIC to reverse en- gineer four proprietary Windows drivers and port them to four different OSes, both for PCs and embedded systems. The synthesized network drivers deliver performance nearly identical to that of the original drivers.},
address = {Paris},
annote = {(From presentation at EuroSys10)

        
What I wanted to do.  Almost.  I wanted a spec for device models, they want a new driver.  Possibly slightly easier?

        
Essentially a fancy decompile and recompile.

        
Symbolic execution of driver.  Don't need real hardware.

        
Phase 1: run initialize and pick a successful run.  Less state forking than I would expect.

        
Exercise produces execution traces.  Includes BB, data?

        
Code gen merges BBs from traces to form CFG.  CFG structure effectively preserved in recompilation: everthing done intra-BB.

        
Eval on native pc and qemu.  Why?

        
Re-engineered drivers faster than original?

        
Spoke to GC after talk, asked how he handles data structure translation (e.g. MDL to skbuff chain).  He said they did it in the template.  Suggests that the templates are actually quite a bit bigger than they're letting on; hmm.  Potentially close to the trivial template==NDISwrapper implementation.

      },
author = {Chipounov, Vitaly and Candea, George},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p167.pdf:pdf},
keywords = {binary,closed-source,device drivers,proprietary software,reverse engineering},
pages = {167--180},
publisher = {ACM},
title = {{Reverse Engineering of Binary Device Drivers with RevNIC}},
year = {2010}
}
@inproceedings{Li2010,
abstract = {Targeting the operating system (OS) kernels, kernel rootkits pose a formidable threat to computer systems and their users. Recent efforts have made significant progress in blocking them from injecting malicious code into the OS kernel for execution. Unfortunately, they cannot block the emerging so-called return-oriented rootkits (RORs).Without the need of injecting their own malicious code, these rootkits can dis- cover and chain together âreturn-oriented gadgetsâ (that con- sist of only legitimate kernel code) for rootkit computation. In this paper, we propose a compiler-based approach to defeat these return-oriented rootkits. Our approach recog- nizes the hallmark of return-oriented rootkits, i.e., the ret instruction, and accordingly aims to completely remove them in a runningOS kernel. Specifically, one key technique named return indirection is to replace the return address in a stack frame into a return index and disallow a ROR from us- ing their own return addresses to locate and assemble return- oriented gadgets. Further, to prevent legitimate instructions that happen to contain return opcodes from being misused, we also propose two other techniques, register allocation and peephole optimization, to avoid introducing them in the first place.We have developed a LLVM-based prototype and used it to generate a return-less FreeBSD kernel. Our evalu- ation results indicate that the proposed approach is generic, effective, and can be implemented on commodity hardware with a low performance overhead.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Return-oriented programming: find a load of fragments of legitimate program which end in ret instructions and chain them together using a corrupted stack and hence do something malicious.

        
Need to consider overlapping instructions

        
Return indirection removes actual return instructions -- indirect all returns through a global lookup table (why global?).

        
Register operands removed by just marking dangerous register use as in-use on certain instructions during compiler register allocations.

        
Other things get peepholed.  Relative offsets fiddled with via nop padding.

        
Binary size += 10\%.  Run-time overhead O(3\%).

        
Comparison to ASLR?

      },
author = {Li, Jinku and Wang, Zhi and Jiang, Xuxian and Grace, Mike and Bahram, Sina},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p195.pdf:pdf},
keywords = {malware defense,return-less kernel,return-oriented rootkits},
pages = {195--208},
publisher = {ACM},
title = {{Defeating Return-Oriented Rootkits With â Return-less â Kernels}},
year = {2010}
}
@inproceedings{Steinberg2010,
abstract = {The availability of virtualization features in modern CPUs has reinforced the trend of consolidating multiple guest operating systems on top of a hypervisor in order to im- prove platform-resource utilization and reduce the total cost of ownership. However, todayâs virtualization stacks are unduly large and therefore prone to attacks. If an adver- sary manages to compromise the hypervisor, subverting the security of all hosted operating systems is easy. We show how a thin and simple virtualization layer reduces the attack surface significantly and thereby increases the overall security of the system.We have designed and implemented a virtualization architecture that can host multiple unmodified guest operating systems. Its trusted computing base is at least an order of magnitude smaller than that of existing sys- tems. Furthermore, on recent hardware, our implementation outperforms contemporary full virtualization environments.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Point of reference Xen 1.0 or VMWare ESX

        
Has no concept of the difference between attack surface and TCB size.

        
Uses a whole bunch of standard TCB reduction tricks -- has expected effect on performance.

        
Zero value.

        
This wasn't actually all that bad (although it wasn't all that great, either), but it started with a complete straw-man characterisation of Xen which annoyed me so much that I spent the entire presentation looking for reasons to hate it.

        
Bit of a cycle of reincarnation going on here.

      },
author = {Steinberg, Udo and Kauer, Bernhard},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p209.pdf:pdf},
keywords = {Architecture,Virtualization},
pages = {209--222},
publisher = {ACM},
title = {{NOVA : A Microhypervisor-Based Secure Virtualization Architecture}},
year = {2010}
}
@inproceedings{Alvaro2010,
abstract = {Building and debugging distributed software remains ex- tremely difficult. We conjecture that by adopting a data- centric approach to system design and by employing declar- ative programming languages, a broad range of distributed software can be recast naturally in a data-parallel program- ming model. Our hope is that this model can significantly raise the level of abstraction for programmers, improving code simplicity, speed of development, ease of software evo- lution, and program correctness. This paper presents our experience with an initial large- scale experiment in this direction. First, we used the Overlog language to implement a âBig Dataâ analytics stack that is API-compatible with Hadoop and HDFS and provides com- parable performance. Second, we extended the system with complex distributed features not yet available in Hadoop, including high availability, scalability, and unique monitor- ing and debugging facilities. We present both quantitative and anecdotal results from our experience, providing some concrete evidence that both data-centric design and declara- tive languages can substantially simplify distributed systems programming.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Want to raise level of abstraction in dist. systems programming.

        
Buzzwords: data-centric, high-level data queries.

        
Yet another Overlog paper -- not convinced they did anything else.

        
HDFS == Google filesystem

        
Prolog+databases for stupid people.

        
Systems people shouldn't be allowed to play with languages.

        
Triggered ranting on part of audience.

      },
author = {Alvaro, Peter and Condie, Tyson and Conway, Neil and Elmeleegy, Khaled and Hellerstein, Joseph M and Sears, Russell},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p223.pdf:pdf},
keywords = {cloud computing,datalog,mapreduce},
pages = {223--236},
publisher = {ACM},
title = {{BOOM Analytics : Exploring Data-Centric , Declarative Programming for the Cloud}},
year = {2010}
}
@inproceedings{Nathuji2010,
abstract = {Cloud computing offers users the ability to access large pools of computational and storage resources on demand. Multiple commercial clouds already allow businesses to re- place, or supplement, privately owned IT assets, alleviating them from the burden of managing and maintaining these facilities. However, there are issues that must be addressed before this vision of utility computing can be fully real- ized. In existing systems, customers are charged based upon the amount of resources used or reserved, but no guaran- tees are made regarding the application level performance or quality-of-service (QoS) that the given resources will pro- vide. As cloud providers continue to utilize virtualization technologies in their systems, this can become problematic. In particular, the consolidation of multiple customer appli- cations onto multicore servers introduces performance inter- ference between collocated workloads, significantly impact- ing application QoS. To address this challenge, we advocate that the cloud should transparently provision additional re- sources as necessary to achieve the performance that cus- tomerswould have realized if they were running in isolation. Accordingly, we have developed Q-Clouds, a QoS-aware control framework that tunes resource allocations to miti- gate performance interference effects. Q-Clouds uses online feedback to build a multi-input multi-output (MIMO) model that captures performance interference interactions, and uses it to perform closed loop resource management. In addition, we utilize this functionality to allow applications to specify multiple levels of QoS as application Q-states. For such ap- plications, Q-Clouds dynamically provisions underutilized resources to enable elevated QoS levels, thereby improving system efficiency. Experimental evaluations of our solution using benchmark applications illustrate the benefits: perfor- mance interference is mitigated completely when feasible, and system utilization is improved by up to 35\% using Q- states.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Interested in interface below cycle-sharing level.

        
Increase provsion when interference happens, post-hoc.  Closed-loop control.

        
Rely on feedback from VM.

        
VMs define a set of QoS states, and prices they're willing to pay, then capacity gets auctioned.

        
Dear God, why did I come to this session?

        
How are they measuring interference?

        
LISA crap, as far as I can tell.

        
There are potentially some trust issues here?

      },
author = {Nathuji, Ripal and Kansal, Aman and Ghaffarkhah, Alireza},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p237.pdf:pdf},
keywords = {cloud computing,resource management,virtualization},
pages = {237--250},
publisher = {ACM},
title = {{Q-Clouds : Managing Performance Interference Effects for QoS-Aware Clouds}},
year = {2010}
}
@inproceedings{Iu2010,
abstract = {MapReduce is a cost-effective way to achieve scalable per- formance for many log-processing workloads. These work- loads typically process their entire dataset. MapReduce can be inefficient, however, when handling business-oriented workloads, especially when these workloads access only a subset of the data. HadoopToSQL seeks to improve MapReduce perfor- mance for the latter class of workloads by transforming MapReduce queries to use the indexing, aggregation and grouping features provided by SQL databases. It statically analyzes the computation performed by the MapReduce queries. The static analysis uses symbolic execution to de- rive preconditions and postconditions for the map and reduce functions. It then uses this information either to generate in- put restrictions, which avoid scanning the entire dataset, or to generate equivalent SQL queries, which take advantage of SQL grouping and aggregation features. We demonstrate the performance of MapReduce queries, when optimized by HadoopToSQL, by both single-node and cluster experiments. HadoopToSQL always improves per- formance over MapReduce and approximates that of hand- written SQL.},
address = {Paris},
annote = {(from presentation at Eurosys10)

        
Compile map-reduce to SQL + optimise

        
Essentially making it use indices.

        
Phase 1: build CFG of map function.  Enum paths through CFG and classify according to whether they produce output.  Discard no-output paths.

        
Phase 2: find path constraints on output paths.  Use that to filter which records the map actually sees.  Optimise filter using indices.

        
Eval a bit weird -- not clear how much of their win was due to SQLServer doing better than HDBC and how much due to their index discovery thing.

        
Also have a ``total query rewriting'', which they don't describe in talk, but apparently do describe in paper?  It gets renamed there to ``complete translation'', though.  Blah.

      },
author = {Iu, Ming-yee and Zwaenepoel, Willy},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p251.pdf:pdf},
keywords = {Languages,Performance},
pages = {251--264},
publisher = {ACM},
title = {{HadoopToSQL a MapReduce Query Optimizer}},
year = {2010}
}
@inproceedings{Zaharia2010,
abstract = {As organizations start to use data-intensive cluster comput- ing systems like Hadoop and Dryad for more applications, there is a growing need to share clusters between users. However, there is a conflict between fairness in schedul- ing and data locality (placing tasks on nodes that contain their input data). We illustrate this problem through our ex- perience designing a fair scheduler for a 600-node Hadoop cluster at Facebook. To address the conflict between local- ity and fairness, we propose a simple algorithm called delay scheduling: when the job that should be scheduled next ac- cording to fairness cannot launch a local task, it waits for a small amount of time, letting other jobs launch tasks instead. We find that delay scheduling achieves nearly optimal data locality in a variety of workloads and can increase through- put by up to 2x while preserving fairness. In addition, the simplicity of delay scheduling makes it applicable under a wide variety of scheduling policies beyond fair sharing.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Diverse workload, most quite short (median in facebook hadoop cluster 84 seconds)

        
Strict fair scheduling hurts locality

        
Tendency to get stuck in particular mode, whihc might be bad.

        
Answer: insert artifical delay if you can't start in a good place; essentially the anticipatory schedule trick.

        
Worth waiting if E(wait time) < E(extra cost to run non-locally).  Followed by bogomaths.  Enough to suggeste actual experimental results with generalise.

        
Experiments are on an EC2 hadoop deployment basewd on FB data.  Looks like fairness and delays both help.

      },
author = {Zaharia, Matei and Borthakur, Dhruba and Sarma, Joydeep Sen and Elmeleegy, Khaled and Shenker, Scott and Stoica, Ion},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p265.pdf:pdf},
keywords = {algorithms,design,performance},
pages = {265--278},
publisher = {ACM},
title = {{Delay Scheduling : A Simple Technique for Achieving Locality and Fairness in Cluster Scheduling}},
year = {2010}
}
@inproceedings{Wobber2009,
abstract = {Combining access control with weakly consistent replica- tion presents a challenge if the resulting system is to sup- port eventual consistency. If authorization policy can be tem- porarily inconsistent, any given operation may be permitted at one node and yet denied at another. This is especially trou- blesomewhen the operation in question involves a change in policy.Without a careful design, permanently divergent state can result. We describe and evaluate the design and implementation of an access control system for weakly consistent replication where peers are not uniformly trusted.Our systemallows for the specification of fine-grained access control policy over a collection of replicated items. Policies are expressed using a logical assertion framework and access control decisions are logical proofs. Policy can grow to encompass new nodes through fine-grain delegation of authority. Eventual consis- tency of the replicated data is preserved despite the fact that access control policy can be temporarily inconsistent.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
More cloud crap.

        
Assume nodes only eventually consistent.

        
AC matrix distributed and inconsistent.

        
Security policy encoded in logical framework? e.g. ``A says B can write *'', ``A says \%p can write pictures if \%p can write cartoons'', ``A says R says C can ...'', ``A says revoke...''.

        
Logic monotone with revocation.

        
Single root of trust called LA, because it's a Microsoft production.

        
Updates always propagate and are ordered and logic is monotone -> policy eventually consistent.

        
Data update races with policy update => effective rollback independent of originating client.

        
Performance not good, but no unusage for small deployments.

        
Large amounts of log-structuredness?

      },
author = {Wobber, Ted and Rodeheffer, T.L. and Terry, D.B.},
booktitle = {Eurosys 2010},
file = {:home/sos22/papers/eurosys10/p293.pdf:pdf},
keywords = {eventual consistency,replication,security logic},
organization = {Citeseer},
pages = {293--306},
publisher = {ACM},
title = {{Policy-based Access Control for Weakly Consistent Replication}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.3281\&amp;rep=rep1\&amp;type=pdf},
year = {2010}
}
@inproceedings{Chen2010,
abstract = {A complex software system typically has a large number of objects in the memory, holding references to each other to implement an object model. Deciding when the objects should be alive/active is non-trivial, but the decisions can be security-critical. This is especially true visual for web browsers: if certain browser objects do not disappear when the new page is switched in, basic security properties can be compromised, such as integrity, document integrity and memory safety. We refer to these browser objects as residue objects. Serious security vulnerabilities due to residue objects have been sporadically discovered in leading browser products in the past, such as IE, Firefox and Safari. However, this class of vulnerabilities has not been studied in the motivated by two questions: (1) what are the challenges imposed by residue research literature. Our work objects on the is browserâs logic correctness; (2) how prevalent can these vulnerabilities be in todayâs commodity browsers. As an example, we analyze the mechanisms for guarding residue objects in Internet Explorer (IE), and use an enumerative approach to expose and understand new vulnerabilities. Although only the native HTML engine is studied so far, we have already discovered five new vulnerabilities and reported them to IE developers (one of the vulnerabilities has been patched in a Microsoft security update). These vulnerabilities demonstrate a diversity of logic errors in the browser code. Moreover, our study empirically suggests that the actual prevalence of this type of vulnerabilities can be higher than what is perceived today. We also discuss how the browser industry should respond to this class of security problems.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Scripts allowed to hold references to objects on other pages.

        
Browsers try to stop residue from doing anything, but prone to bugs.

        
DOM security model: document switched on navigate, window preserved.

        
Important C++ classes have validity flags.

        
Basic approach: have an outer hosting window, an inner window, and an object in the inner window.  Create a ref from the outer window to the object.

        
Found lots of fiddly little bugs in IE.

        
Worthwhile work, not sure how it got published here; should have gone to an MS-internal journal.

      },
author = {Chen, Shuo and Chen, H. and Caballero, Manuel},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p279.pdf:pdf},
keywords = {browser security,component object model (COM),reside object},
pages = {279--291},
publisher = {ACM},
title = {{Residue Objects: A Challenge to Web Browser Security}},
url = {http://research.microsoft.com/pubs/119059/euro054-chen.pdf},
year = {2010}
}
@inproceedings{Chew2010a,
abstract = {Bugs in concurrent programs are extremely difficult to find and fix during testing. In this paper, we propose Kivati, which can efficiently detect and prevent atomicity violation bugs. Kivati imposes an average run-time overhead of 19\%, whichmakes it practical to deploy on software in production environments. The key attribute that allows Kivati to impose this low overhead is its use of hardware watchpoints, which can be found on most commodity processors. Kivati com- bines watchpoints with a simple static analysis that anno- tates regions of codes that likely need to be executed atomi- cally. Thewatchpoints are then used tomonitor these regions for interleaving accesses that may lead to an atomicity vio- lation. When an atomicity violation is detected, Kivati dy- namically reorders the access to prevent the violation from occurring.Kivati can be run in preventionmode,which opti- mizes for performance, or in bug-findingmode,which trades some performance for an enhanced ability to find bugs. We implement and evaluate a prototype ofKivati that pro- tects applications written in C on Linux/x86 platforms. We find that Kivati is able to detect and prevent atomicity viola- tion bugs in real applications, and imposes very reasonable overheadswhen doing so.},
address = {Paris},
annote = {(from presentation at eurosys10)

        
Re-order unserializable interleavings into serializable ones.

        
Use static analysis to label possible violations.  Mark as atomic block.

        
Seem to mostly consider single-variable AVs? --- future work.

        
Can use overlapping critical sections?

        
Atomic blocks enforced with hardware watchpoints.

        
Challenge: data watches fire *after* interesting access.  Undo accesses when necessary to fix it up.

        
Potential source of bugs?

      },
author = {Chew, Lee and Lie, David},
booktitle = {Eurosys 2010},
file = {:home/sos22/papers/eurosys10/p307.pdf:pdf},
keywords = {atomicity violation,kivati,watchpoint},
pages = {307--319},
publisher = {ACM},
title = {{Kivati : Fast Detection and Prevention of Atomicity Violations}},
year = {2010}
}
@inproceedings{Zamfir2010a,
abstract = {Debugging real systems is hard, requires deep knowledge of the code, and is time-consuming.Bug reports rarely provide sufficient information, thus forcing developers to turn into detectives searching for an explanation of how the program could have arrived at the reported failure point. Execution synthesis is a technique for automating this de- tective work: given a program and a bug report, it automat- ically produces an execution of the program that leads to the reported bug symptoms. Using a combination of static analysis and symbolic execution, it âsynthesizesâ a thread schedule and various required programinputs that cause the bug to manifest. The synthesized execution can be played back deterministically in a regular debugger, like gdb. This is particularly useful in debugging concurrency bugs. Our technique requires no runtime tracing or program modifications, thus incurring no runtime overhead and being practical for use in production systems.We evaluate ESDâ a debugger based on execution synthesisâon popular soft- ware (e.g., the SQLite database, ghttpdWeb server,HawkNL network library, UNIX utilities): starting from mere bug re- ports, ESD reproduces on its own several real concurrency and memory safety bugs in less than three minutes.},
address = {Paris},
annote = {(from presentation at eurosys10)

        
Check related work for conc. bug info.

        
Want to recreate *an* execution which will exhibit the target bug (i.e. not *the* execution).

        
Naive approach leads to state space explosion.

        
Key idea: use static analysis to find intermediate goals.

        
Intermediate goals: find CFG edges which must be present on path.  Use that to find data constraints, recur.

        
Focused path search: exhaustive heuristic-driven search from current root to first goal, then repeat from there.

        
Prox. heuristic == lowe bound on \# instructions to goal.

        
Thread scheduling: schedule is a symbolic var. with a bool at each pre-emption point.  Bias towards stack in core dumps.},
author = {Zamfir, Cristian and Candea, George},
booktitle = {Eurosys 2010},
file = {:home/sos22/papers/eurosys10/p321.pdf:pdf},
keywords = {automated debugging,symbolic execution},
pages = {321--334},
publisher = {ACM},
title = {{Execution Synthesis: A Technique for Automated Software Debugging}},
url = {http://dslab.epfl.ch/pubs/esd.pdf},
year = {2010}
}
@inproceedings{Vigfussion2010,
abstract = {IP Multicast (IPMC) in data centers becomes disruptive when the technology is used by a large number of groups, a capability desired by event notification systems. We trace the problem to root causes, and introduce Dr. Multicast (MCMD), a system that eliminates the issue by mapping IPMC operations to a combination of point-to-point uni- cast and traditional IPMC transmissions guaranteed to be safe. MCMD optimizes the use of IPMC addresses within a data center by merging similar multicast groups in a princi- pled fashion, while simultaneously respecting hardware lim- its expressed through administrator-controlled policies. The system is fully transparent, making it backward-compatible with commodity hardware and software found in modern data centers. Experimental evaluation shows that MCMD allows a large number of IPMC groups to be used without disruption, restoring a powerful group communication prim- itive to its traditional role.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Filters have limited capacity, overflow sometimes leads to multicast storms.

        
Dr. Multicast automatically degrades to multiple unicasts if multicast unsafe.

        
Also merge similar mcast groups to reduce resource usage.  *very* effective in realistic test.

        
And there's some social nets stuff.  Not sure where that came from, except possibly trying to pick up more buzzwords.

        
Logo is photoshop fail.},
author = {Vigfussion, Ymir and Abu-Libdeh, Hussam and Balakrishnan, Mahesh and Birman, Ken and Burgess, Robert and Chockler, Gregory and Li, Haoyuan and Tock, Yoav},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p349.pdf:pdf},
keywords = {Data centers,IPMC,Multicast},
pages = {349--362},
publisher = {ACM},
title = {{Dr . Multicast : Rx for Data Center Communication Scalability}},
year = {2010}
}
@inproceedings{Guerraoui2010,
abstract = {Modern Byzantine fault-tolerant state machine replication (BFT) protocols involve about 20,000 lines of challenging C++ code encompassing synchronization, networking and cryptography. They are notoriously difficult to develop, test and prove. We present a new abstraction to simplify these tasks.We treat a BFT protocol as a composition of instances of our abstraction. Each instance is developed and analyzed independently. To illustrate our approach, we first show how our ab- straction can be used to obtain the benefits of a state-of- the-art BFT protocol with much less pain. Namely, we de- velop AZyzzyva, a new protocol that mimics the behavior of Zyzzyva in best-case situations (for which Zyzzyva was op- timized) using less than 24\% of the actual code of Zyzzyva. To coverworst-case situations, our abstraction enables to use in AZyzzyva any existing BFT protocol, typically, a classical one like PBFT which has been tested and proved correct. We then present Aliph, a new BFT protocol that outper- forms previous BFT protocols both in terms of latency (by up to 30\%) and throughput (by up to 360\%). The develop- ment of Aliph required two new instances of our abstraction. Each instance contains less than 25\% of the code needed to develop state-of-the-art BFT protocols.},
address = {Paris},
annote = {(from presentation at EuroSys10)

        
Want a modular BFT protocol

        
ABSTRACT = ABortable STate mACine replicaTion.  Like state machine rep, but can abort and return request history.  Req. history is unforgeable pickle fo system state.

        
BFT = ABSTRACT which never aborts.

        
ABSTRACTS are composable.

        
I'm not awake enough to deal with this right now.

      },
author = {Guerraoui, Rachid and Knezevic, Nikola and Qu, Vivien and Vukolic, Marko},
booktitle = {EuroSys 2010},
file = {:home/sos22/papers/eurosys10/p363.pdf:pdf},
keywords = {byzantine failures,modularity,performance},
pages = {363--376},
publisher = {ACM},
title = {{The Next 700 BFT Protocols}},
year = {2010}
}
@inproceedings{Kapitza2010,
abstract = {State-machine replication is a general approach to ad- dress the increasing importance of network-based services by improving their availability and reliability via repli- cated execution. If a service is deterministic, multiple replicas will produce the same results, and faults can be tolerated by means of agreement protocols. Unfortunately, real-life services are often not determin- istic. One source of non-determinism is multithreaded execution with shared data access in which the thread exe- cution order is determined by the run-time system and the outcome may depend on which thread accesses data first. We present Storyboard, an approach that ensures de- terministic execution of multi-threaded programs. Sto- ryboard achieves this by utilizing application-specific knowledge to minimize costly inter-replica coordination and to exploit concurrency in a similar way as non- deterministic execution. This is accomplished by making a forecast for a likely execution path, provided as an or- dered sequence of locks that protect critical sections. If this forecast is correct, a request is executed in parallel to other running requests without further actions. Only in case of an incorrect forecast will an alternative execution path be resolved by inter-replica coordination.},
annote = {Optimised deterministic replay for state synchronisation.  Seem to assume that execution is race-free, then make a prediction of the actual lock acquisition order based on application-specific knowledge, and then optimise based on that prediction.  Model assumes that the service is request-response, and each request only uses a single thread; not clear that that's actually necessary.  When you get it wrong, you stop immediately and re-coordinate across all replicas to figure out what to do next.  There's apparently some subtlty to do with nested locks and deadlocks which I'm not convinced I fully understand.

        
Deadlock avoidance in general requires some assumptions about the types of synchronisation available to the service which aren't explicitly spelt out anywhere in the paper.

        
Kind of cute.

      },
author = {Kapitza, Rudiger and Schunter, Matthias and Cachin, Christian and Stengel, Klaus and Distler, Tobias},
booktitle = {HotDep2010},
file = {:home/sos22/papers/random/storyboard.pdf:pdf},
keywords = {unpublished},
mendeley-tags = {unpublished},
publisher = {IEEE},
title = {{Storyboard : Optimistic Deterministic Multithreading}},
year = {2010}
}
@inproceedings{Feiner2010,
abstract = {Dynamic binary instrumentation (DBI) has been used extensively at the user level to develop bugfinding and security tools, such as Memcheck and Program Shepherding. However, comprehensive DBI frameworks do not exist for operating system kernels, thwarting the development of dependability and se- curity tools for kernels. In this paper, we identify the key challenges in designing an in-kernel DBI framework and propose a design that addresses them.},
annote = {Basically ring0 valgrind.  Kind of obvious, really.  Very early; little more than a position paper.

      },
author = {Feiner, Peter and Brown, Angela Demke and Goel, Ashvin},
booktitle = {HotDep2010},
file = {:home/sos22/papers/random/kerninst.pdf:pdf},
keywords = {unpublished},
mendeley-tags = {unpublished},
publisher = {IEEE},
title = {{A Design for Comprehensive Kernel Instrumentation}},
year = {2010}
}
@inproceedings{Barham2003,
abstract = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
booktitle = {ACM Symposium on Operating Systems Principles},
keywords = {hypervisors,paravirtualization,virtual machine monitors},
number = {5},
pages = {164},
title = {{Xen and the art of virtualization}},
url = {http://portal.acm.org/citation.cfm?id=945462},
volume = {37},
year = {2003}
}
@inproceedings{Bellard,
abstract = {We present the internals of QEMU, a fast machine em- ulator using an original portable dynamic translator. It emulates several CPUs (x86, PowerPC,ARMand Sparc) on several hosts (x86, PowerPC, ARM, Sparc, Alpha and MIPS). QEMU supports full system emulation in which a complete and unmodified operating system is run in a virtual machine and Linux user mode emulation where a Linux process compiled for one target CPU can be run on another CPU.},
author = {Bellard, F},
booktitle = {USENIX Annual Technical Conference},
file = {:home/sos22/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellard - Unknown - QEMU, a Fast and Portable Dynamic Translator.pdf:pdf},
pages = {41--46},
title = {{QEMU, a Fast and Portable Dynamic Translator}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.4082\&rep=rep1\&type=pdf},
year = {2005}
}
@article{Portokalidis2006,
author = {Portokalidis, G and Slowinska, A and Bos, H},
journal = {Proceedings of the 1st ACM  \ldots},
title = {{Argos: an emulator for fingerprinting zero-day attacks for advertised honeypots with automatic signature generation}},
url = {http://portal.acm.org/citation.cfm?id=1217935.1217938},
year = {2006}
}
@inproceedings{Ousterhout,
author = {Ousterhout, J},
booktitle = {Invited talk at the 1996 Usenix Technical conference},
file = {::},
title = {{Why threads are a bad idea (for most purposes)}},
url = {http://rtcc.hanyang.ac.kr/course/2009\_01\_SLOS/04\_Slide\_Why\_Threads\_Are\_A\_Bad\_Idea.pdf},
year = {1996}
}
@article{Devietti2009,
abstract = {Current shared memory multicore and multiprocessor systems are nondeterministic. Each time these systems execute a multithreaded application, even if supplied with the same input, they can produce a different output. This frustrates debugging and limits the ability to properly test multithreaded code, becoming a major stumbling block to the much-needed widespread adoption of parallel programming. In this paper we make the case for fully deterministic shared memory multiprocessing (DMP). The behavior of an arbitrary multithreaded program on a DMP system is only a function of its inputs. The core idea is to make inter-thread communication fully deterministic. Previous approaches to coping with nondeterminism in multithreaded programs have focused on replay, a technique useful only for debugging. In contrast, while DMP systems are directly useful for debugging by offering repeatability by default, we argue that parallel programs should execute deterministically in the field as well. This has the potential to make testing more assuring and increase the reliability of deployed multithreaded software. We propose a range of approaches to enforcing determinism and discuss their implementation trade-offs. We show that determinism can be provided with little performance cost using our architecture proposals on future hardware, and that software-only approaches can be utilized on existing systems.},
author = {Devietti, Joseph and Lucia, Brandon and Ceze, Luis and Oskin, Mark},
journal = {Architectural Support for Programming Languages and Operating Systems},
keywords = {debugging,determinism,multicores,parallel programming},
number = {3},
pages = {85--96},
title = {{DMP:Â deterministic shared memory multiprocessing}},
url = {http://portal.acm.org/citation.cfm?id=1508255},
volume = {44},
year = {2009}
}
@inproceedings{Aspinall2010,
author = {Aspinall, David and Atkey, Robert and MacKenzie, K and Sannella, Donald},
booktitle = {Proc. 5th Symp. on Trustworthy Global Computing (TGC 2010)},
file = {:home/sos22/papers/random/tgc2010.pdf:pdf},
title = {{Symbolic and Analytic Techniques for Resource Analysis of Java Bytecode}},
url = {http://homepages.inf.ed.ac.uk/dts/pub/tgc2010.pdf},
year = {2010}
}
@inbook{Sagonas2010,
abstract = {This invited talk will present the key ideas in the design and implementation of Dialyzer, a static analysis tool for Erlang programs. Dialyzer started as a defect detection tool using a rather ad hoc dataflow analysis to detect type errors in Erlang programs, but relatively early in its development it adopted a more disciplined approach to detecting definite type clashes in dynamically typed languages. Namely, an approach based on using a constraint-based analysis to infer success typings which are also enhanced with optional contracts supplied by the programmer. In the first part of the talk, we will describe this constraint-based approach to type inference and explain how it differs with past and recent attempts to type check programs written in dynamic languages. In the second part of the talk, we will present important recent additions to Dialyzer, namely analyses that detect concurrency defects (such as race conditions) in Erlang programs. For a number of years now, Dialyzer has been part of the Erlang/OTP system and has been actively used by its community. Based on this experience, we will also critically examine Dialyzerâs design choices, show interesting cases of Dialyzerâs use, and distill the main lessons learned from using static analysis in open source as well as commercial code bases of significant size.},
address = {Berlin, Heidelberg},
author = {Sagonas, Konstantinos},
booktitle = {Functional and Logic Programming},
doi = {10.1007/978-3-642-12251-4},
editor = {Blume, Matthias and Kobayashi, Naoki and Vidal, Germ\'{a}n},
isbn = {978-3-642-12250-7},
pages = {13--18},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Using Static Analysis to Detect Type Errors and Concurrency Defects in Erlang Programs}},
url = {http://www.springerlink.com/content/cj763t86728623t3},
volume = {6009},
year = {2010}
}
@article{Naik2006,
abstract = {We present a novel technique for static race detection in Java programs, comprised of a series of stages that employ a combination of static analyses to successively reduce the pairs of memory accesses potentially involved in a race. We have implemented our technique and applied it to a suite of multi-threaded Java programs. Our experiments show that it is precise, scalable, and useful, reporting tens to hundreds of serious and previously unknown concurrency bugs in large, widely-used programs with few false alarms.},
annote = {Tool called Chord. Static race detection.  Depend on lexically-scoped-ness of synchronisation for goodness.  Require type annotations, but not many (42 in 1.5 million lines of code).  Unsound.  Not flow sensitive.  Use stubs and harnesses in a few places.  Another BDDBDDB thing.  Seems to be pretty songly java-focused.

        
Implicit assumption that they ``start'' soon enough that they don't have to worry about ambient threads (true for static analysis, not for dynamic attach).

        
Solid enough, but not very useful for anything I want to be working on.},
author = {Naik, Mayur and Aiken, Alex and Whaley, John},
file = {:home/sos22/papers/random/p308-naik.pdf:pdf},
issn = {0362-1340},
journal = {ACM SIGPLAN Notices},
keywords = {Java,concurrency,multi-threading,static race detection,synchronization},
number = {6},
pages = {308},
title = {{Effective static race detection for Java}},
url = {http://portal.acm.org/citation.cfm?id=1134018},
volume = {41},
year = {2006}
}
@inproceedings{Pratikakis2006,
abstract = {One common technique for preventing data races in multi-threaded programs is to ensure that all accesses to shared locations are con- sistently protected by a lock.We present a tool called LOCKSMITH for detecting data races in C programs by looking for violations of this pattern. We call the relationship between locks and the loca- tions they protect consistent correlation, and the core of our tech- nique is a novel constraint-based analysis that infers consistent cor- relation context-sensitively, using the results to check that locations are properly guarded by locks.We present the core of our algorithm for a simple formal language $\lambda$î which we have proven sound, and discuss how we scale it up to an algorithm that aims to be sound for all of C. We develop several techniques to improve the preci- sion and performance of the analysis, including a sharing analysis for inferring thread locality; existential quantification for modeling locks in data structures; and heuristics for modeling unsafe fea- tures of C such as type casts. When applied to several benchmarks, including multi-threaded servers and Linux device drivers, LOCK- SMITH found several races while producing a modest number of false alarms.},
address = {Ottawa},
annote = {Static lockset analysis for C.

        
Ignore unsafety due to pointer arithmetic, assume correct types for everything.  Quite a lot of semantics here; not sure how useful it all is.  Analysis is conservative in the sense that it always reports a race whenever one is possible.  Need programmer annotations in a few places to deal with the typing weirdnesses (mostly existential structs; not trivial things to explain to a naive programmer).  Don't seem to need very many, though.

        
At a high enough level, most of this is obvious, although the details are annoyingly fiddly.  There are a few places where they seem to be reasoning by analogy?

        
Eval: find a few bugs, but most of what they found seem to be false positives.

        
Actually kind of nice.  Not sure how much their ``safety'' buys them, because it's safety modulo features which are widely used.

      },
author = {Pratikakis, Polyvios and Foster, Jeffrey S and Hicks, Michael},
booktitle = {PLDI'06},
file = {:home/sos22/papers/random/10.1.1.83.8841.pdf:pdf},
keywords = {context-sensitivity,correlation,locksmith,multi-threaded programming,race detection,type inference},
pages = {320--331},
publisher = {ACM},
title = {{LOCKSMITH : Context-Sensitive Correlation Analysis for Race Detection}},
year = {2006}
}
@article{Bacon1994,
abstract = {In the last three decades a large number of compiler transformations for optimizing programs have been implemented. Most optimizations for uniprocessors reduce the number of instructions executed by the program using transformations based on the analysis of scalar quantities and data-flow techniques. In contrast, optimizations for high-performance superscalar, vector, and parallel processors maximize parallelism and memory locality with transformations that rely on tracking the properties of arrays using loop dependence analysis.This survey is a comprehensive overview of the important high-level program restructuring techniques for imperative languages, such as C and Fortran. Transformations for both sequential and various types of parallel architectures are covered in depth. We describe the purpose of each transformation, explain how to determine if it is legal, and give an example of its application.Programmers wishing to enhance the performance of their code can use this survey to improve their understanding of the optimizations that compilers can perform, or as a reference for techniques to be applied manually. Students can obtain an overview of optimizing compiler technology. Compiler writers can use this survey as a reference for most of the important optimizations developed to date, and as bibliographic reference for the details of each optimization. Readers are expected to be familiar with modern computer architecture and basic program compilation techniques.},
author = {Bacon, David F. and Graham, Susan L. and Sharp, Oliver J.},
issn = {0360-0300},
journal = {ACM Computing Surveys (CSUR)},
keywords = {compilation,dependence analysis,locality,multiprocessors,optimization,parallelism,superscalar processors,vectorization},
number = {4},
pages = {345},
title = {{Compiler transformations for high-performance computing}},
volume = {26},
year = {1994}
}
@article{Shacham2004,
abstract = {Address-space randomization is a technique used to fortify systems against buffer overflow attacks. The idea is to introduce artificial diversity by randomizing the memory location of certain system components. This mechanism is available for both Linux (via PaX ASLR) and OpenBSD. We study the effectiveness of address-space randomization and find that its utility on 32-bit architectures is limited by the number of bits available for address randomization. In particular, we demonstrate a <i>derandomization attack</i> that will convert any standard buffer-overflow exploit into an exploit that works against systems protected by address-space randomization. The resulting exploit is as effective as the original exploit, although it takes a little longer to compromise a target machine: on average 216 seconds to compromise Apache running on a Linux PaX ASLR system. The attack does not require running code on the stack. We also explore various ways of strengthening address-space randomization and point out weaknesses in each. Surprisingly, increasing the frequency of re-randomizations adds at most 1 bit of security. Furthermore, compile-time randomization appears to be more effective than runtime randomization. We conclude that, on 32-bit architectures, the only benefit of PaX-like address-space randomization is a small slowdown in worm propagation speed. The cost of randomization is extra complexity in system support.},
author = {Shacham, Hovav and Page, Matthew and Pfaff, Ben and Goh, Eu-Jin and Modadugu, Nagendra and Boneh, Dan},
journal = {Conference on Computer and Communications Security},
keywords = {address-space randomization,automated attacks,diversity},
pages = {298},
title = {{On the effectiveness of address-space randomization}},
url = {http://portal.acm.org/citation.cfm?id=1030083.1030124},
year = {2004}
}


@phdthesis{Weiser1979,
author = {Weiser, M.},
school = {University of Michigan},
title = {{Program slices: formal, psychological, and practical investigations of an automatic program abstraction method}},
year = {1979}
}

@inbook{DebugRegisters,
title={Intel 64 and IA-32 Architectures Software Developer's Manual},
chapter={16.2: Debug Registers},
publisher={Intel Corporation},
year={2010},
volume={3A},
author={Intel}
}

@misc{glibc2644,
title={Race condition during unwind code after thread cancellation},
url={{http://sourceware.org/bugzilla/show\_bug.cgi?id=2644}},
year={2006},
howpublished={Glibc bug number 2644},
author={Neil Campbell}
}

@misc{thunderbird39125,
title={crash [@ nsProxyReleaseEvent::Run()]},
url={{https://bugzilla.mozilla.org/show\_bug.cgi?id=391259}},
year={2007},
howpublished={Mozilla bug number 39125},
author={Wayne Mery}
}

