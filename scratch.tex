% All the bits which I've written, and would like to keep, but don't
% currently have anywhere to put.
\begin{itemize}
\item These predictors are, in effect, generalising the observed
  crash.  This is necessary in order to obtain a useful fix.  It would
  be straightforward to design a system which logged the entire
  execution of the crashing program and then monitored subsequent
  executions, noticed if they followed the exact same pattern, and
  then flipped around the last race.  This would prevent the bug from
  reproducing in exactly the same way on subsequent runs, but would
  also be completely useless.  This is because the probability of the
  exact same execution reproducing on subsequent runs, down to the
  level of individual instruction interleavings, is negligible for
  non-trivial programs, and so eliminating the possibility completely
  would have no observable effect.  Generalising the crash means that
  a large subset of possible executions can be eliminated in one step,
  and so the derived fixes are more likely to be useful.

  The straw-man approach also has the weakness that there is no
  particular reason to assume that flipping an arbitrarily chosen race
  would alter the observed behaviour.  Our algorithm for deriving the
  crash predictors performs sufficient analysis that we can be
  confident that, if it predicts successful execution, the program
  will at least not crash immediately in exactly the same way
  (although we cannot, of course, guarantee that it will not crash
  later on).

\item Second, we apply this predictor to a log obtained from a
  deterministic replay system, and hence partition the log into safe
  regions, where the predictor predicts that the thread would execute
  successfully, and unsafe ones, where it does not.  Examining the
  boundaries of these regions then allows us to infer which
  instructions in remote threads modify the structure which the
  reading thread was reading, and hence which other parts of the
  program might need also need synchronizing.
\item Finally, we synthesise additional synchronization which ensures
  that the relevant part of the crashing thread executes atomically
  and does not execute in an unsafe region, and use a binary patcher
  to insert it into the program.  This prevents the bug from
  recurring.\editorial{Name the phases?}
\end{itemize}

There are two major sources of approximation in this approach.  First,
there are a number of inherent trade-offs involved in generating the
predictors in step one:

\begin{itemize}
\item How much of the crashing thread should be considered?  Going
  back further allows more complicated data structure invariants to be
  discovered, but also leads to more complicated predictors and
  increases the risk of irrelevant memory accesses being bundled into
  the critical sections.
\item What does it mean for a thread to ``execute successfully''?
  Simply delaying a crash by a few instructions is not useful, but at
  the same time it attempting to run the thread until it terminates
  normally is unlikely to be practical.
\item Even within the region modelled, how accurate should the
  approximation be?  We would like to model just the parts of the
  computation which are relevant to inter-thread communication, and
  factor out the entirely thread-local parts, but the division is not
  always entirely unambiguous.  Even when it is, it may not be
  possible to reliably infer it from the information available.
\end{itemize}

We discuss these trade-offs in more detail in later sections.  In some
cases we consider several possible answers, and this can lead to SLI
producing more than one possible fix; we discuss heuristics for
selecting the most appropriate one later.

Second, we assume that the log used in step two contains all of the
relevant write-side behaviour: if some update to the relevant
structures is not represented in the log, we will not be able to
discover it, and will not be able to synthesize appropriate
synchronization.  This could allow a very similar bug to recur.

Third, we assume that stacks are thread-private.

\subsection{Safety}

The fixes generated by SLI are safe, in the sense that any behavior
which is possible with a fix applied is also possible when the fix is
not applied, assuming a model of program execution in which
instructions can be arbitrarily delayed.  This corresponds to the
model presented to user-level applications by most non-realtime
operating systems, including Linux and Windows, and so is likely to be
sufficient for many pieces of widely used software.

Of course, possible is not always the same as desirable.  For example,
it might be that a fix causes certain operations which would otherwise
always succeed to instead always time out, and this might render the
software unusable without being unsafe within this definition.
Identifying such time-dependent correctness constraints is difficult
without semantic knowledge of the program's intended behavior, and
automatically determining whether a fix causes some to be violated is
even more challenging, and so it seems likely that this weakness will
be inherent in any scheme which fixes bugs without reference to a
manually-generated specification of the fixed program.\editorial{Not
  very happy with that phrasing.}


The first phase of our algorithm is to derive a predicate on the state
of a machine which can determine whether global memory is currently in
a ``good'' state.  This takes the form of an acyclic state machine
which approximates a suffix of the crashing thread's execution.  This
process is closely related to program slicing\needCite{}, in that the
state machines are roughly similar to a backwards dynamic slice of the
part of the crashing thread which is most relevant to the observed
crash.

These predicates can also be regarded as single-threaded explanations
of the crash.  For instance, the predicate might be that the value
loaded at instruction $i$ must point at a structure which contains a
valid pointer at offset $n$, in which case the equivalent explanation
would be that the thread crashed because the value loaded at
instruction $i$ pointed at something which did \emph{not} contain a
valid pointer at offset $n$.  Of course, most data structures have
multiple valid invariants, and likewise most crashes have multiple
valid explanations.  There are two obvious ways for these variations
to come about:

\begin{itemize}
\item Horizon effects.  While it would be possible in principle to
  consider every aspect of the thread's execution back to the moment
  at which it was created, doing so would be both computationally
  infeasible and unhelpful.  As such, we do not consider any part of
  execution which occurs before some horizon.  This horizon changes as
  the algorithm executes.
\item Something about which bits are taken as givens and which are
  computed?\editorial{...}
\end{itemize}

\editorial{Analogy with regular expressions?  Regexps compile down to
  state machines which examine a string; we compile a chunk of machine
  code down to a state machine which examines global memory.}
  
In principle, this phase of the algorithm could be completed using
pure static analysis.  However, since we need to have a DRS log anyway
for the next stage of the algorithm, we use it here as a limited kind
of oracle, providing hints as to which parts of the program need to be
most accurately approximated.

These predictors are approximations of the program's real behavior,
exhibiting both false positive and false negative errors, and can
predict a crash where none will actually occur or safe execution when
a crash is possible.  It would be possible to design an algorithm
which avoids either or both of these classes of errors, but doing so
would not necessarily be helpful.

Consider false positives, where a state machine predicts a crash when
none is possible, first.  These will cause the final fix to eliminate
some schedules which would have been permitted with a perfect
characterization, but---provided that there is always some schedule
available---this cannot cause a correct program to become incorrect.
SLI can therefore tolerate some false positives in the
characterization without producing incorrect fixes.  Likewise, the
effect of a false negative is to only partially resolve a bug, so that
some possible manifestations remain possible.  This is
indistinguishable from there simply being several bugs, and so,
provided that the characterization is sufficiently accurate to predict
a crash for the captured execution, false negatives do not prevent SLI
from fulfilling its goal of fixing the observed crash.\editorial{Feels
  glib.}

Hence we do not attempt to produce a perfect characterization.  This
allows us to use much simpler analyses and to produce much simpler
characterizations, which makes it more likely that the system will be
able to generate a correct fix.  When we do make approximations in the
characterization, we attempt to ensure that the captured execution
remains possible, and apply coarser approximation to executions which
are very different from the captured one.  The intuition behind this
is that the captured execution is the only one which is known to be
relevant to the bug, while all of the other paths discovered by static
analysis are speculative: we do not know if they can ever happen at
all, much less if they are relevant to the observed behavior.  It
therefore makes sense that, when an approximation must be made in
order to reduce execution time, or simply to make an analysis
feasible, it should be made on these speculative paths.







Once a single-threaded characterization of the bug has been produced,
SLI begins to investigate the crashed thread's interactions with other
threads in the system.  Our basic approach here is to replay the
program's execution and discover points at which the state machine
would have returned \verb|no-crash| if it had executed atomically.
From these, it is possible to infer which parts of the program's
execution should be synchronized against each other, and hence to
suggest potentially useful synchronizations.  In principle, this stage
can run as soon as the first state machine has been derived, but it is
possible to derive a significant performance advantage by grouping
machines into small batches and hence reducing the number of times
which the log file has to be replayed.  As we show in
\S\ref{sect:evaluation}, replaying the log file contributes a large
proportion of the entire cost of generating a fix, and so this is a
very worthwhile optimization.\editorial{Batch size?}

Each specialized state machine is then used to partition the log into
\verb|crash| and \verb|no-crash| regions.  Conceptually, each machine
is evaluated for every possible state of memory, and the final result
of the machine is then used to label the state.  Our implementation
optimises this slightly by only re-evaluating a machine when a
location which it loads changes, but is otherwise straightforward.

These specialized partitionings are then combined to produce a
partitioning for the original unspecialized state machine: wherever
all of the specialized partitionings make the same prediction, the
unspecialized partition will have the same prediction, and wherever
they differ the unspecialized partition will make no prediction at
all.  If the resulting partition never predicts \verb|no-crash| then
the original state machine is discarded, as it cannot possibly
generate a useful fix.\editorial{This is what I actually do, but it's
  also obviously utter nonsense.  Should say that any machine
  specialisation crashing makes the whole thing crash.}

%% See \S\ref{sect:final_example} for a situation in which specialization
%% allows us to derive an additional potential fix, and the discussion of
%% the \verb|twovar| test case in \S\ref{sect:bug_descr} for one in which
%% the specialized predictions would have produced an incorrect fix if
%% they had been used without the recombination step.\editorial{Need more
%%   explanation of why we're doing this.}

